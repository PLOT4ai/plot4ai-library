[
	{
		"category": "Data & Data Governance",
		"id": 1,
		"colour": "83b3db",
		"cards": [
			{
				"question": "Can we assure that the data that we need is complete and trustworthy?",
				"threatif": "No",
				"explanation": "Can you avoid the known principle of “garbage in, garbage out”? Your AI system is only as reliable as the data it works with.",
				"recommendation": "* Verify the data sources:\n * Is there information missing within the dataset?\n * Are all the necessary classes represented?\n * Does the data belong to the correct time frame and geographical coverage?\n * Evaluate which extra data you need to collect/receive.\n* Carefully consider representation schemes, especially in cases of text, video, APIs, and sensors. Text representation schemes are not all the same. If your system is counting on ASCII and it gets Unicode, will your system recognize the incorrect encoding? Source: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Are we preventing Target Leakage?",
				"threatif": "No",
				"explanation": " Target Leakage is present when your features contain information that your model should not legitimately be allowed to use, leading to overestimation of the model's performance. It can occur when information from outside the training dataset is improperly included in the model during training, leading to an unrealistically high performance during evaluation",
				"recommendation": "* Avoid using proxies for the outcome variable as a feature.\n* Do not use the entire data set for imputations, data-based transformations or feature selection.\n* Avoid doing standard k-fold cross-validation when you have temporal data.\n* Avoid using data that happened before model training time but is not available until later. This is common where there is delay in data collection.\n* Do not use data in the training set based on information from the future: if X happened after Y, you shouldn’t build a model that uses X to predict Y.",
				"sources": "[Leakage in data mining: formulation, detection, and avoidance](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf)\n[The Treachery of Leakage](https://medium.com/@colin.fraser/the-treachery-of-leakage-56a2d7c4e931)\n[Top 10 ways your Machine Learning models may have leakage](http://www.rayidghani.com/2020/01/24/top-10-ways-your-machine-learning-models-may-have-leakage/)\n[Leakage and the Reproducibility Crisis in ML-based Science](https://reproducible.cs.princeton.edu/)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Are we preventing Concept and Data Drift?",
				"threatif": "No",
				"explanation": "* Data Drift weakens performance because the model receives data on which it hasn’t been trained. It causes changes in the statistical properties of the input data distribution (e.g., feature distributions shift over time).\n* With Concept Drift the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways causing accuracy issues. It causes changes in the relationship between input features and the target variable (e.g., customer behavior changes over time, impacting a predictive model)",
				"recommendation": "* Implement robust monitoring tools to detect data and concept drift, and establish governance policies for regular data validation and model retraining.\n* Select an appropriate drift detection algorithm and apply it separately to labels, model’s predictions and data features.",
				"sources": "[Data Drift vs. Concept Drift](https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/)\n[Characterizing Concept Drift](https://www.researchgate.net/publication/283761478_Characterizing_Concept_Drift)\n[Inferring Concept Drift Without Labeled Data](https://concept-drift.fastforwardlabs.com/)\n[Automatic Learning to Detect Concept Drift](https://arxiv.org/abs/2105.01419)\n[From concept drift to model degradation: An overview on performance-aware drift detectors](https://www.sciencedirect.com/science/article/pii/S0950705122002854)\n[Learning under Concept Drift: A Review](https://arxiv.org/abs/2004.05785)\n[Detect data drift (preview) on datasets](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-monitor-datasets?tabs=python)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Once our model is running, can we keep feeding it data?",
				"threatif": "No",
				"explanation": "* Will you use the output from other models to feed your model again (looping)? Or will you use other sources ?\n* Are you sure this data will be continuously available?",
				"recommendation": "* Consider how the model will keep learning. Design a strategy to prevent issues with the next steps.\n* Imagine you planned to feed your model with input obtained by mining surveys and it appears these surveys contain a lot of free text fields. To prepare that data and avoid issues (bias, inaccuracies, etc) you might need extra time. Consider these type of scenarios that could impact the whole life cycle of your system!",
				"sources": "[Text Mining in Survey Data](https://www.surveypractice.org/article/6384-text-mining-in-survey-data)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "When datasets from external sources are updated, can we receive and process the new data on time?",
				"threatif": "No",
				"explanation": "* This could be especially risky in health and finance environments. How much change are you expecting in the data you receive?\n* How can you make sure that you receive the updates on time?",
				"recommendation": "Not only do you need to be able to trust the sources but you also need to design a process in which data is prepared on time to be used in the model and where you can timely consider the impact it could have in the output of the model, especially when this could have a negative impact on the users. This process can be designed once you know how often changes in the data can be expected and how big the changes are.",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we confirm the legitimacy of the data sources that we need?",
				"threatif": "No",
				"explanation": "* Data lineage can be necessary to demonstrate trust as part of your information transparency policy, but it can also be very important when it comes to assessing impact on the data flow. If sources are not verified and legitimised you could run risks such as data being wrongly labelled for instance.\n* Do you know where you need to get the data from? Who is responsible for the collection, maintenance and dissemination? Are the sources verified? Do you have the right agreements in place? Are you allowed to receive or collect that data? Also keep ethical considerations in mind!",
				"recommendation": "* Develop a robust understanding of your relevant data feeds, flows and structures such that if any changes occur to the model data inputs, you can assess any potential impact on model performance. In case of third party AI systems contact your vendor to ask for this information.\n* If you are using synthetic data you should know how it was created and the properties it has. Also keep in mind that synthetic data might not be the answer to all your privacy related problems; synthetic data does not always provide a better trade-off between privacy and utility than traditional anonymisation techniques.\n* Do you need to share models and combine them? The usage of Model Cards and Datasheets can help providing the source information.",
				"sources": "[Providing Assurance and Scrutability on Shared Data and Machine Learning Models with Verifiable Credentials](https://arxiv.org/pdf/2105.06370.pdf)\n[Synthetic Data – Anonymisation Groundhog Day](https://arxiv.org/pdf/2011.07018.pdf)\n[Model Cards](https://modelcards.withgoogle.com/about)\n[Model Cards for Model Reporting](https://arxiv.org/pdf/1810.03993.pdf)\n[Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Can we collect all the data that we need for the purpose of the algorithm?",
				"threatif": "No",
				"explanation": "Could you face difficulties obtaining certain type of data? This could be due to different reasons such as legal, proprietary, financial, physical, technical, etc. This could put the whole project in danger.",
				"recommendation": "In the early phases of the project (as soon as the task becomes more clear), start considering which raw data and types of datasets you might need. You might not have the definitive answer until you have tested the model, but it will already help to avoid extra delays and surprises. You might have to involve your legal and financial department. Remember that this is a team effort.",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input"
				]
			},
			{
				"question": "Are we tracking data provenance and lineage for AI models?",
				"threatif": "No",
				"explanation": "AI models require traceability of data sources to ensure ethical usage, reproducibility, and compliance. Without proper data lineage, it is difficult to verify the credibility and accuracy of training data.",
				"recommendation": "Use data lineage tracking tools to monitor where data originates and how it is modified over time.\n* Implement metadata standards (e.g., Datasheets for Datasets) to ensure clear documentation of data sources. \n*Regularly audit data providers to verify their reliability and adherence to ethical guidelines.",
				"sources": "[Datasheets for Datasets](https://arxiv.org/pdf/1803.09010)\n[Datasheets for Datasets](https://arxiv.org/pdf/1803.09010)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input"
				]
			},
			{
				"question": "Could our dataset have copyright or other legal restrictions?",
				"threatif": "Yes",
				"explanation": "Can you use the datasets that you need? or are there any restrictions? This could also apply to libraries and any other proprietary elements you might want to use.",
				"recommendation": "* Consider if you also need to claim ownership or give credits to creators.\n* Think about trademarks, copyrights in databases or training data, patents, license agreements that could be part of the dataset, library or module that you are using.\n* Legal ownership of digital data can sometimes be complex and uncertain so get the proper legal advise here.",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Transparency & Accessibility",
		"id": 2,
		"colour": "7fccdc",
		"cards": [
			{
				"question": "Does the AI system need to be explainable for users or affected persons?",
				"threatif": "No",
				"explanation": "Can users understand how the system generates its output? \n*Can users learn more about how the algorithm works or is your AI a 'black box'? \n* Is your model explainable and are you transparent about the data used to train the model, including where and how you sourced them?",
				"recommendation": "* Evaluate the type of models that you could use to solve the problem as specified in your task.\n* Consider what the impact is if certain black box models cannot be used and interpretability tools do not offer sufficient results. You might need to evaluate a possible change in strategy.\n* An explainable AI system refers not only to the model but also the user interfaces, data pipelines, and other components supporting the model's deployment and interpretation. \n*Data scientists can evaluate the impact from a technical perspective and discuss this with the rest of stakeholders. The decision keeps being a team effort. ",
				"sources": "[Explainable Artificial Intelligence (XAI)](https://www.darpa.mil/program/explainable-artificial-intelligence)\n[LIME](https://github.com/marcotcr/lime)\n[Why Should I Trust You? Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n[SHAP and LIME: An Evaluation of Discriminative Power in Credit Risk](https://www.frontiersin.org/articles/10.3389/frai.2021.752558/full)\n[Explainable AI](https://www.ibm.com/watson/explainable-ai)\n[Explainable AI - The TAILOR Handbook of Trustworthy AI](http://tailor.isti.cnr.it/handbookTAI/T3.1/T3.1.html)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy"
				]
			},
			{
				"question": "Is our AI system inclusive and accessible?",
				"threatif": "No",
				"explanation": "*Think for instance of elderly people, children, or people with disabilities. \n* Does our AI system need to be accessible and usable for users of assistive technologies (such as screen readers)? Is it possible to provide text alternatives for instance? \n* Does the system require a minimum level of AI literacy to be used? \n* Do we make sure that users can access the learning material needed to be able to use the system?\n\n Reference: Charter of fundamental rights of the European Union (rights of the elderly, rights of the child, Integration of persons with disabilities), AI Act(Article 4).",
				"recommendation": "* Implement Universal Design principles during every step of the planning and development process. This is not only important for web interfaces but also when AI systems/robots assist individuals.\n* Test the accessibility of your design with different users (also with disabilities).",
				"sources": "[A Proposal of Accessibility Guidelines for Human-Robot Interaction](https://www.mdpi.com/2079-9292/10/5/561/htm)\n[ISO/IEC 40500:2012 Information technology — W3C Web Content Accessibility Guidelines (WCAG) 2.0](https://www.iso.org/standard/58625.html)\n[ISO/IEC GUIDE 71:2001 Guidelines for standards developers to address the needs of older persons and persons with disabilities](https://www.iso.org/standard/33987.html)\n[ISO 9241-171:2008(en) Ergonomics of human-system interaction](https://www.iso.org/obp/ui/#iso:std:iso:9241:-171:ed-1:v1:en)\n[Mandate 376 Standards EU](https://ec.europa.eu/growth/tools-databases/mandates/index.cfm?fuseaction=search.detail&id=333)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we need to offer a redress mechanism to the users?",
				"threatif": "Yes",
				"explanation": "* For applications that can adversely affect individuals, you might need to consider implementing a redress by design mechanism where affected individuals can request remedy or compensation.\n* Article 22(3) GDPR provides individuals with a right to obtain human intervention if a decision is made solely by an AI system and it also provides the right to contest the decision.",
				"recommendation": "* Think about implementing mechanisms to effectively detect and rectify wrong decisions made by your system.\n* Provide a mechanism to ignore or dismiss undesirable features or services.\n* Wrong decisions could also have an impact on people that have not been the target of the data collection (data spillovers). Consider designing a way to offer all affected people the opportunity to contest the decisions of your system and request remedy or compensation. This mechanism should be easily accessible and it implies that you would need to have internally implemented a process where redress can be effectibily executed. This also has impact on the resources/skills needed to fulfil this process.\n* Consider this a necessary step to ensure responsibility and accountability.",
				"sources": "[EU guidelines on ethics in artificial intelligence: Context and implementation](https://www.europarl.europa.eu/RegData/etudes/BRIE/2019/640163/EPRS_BRI(2019)640163_EN.pdf)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "If users need to provide consent, can we make the required information easily available?",
				"threatif": "No",
				"explanation": "* Can the information be easily accessible and readable?\n* Do you need to build a special place for it (think of a robot where you might need to have a screen for showing the text)",
				"recommendation": "* As part of privacy compliance you need to provide clear information about the processing and the logic of the algorithm. This information should be easily readable and accessible. During the design phase consider when and how you are going to provide this information. Especially in robots using AI this could be a challenge.\n* Comply with accessibility rules.",
				"sources": "[A Proposal of Accessibility Guidelines for Human-Robot Interaction](https://www.mdpi.com/2079-9292/10/5/561/htm)\n[ISO/IEC 40500:2012 Information technology — W3C Web Content Accessibility Guidelines (WCAG) 2.0](https://www.iso.org/standard/58625.html)\n[ISO/IEC GUIDE 71:2001 Guidelines for standards developers to address the needs of older persons and persons with disabilities](https://www.iso.org/standard/33987.html)\n[ISO 9241-171:2008(en) Ergonomics of human-system interaction](https://www.iso.org/obp/ui/#iso:std:iso:9241:-171:ed-1:v1:en)\n[Mandate 376 Standards EU](https://ec.europa.eu/growth/tools-databases/mandates/index.cfm?fuseaction=search.detail&id=333)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the user perceive the message from the AI system in a different way than intended?",
				"threatif": "Yes",
				"explanation": "* Is the perception of the provided information the same as the one intended?\n* Explainability is critical for end-users in order to take informed and accountable actions.",
				"recommendation": "* Understanding who is going to interact with the AI system can help to make the interaction more effective. Identify your different user groups.\n* Involve communication experts and do enough user testing to reduce the gap between the intended and the perceived meaning.",
				"sources": "[The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations](https://arxiv.org/pdf/2107.13509.pdf)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the learning curve of the product be an issue?",
				"threatif": "Yes",
				"explanation": "* Does usage of the AI system require new (digital) skills?\n*Could a steep learning curve lead to misuse or harm?\n* How quickly are users expected to learn how to use the product?\n* Difficulties to learn how the system works could also bring the users in danger and have consequences for the reputation of the product or organisation.",
				"recommendation": "* You can also provide assistance, appropriate training material and disclaimers to users on how to adequately use the system.\n* The words and language used in the interface, the complexity and lack of accessibility of some features could exclude people from using the application. Consider making changes in the design of the product where necessary.\n* Consider this also when children are future users.",
				"sources": "",
				"qr": "",
				"categories": [
					"Transparency & Accessibility",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we inform users that they are interacting with an AI system?",
				"threatif": "No",
				"explanation": "* Are users adequately made aware that a decision, content, advice or outcome is the result of an algorithmic decision?\n* Users may be unaware they are interacting with AI, causing confusion or lack of trust.",
				"recommendation": "If you are developing AI systems intended to interact directly with natural persons (e.g., chatbots, robots), you are obliged to inform your end-users that they are interacting with an AI system (Article 50 AI Act). This information should be received at the beginning of the interaction.\n*If you are developing generative AI solutions, you must ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated (Article 50 AI Act).\n*If you are a deployer, you must inform your users if you are using an emotion recognition system or a biometric categorisation system (Article 50 AI Act). \n*If you are a deployer of AI systems that produce deep fakes, you must disclose that the content has been artificially generated or manipulated (Article 50 AI Act).",
				"sources": "",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we inform users about the AI system's reliability, limitations, and risks?",
				"threatif": "No",
				"explanation": "*Do users understand the specific context for which the system is designed? Are they aware of any actions or purposes they should avoid when using the system? \n* Do we communicate the level of accuracy of the system's output?\n* If an AI-assisted decision has been made about a person without any type of explanation or information then this may limit that person's autonomy, scope and self-determination. This is unlikely to be fair.",
				"recommendation": "* Provide clear information about how and why an AI-assisted decision was made and which personal data was used to train and test the model.\n* The model you choose should be at the right level of interpretability for your use case and for the impact it will have on the decision recipient. If you use a black box model make sure the supplementary explanation techniques you use provide a reliable and accurate representation of the systems behaviour. Source: UK ICO\n* Communicate the benefits, the technical limitations and potential risks of the AI system to users, such as its level of accuracy and/or error rates.\n* Ask your users (with a survey for instance) if they understand the decisions that your AI system makes.\n* Update your users about changes in the system that might have an impact on them",
				"sources": "",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are users equipped to understand how to use the AI system effectively??",
				"threatif": "No",
				"explanation": "Are users aware of the capabilities of the AI system? Users need to be informed about what to expect, not only for transparency reasons but in some products also for safety precautions.",
				"recommendation": "* Consider this as part of the GDPR transparency principle.\n* Users should be aware of what the AI system can do.\n* Clear Information should be provided on time and made accessible following accessibility design principles.",
				"sources": "[GDPR transparency principle](https://gdpr-info.eu/recitals/no-58/)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Privacy & Data Protection",
		"id": 3,
		"colour": "94cfbd",
		"cards": [
			{
				"question": "Can the training data be linked to individuals?",
				"threatif": "Yes",
				"explanation": "Do you need to use unique identifiers in your training dataset? If personal data is not necessary for the model you would not really have a legal justification for using it.",
				"recommendation": "* Unique identifiers might be included in the training set when you want to be able to link the results to individuals. Consider using pseudo-identifiers or other techniques that can help you protect personal data.\n* Document the measures you are taking to protect the data. Consider if your measures are necessary and proportional.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could we be revealing information that a person has not chosen to share?",
				"threatif": "Yes",
				"explanation": "* How can you make sure the product doesn’t inadvertently disclose sensitive or private information during use (e.g., indirectly inferring user locations or behaviour)?\n* Could movements or actions be revealed through data aggregation?",
				"recommendation": "* Be careful when making data public that you think is anonymised. Location data and routes can sometimes be de-anonymised (e.g. users of a running app disclosing location by showing heatmap).\n* It is also important to offer privacy by default: offer the privacy settings by default at the maximum protection level. Let the users change the settings after having offered them clear information about the consequences of reducing the privacy levels.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could geolocation restrictions or regional regulations impact the implementation of our AI system in other countries?",
				"threatif": "Yes",
				"explanation": "* AI systems often process sensitive data, including personal or location-based information, which may be subject to regional data sovereignty laws and ethical restrictions. Additionally, certain countries may restrict the deployment of AI technologies based on local regulatory frameworks, ethical concerns, or national security considerations. This could limit the usage of your product in those regions.",
				"recommendation": " Stay informed about the evolving regulatory landscape for AI, including data sovereignty, privacy laws, and ethical standards in different countries. Engage legal and compliance experts to assess restrictions in your target markets.\n* Consider designing your AI system with adaptability for regional requirements, such as geofencing, localized processing, or compliance with specific regulations (e.g., GDPR, AI Act, CCPA).\n* Monitor new AI-related regulations and international agreements to proactively address potential restrictions or adapt your system to comply with local laws.*",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Is data minimization possible while ensuring compliance with regulations like GDPR and maintaining model performance?",
				"threatif": "No",
				"explanation": "The principle of data minimization, as outlined in the General Data Protection Regulation (GDPR) and reflected in many global privacy standards, requires that only data necessary for achieving the system's purpose is collected and processed. However, reducing data too much can sometimes negatively impact the accuracy and performance of AI models, leading to critical or damaging consequences. Balancing regulatory compliance with operational effectiveness is essential to avoid undermining the model's reliability while adhering to privacy principles",
				"recommendation": "* Achieve data minimization by starting with a smaller dataset and iteratively adding data only as needed, based on observed performance improvements, to justify why additional data is necessary.\n* Use high-quality data to reduce the need for large datasets while ensuring sufficient diversity and representativeness for your model.\n* Apply advanced privacy-preserving techniques such as pseudonymization, perturbation, differential privacy, federated learning, or synthetic data generation to comply with privacy regulations while using larger datasets.\n* Collaborate with experts to select the minimum set of features needed, ensuring relevance to the objective and avoiding issues like the Curse of Dimensionality, which can degrade model performance when unnecessary features are included.",
				"sources": "pag. 13 [Artificial Intelligence and Data Protection How the GDPR Regulates AI](https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl-hunton_andrews_kurth_legal_note_-_how_gdpr_regulates_ai__12_march_2020_.pdf)\n[Data Minimization for GDPR Compliance in Machine Learning Models](https://arxiv.org/pdf/2008.04113.pdf): Methods like the one proposed in this paper can inspire you to find a way to mitigate the accuracy risk. They show how to reduce the amount of personal data needed to perform predictions, by removing or generalizing some of the input features.\nThe answer to this post also contains information about this problems in different models: [Does Dimensionality curse effect some models more than others?](https://stats.stackexchange.com/questions/186184/does-dimensionality-curse-effect-some-models-more-than-others)\n[Towards Breaking the Curse of Dimensionality for High-Dimensional Privacy](https://epubs.siam.org/doi/epdf/10.1137/1.9781611973440.84)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could we be processing sensitive data?",
				"threatif": "Yes",
				"explanation": "* According to art. 9 GDPR you might not be allowed to process, under certain circunstances, personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health data or data concerning a person’s sex life or sexual orientation.\n* You might be processing sensitive data if the model includes features that are correlated with these protected characteristics (these are called proxies).",
				"recommendation": "* If you need to use special categories of data as defined in the GDPR art. 9, then you need to check if you have the right lawful basis to do this.\n* Applying techniques like anonymisation might still not justify the fact that you first need to process the original data. Check with your privacy/legal experts.\n* Prevent proxies that could infer sensitive data (especially from vulnerable populations).\n* Check if historical data/practices might bias your data.\n* Identify and remove features that are correlated to sensitive characteristics.\n* Use available methods to test for fairness with respect to different affected groups.",
				"sources": "[AI Fairness 360](https://aif360.mybluemix.net)\n[What-if Tool: Playing with AI Fairness](https://pair-code.github.io/what-if-tool/ai-fairness.html)\n[Hunting for Discriminatory Proxies in Linear Regression Models](https://papers.nips.cc/paper/7708-hunting-for-discriminatory-proxies-in-linear-regression-models.pdf)\n[Differential Privacy Blog Series](https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/dp-blog)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Do we have a lawful basis for processing the personal data?",
				"threatif": "No",
				"explanation": "Do you know which GDPR legal ground you can apply?\n* (a) Consent: the individual has given clear consent for you to process their personal data for a specific purpose.\n* (b) Contract: the processing is necessary for a contract you have with the individual, or because they have asked you to take specific steps before entering into a contract.\n* (c) Legal obligation: the processing is necessary for you to comply with the law (not including contractual obligations).\n* (d) Vital interests: the processing is necessary to protect someone’s life.\n* (e) Public task: the processing is necessary for you to perform a task in the public interest or for your official functions, and the task or function has a clear basis in law.\n* (f) Legitimate interests: the processing is necessary for your legitimate interests or the legitimate interests of a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the individual which require protection of personal data, in particular where the individual is a child. (This cannot apply if you are a public authority processing data to perform your official tasks.)",
				"recommendation": "In the case of the GDPR you need to be able to apply one of the six available legal grounds for processing the data (art. 6). Check with your privacy expert, not being able to apply one of the legal grounds could bring the project in danger.\n\nTake into account, that also other laws besides the GDPR could be applicable.",
				"sources": "[Lawful basis for processing](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/lawful-basis-for-processing/)\n[Artificial Intelligence and Data Protection How the GDPR Regulates AI](https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl-hunton_andrews_kurth_legal_note_-_how_gdpr_regulates_ai__12_march_2020_.pdf)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we comply with the purpose limitation principle under GDPR and other global privacy regulations?",
				"threatif": "No",
				"explanation": "* The principle of purpose limitation, as defined in the General Data Protection Regulation (GDPR) and echoed in many global privacy frameworks, requires that personal data is collected for specified, explicit, and legitimate purposes and not further processed in a way incompatible with those purposes. Data repurposing is a significant challenge when applying this principle. If datasets were originally collected for a different purpose, their reuse without proper consent or legal justification may violate privacy regulations and ethical standards.",
				"recommendation": "Consult with your privacy officer or legal team to verify the original purpose of the data collection and evaluate any constraints or legal requirements. If data repurposing is necessary, consider obtaining additional consent, performing a legitimate interest assessment, or applying anonymization techniques to ensure compliance. Additionally, document all decisions and justifications for data reuse to demonstrate accountability under privacy regulations.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we comply with all the applicable GDPR data subjects’ rights?",
				"threatif": "No",
				"explanation": "* Can you implement the right to withdraw consent, the right to object to the processing and the right to be forgotten into the development of the AI system?\n* Can you provide individuals with access and a way to rectify their data?",
				"recommendation": "* Complying with these provisions from the GDPR (art. 15-21) could have an impact on the design of your product. What if users withdraw their consent? Do you need to delete their data used to train the model? What if you cannot identify the users in the datasets anymore? And what information should the users have access to?\n* Consider all these possible scenarios and involve your privacy experts early in the design phase.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Do we need to start conducting a Data Protection Impact Assessment (DPIA) to address privacy risks?",
				"threatif": "No",
				"explanation": "The use of AI is more likely to trigger the requirement for a DPIA, based on criteria in Art 35 GDPR. The GDPR and the EDPB’s Guidelines on DPIAs identify both “new technologies” and the type of automated decision-making that produce legal effects or similarly significantly affect persons as likely to result in a “high risk to the rights and freedoms of natural persons”.",
				"recommendation": "* This threat modeling library can help you to assess possible risks.\n* Remember that a DPIA is not a piece of paper that needs to be done once the product is in production. The DPIA starts in the design phase by finding and assessing risks, documenting them and taking the necessary actions to create a responsible product from day one until it is finalized.\n* Consider the time and resources that you might need for the execution of a DPIA, as it could have some impact on your project deadlines.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy"
				]
			},
			{
				"question": "Are we using third-party providers while processing data from children or other vulnerable individuals?",
				"threatif": "Yes",
				"explanation": "* If your system processes data from children or other vulnerable groups, any third-party providers you rely on (such as libraries, SDKs, or other tools) may also have access to this data. In such cases, you must ensure they comply with relevant privacy regulations like GDPR, COPPA, or similar frameworks. Even if your own system adheres to strong data protection measures, vulnerabilities or non-compliance on the part of third-party providers could expose sensitive data or create ethical risks.",
				"recommendation": "* Audit all third-party applications, libraries, and tools you use to determine what data they collect and ensure they comply with applicable regulations.\n* Confirm that proper agreements (e.g., Data Processing Agreements) are in place with all third-party providers to specify how data is handled.\n* Where possible, configure third-party tools to limit or avoid sharing sensitive data. Implement pseudonymization or anonymization techniques to protect data before sharing.\n* Evaluate the necessity of each third-party provider. If risks are identified, consider replacing or discontinuing use of certain providers, weighing the operational impact on your organization.*",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Do we need to use metadata to train or feed our AI model?",
				"threatif": "Yes",
				"explanation": "* Metadata provides descriptive information about other data, such as date, time, author, file size, or geolocation.\n* Although metadata may seem innocuous, it is often considered personal data under privacy regulations (e.g., GDPR) and can contain sensitive information. Misusing or failing to protect metadata can lead to privacy violations and unintended risks, especially if it reveals identifiable information.",
				"recommendation": "* Ensure that your use of metadata complies with applicable privacy regulations by verifying whether the data can be lawfully processed for your intended purpose.\n* Audit and verify metadata sources to confirm their accuracy and legitimacy.\n* Implement anonymization or pseudonymization techniques to minimize privacy risks while using metadata.\n* Limit the collection of metadata to only what is strictly necessary for the model, adhering to the principle of data minimization.*",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Could we compromise users’ privacy?",
				"threatif": "Yes",
				"explanation": "* The AI system may intrude on users' right to privacy by exposing sensitive aspects of their private lives, such as personal behaviors, preferences, or relationships, without their explicit consent or awareness. This can occur through excessive surveillance, unintended inferences, profiling, or sharing personal data without proper safeguards. Such compromises may undermine users' autonomy, dignity, and trust in the system, leading to legal, ethical, and reputational consequences for providers.",
				"recommendation": "* Ensure that the AI system respects the contextual integrity of users' private lives by limiting inferences and decisions to what is strictly necessary for its intended purpose. \n* Minimize the risk of profiling that could reveal sensitive personal attributes or behaviors unless explicitly justified by the intended use and supported by users’ consent or legal ground. \n*Design the AI system to avoid unnecessary observation or analysis of users’ private spaces, behaviors, or communications unless explicitly required by the use case. \n* Provide clear and accessible information to users about the extent and nature of the AI system's interaction with their private lives, ensuring that they are fully informed about its capabilities. \n* Empower users to set boundaries for their privacy by allowing them to control the scope of data collection and interaction with the AI system (Privacy by default). \n* Include ethical reviews and stakeholder consultations to assess the potential implications of the system on users’ privacy in diverse cultural and social contexts. \n* Implement safeguards to prevent the system from drawing unintended, intrusive, or harmful conclusions about individuals’ private lives. \n* Ensure robust security measures to prevent unauthorized access, surveillance, or other misuse of the system that could violate users’ privacy rights. \n* Provide mechanisms for users to report and address concerns if they feel their privacy has been violated, including remedies for potential harm caused.*",
				"sources": "Right to privacy (Universal Declaration of Human Rights), Article 7 Respect for Private and Family Life (Charter of fundamental rights of the European Union)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we comply with the storage limitation principle and international data retention regulations?",
				"threatif": "No",
				"explanation": "* The principle of storage limitation, as stated in Article 5(e) of the GDPR, requires personal data to be stored only as long as necessary for the intended purpose. Similarly, many global privacy regulations, such as CCPA (California), LGPD (Brazil), and PDPB (India), impose strict rules on data retention and deletion. Do you have a clear understanding of how long you need to keep the data (training data, output data, etc.) and whether you comply with internal, local, national, or international retention requirements?",
				"recommendation": " Personal data must not be stored longer than necessary for its intended purpose. To comply, it is essential to have a comprehensive overview of the data flow during the life cycle of the model.\n* Analyze all data types, including raw input data, training and testing sets, processed outputs (linked or merged data), and associated metrics. Understand where this data will be stored and for how long.\n* Define clear retention and deletion schedules, ensuring responsible individuals are assigned for managing data retention and disposal.\n* If data must be retained for auditing or quality purposes, anonymize it where possible to minimize privacy risks.\n* Stay informed about and comply with retention rules not only under GDPR but also under international frameworks such as CCPA (California Consumer Privacy Act), LGPD (Brazilian General Data Protection Law), and others. Retention and deletion policies should meet these diverse requirements. \n* Be aware that deleting data from a trained model is inherently challenging, as input data influences the model's internal representation during training. Consider legal implications for the model itself, as encoded thresholds and weights may also be subject to retention laws. Source: [BerryvilleiML](https://berryvilleiml.com/)",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Cybersecurity",
		"id": 4,
		"colour": "bdd895",
		"cards": [
			{
				"question": "Do we need to red-team/pen test the AI system for adversarial robustness and systemic vulnerabilities?",
				"threatif": "Yes",
				"explanation": "AI systems can be targeted in unique ways, such as adversarial inputs, poisoning attacks, or reverse-engineering of model outputs. These threats could compromise the system's confidentiality, integrity and availability, leading to reputational damage or harm to users. Testing for these issues may require specialized expertise, tools, and time, which could affect project timelines",
				"recommendation": "Plan for AI-specific penetration testing or red-teaming exercises, focusing on adversarial robustness, data governance, and model-specific vulnerabilities. Allocate time in the project for external audits, agreement on scope, and retesting if vulnerabilities are found.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are our APIs securely implemented?",
				"threatif": "No",
				"explanation": "APIs act as a critical interface for connecting systems and enabling AI model inference. Poorly secured APIs serve as entry points for adversaries to exploit vulnerabilities, potentially gaining unauthorized access to AI models. This access can expose sensitive information about the model's architecture, ontology, or training data. Adversaries may use legitimate API access to: \n* Discover the AI model's ontology and family.\n* Stage attacks by crafting adversarial inputs or verifying the success of exploits.\n* Introduce data to compromise the model, such as eroding model integrity or causing misclassification.\nThis risk is especially pronounced for shared foundational models used via inference APIs, as vulnerabilities in one implementation can impact multiple systems relying on the same model. Examples include identifying jailbreaks or leveraging hallucinations to compromise downstream applications.",
				"recommendation": "* Implement strong authentication mechanisms, such as API keys or OAuth, to prevent unauthorized access. Enforce role-based access controls (RBAC) to limit the API functionality available to different users.\n* Make sure that sensitive information such us API calls secrets are not sent in your commands.\n* Implement encryption at rest and in transit (TLS) and test often your APIs for vulnerabilities.\n* Regularly test APIs for vulnerabilities, such as injection attacks, improper state management, and insufficient rate limiting. Deploy anomaly detection to monitor for unusual usage patterns indicative of adversarial activity.\n* Limit the granularity of API outputs to prevent adversaries from reverse-engineering the model. Consider obfuscating or truncating prediction confidence levels to minimize exploitable information. Monitor and log all API activity to identify and respond to potential abuse. \n* When using foundational models shared across systems, collaborate with providers to ensure model robustness and security. Audit shared inference APIs to identify vulnerabilities, such as susceptibility to adversarial inputs, hallucinations, or jailbreak attempts.",
				"sources": "[OWASP API Security Project](https://owasp.org/www-project-api-security/)\n[BerryVilleiML](https://berryvilleiml.com/interactive/)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Is our data storage protected?",
				"threatif": "No",
				"explanation": "Is your data stored and managed in a secure way? Think about training data, tables, models, etc. Are you the only one with access to your data sources?\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "* Implement access control rules.\n* Verify the security of the authentication mechanism (and the system as a whole).\n* Consider the risk when utilizing public/external data sources.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "If our AI system uses randomness, is the source of randomness properly protected?",
				"threatif": "No",
				"explanation": "Randomness plays an important role in stochastic systems. “Random” generation of dataset partitions may be at risk if the source of randomness is easy to control by an attacker interested in data poisoning.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "Use of cryptographic randomness sources is encouraged. When it comes to machine learning (ML), setting weights and thresholds “randomly” must be done with care. Many pseudo-random number generators (PRNG) are not suitable for use. PRNG loops can really damage system behaviour during learning. Cryptographic randomness directly intersects with ML when it comes to differential privacy. Using the wrong sort of random number generator can lead to subtle security problems.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model"
				]
			},
			{
				"question": "Is our model suited for processing confidential information?",
				"threatif": "No",
				"explanation": "* There are certain kinds of machine learning (ML) models which actually contain parts of the training data in its raw form within them by design. For example, ‘support vector machines’ (SVMs) and ‘k-nearest neighbours’ (KNN) models contain some of the training data in the model itself.\n* Algorithmic leakage is an issue that should be considered carefully.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "When selecting the algorithm perform analyses and test to rule out algorithmic leakage.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[ICO - How should we assess security and data minimisation in AI?](https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/how-should-we-assess-security-and-data-minimisation-in-ai/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy"
				]
			},
			{
				"question": "Are we protected from insider threats?",
				"threatif": "No",
				"explanation": "AI designers and developers may deliberately expose data and models for a variety of reasons, e.g. revenge or extortion. Integrity, data confidentiality and trustworthiness are the main impacted security properties. Source: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "* Implement on and off boarding procedures to help guarantee the trustworthiness of your internal and external employees.\n* Enforce separation of duties and least privilege principle.\n* Enforce the usage of managed devices with appropriate policies and protective software.\n* Awareness training.\n* Implement strict access control and audit trail mechanisms.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected against model sabotage?",
				"threatif": "No",
				"explanation": "Model sabotage refers to deliberate exploitation or physical damage to machine learning platforms, libraries, or hosted services that provide AI/ML functionalities. Such sabotage could involve introducing vulnerabilities, altering model behaviors, or embedding backdoors during development, deployment, or updates. Backdoored models are particularly insidious, as they allow attackers to control the model’s behavior under specific conditions while appearing functional otherwise. Attackers could target third-party tooling, open-source libraries, or external providers to introduce malicious elements.Sources: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "* Implement strong security measures, including regular audits and penetration testing, to ensure the integrity of models and the platforms hosting them.\n* Assess and monitor the security profile of third-party libraries, tooling, and providers to ensure they are not compromised.\n* Develop and maintain a robust disaster recovery plan with explicit mitigation strategies for model sabotage scenarios.\n* Use model inspection tools to detect backdoors and ensure that the model’s behavior aligns with its intended function.\n* Incorporate supply chain security principles by verifying the authenticity and integrity of the components used in model development and deployment.\n* Maintain strict version control to detect and prevent unauthorized changes to libraries or model artifacts. \n*Implement anomaly detection systems to identify unusual usage patterns that may indicate attempted sabotage or exploitation.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[An Effective and Resilient Backdoor Attack Framework against Deep Neural Networks and Vision Transformers](https://arxiv.org/abs/2412.06149)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could there be possible malicious use, misuse or inappropriate use of our AI system?",
				"threatif": "Yes",
				"explanation": "An example of abusability:\nA product that is used to spread misinformation; for example, a chatbot being misused to spread fake news.",
				"recommendation": "* Threat model your system: anticipate vulnerabilities and look for ways to hijack and weaponize your system for malicious activity.\n* Conduct *red team* exercises.",
				"sources": "[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from evasion attacks?",
				"threatif": "No",
				"explanation": "* Evasion attacks involve modifying the input data to evade detection or classification by the model. These attacks can be used to bypass security systems, such as intrusion detection systems or spam filters.Example: Specific malware is crafted to avoid being flagged by a machine-learning-based antivirus",
				"recommendation": "**Develop anomaly detection systems to monitor deviations in input distributions and flag suspicious patterns.\n* Integrate robust logging mechanisms to analyze and mitigate the impact of detected attacks.\n* Train models with diverse and adversarial data, including known evasion techniques.\n* Implement ensemble modeling to reduce susceptibility to evasion attacks.\n* Ensure that thresholds and rules are periodically reviewed to adapt to evolving evasion techniques.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Adversarially Robust Malware Detection Using Monotonic Classification](https://people.eecs.berkeley.edu/~daw/papers/monotonic-iwspa18.pdf)\n[Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training](https://arxiv.org/abs/1711.08001)\n[Feature Denoising for Improving Adversarial Robustness](https://arxiv.org/abs/1812.03411)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from poisoning attacks?",
				"threatif": "No",
				"explanation": "* In a poisoning attack, the goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase. This attack could also be caused by insiders.\n* Example: in a medical dataset where the goal is to predict the dosage of a medicine using demographic information, researchers introduced malicious samples at 8% poisoning rate, which changed the dosage by 75.06% for half of the patients.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.\n\n**Other scenarios:**\n* Data tampering: Actors like AI/ML designers and engineers can deliberately or unintentionally manipulate and expose data. Data can also be manipulated during the storage procedure and by means of some processes like feature selection. Besides interfering with model inference, this type of threat can also bring severe discriminatory issues by introducing bias.\nSource: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)\n* An attacker who knows how a raw data filtration scheme is set up may be able to leverage that knowledge into malicious input later in system deployment.\nSource:[BerryVilleiML](https://berryvilleiml.com/interactive/)\n* Adversaries may fine-tune hyper-parameters and thus influence the AI system’s behaviour. Hyper-parameters can be a vector for accidental overfitting. In addition, hard to detect changes to hyper-parameters would make an ideal insider attack.\nSource: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "* Define anomaly sensors to look at data distribution on day to day basis and alert on variations.\n* Measure training data variation on daily basis, telemetry for skew/drift.\n* Input validation, both sanitization and integrity checking.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.\n\n* Implement measures against insider threats.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[Robustness Techniques & Toolkits for Applied AI](https://www.borealisai.com/research-blogs/robustness-techniques-toolkits-applied-ai/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from model inversion attacks?",
				"threatif": "No",
				"explanation": "* In a model inversion attack, if attackers already have access to some personal data belonging to specific individuals included in the training data, they can infer further personal information about those same individuals by observing the inputs and outputs of the ML model.\n* In model Inversion the private features used in machine learning models can be recovered. This includes reconstructing private training data that the attacker should not have access to.\n* Example: an attacker recovers the secret features used in the model through careful queries.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "* Interfaces to models trained with sensitive data need strong access control.\n* Implement rate-limiting on the queries allowed by the model.\n* Implement gates between users/callers and the actual model by performing input validation on all proposed queries, rejecting anything not meeting the model’s definition of input correctness and returning only the minimum amount of information needed to be useful.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from membership inference attacks?",
				"threatif": "No",
				"explanation": "* In a membership inference attack, the attacker can determine whether a given data record was part of the model’s training dataset or not.\n* Example: researchers were able to predict a patient’s main procedure (e.g.: Surgery the patient went through) based on the attributes (e.g.: age, gender, hospital).\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "* Some research papers indicate Differential Privacy would be an effective mitigation. Check for more information [Threat Modeling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml).\n* The usage of neuron dropout and model stacking can be effective mitigations to an extent. Using neuron dropout not only increases resilience of a neural net to this attack, but also increases model performance.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from model stealing attacks?",
				"threatif": "No",
				"explanation": "* In model stealing, the attackers recreate the underlying model by legitimately querying the model. The functionality of the new model is the same as that of the underlying model.\n* Example: in the BigML case, researchers were able to recover the model used to predict if someone should have a good/bad credit risk using 1,150 queries and within 10 minutes.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "* Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to *honest* applications.\n* Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n* Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from reprogramming deep neural nets attacks?",
				"threatif": "No",
				"explanation": "* By means of a specially crafted query from an adversary, Machine Learning systems can be reprogrammed to a task that deviates from the creator’s original intent.\n* Example: ImageNet, a system used to classify one of several categories of images was repurposed to count squares.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "* Configure a strong client-server mutual authentication and access control to model interfaces.\n* Takedown of the offending accounts.\n* Identify and enforce a service-level agreement for your APIs. Determine the acceptable time-to-fix for an issue once reported and ensure the issue no longer repros once SLA expires.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from adversarial example?",
				"threatif": "No",
				"explanation": "* Adversarial examples are a type of adversarial attack where malicious inputs are deliberately crafted to mislead AI models. These inputs are minimally modified, often imperceptible to humans, but can cause the model to produce incorrect or harmful predictions. Examples include researchers demonstrating that carefully designed patterns on accessories, like sunglasses, could deceive facial recognition systems into misidentifying individuals. Such examples are particularly problematic in critical domains like healthcare, finance, and security, where incorrect predictions could lead to severe consequences.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "Include adversarial examples in the training data to make models more robust against similar attacks. \n* Apply techniques such as input normalization, noise addition, or image resizing to reduce the impact of adversarial perturbations. \n* Design models with built-in robustness features to detect and counteract adversarial modifications. \n* Use multiple models and aggregate their predictions to make it harder for adversarial examples to deceive all models simultaneously. \n* Develop and apply techniques that mathematically guarantee the model’s resistance to certain adversarial manipulations. \n* Regularly test and monitor the system for new adversarial techniques to stay ahead of potential attacks.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[Attribution-driven Causal Analysis for Detection of Adversarial Examples](https://arxiv.org/abs/1903.05821)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from malicious AI/ML providers who could recover training data?",
				"threatif": "No",
				"explanation": "* Malicious ML providers could query the model used by a customer and recover this customer’s training data. The training process is either fully or partially outsourced to a malicious third party who wants to provide the user with a trained model that contains a backdoor.\n* Example: researchers showed how a malicious provider presented a backdoored algorithm, wherein the private training data was recovered. They were able to reconstruct faces and texts, given the model alone.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "* Research papers demonstrating the viability of this attack indicate Homomorphic Encryption could be an effective mitigation. Check for more information [Threat Modeling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n* Train all sensitive models in-house.\n* Catalog training data or ensure it comes from a trusted third party with strong security practices.\n* Threat model the interaction between the MLaaS provider and your own systems.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from attacks to the AI/ML Supply Chain?",
				"threatif": "No",
				"explanation": "* Owing to large resources (data + computation) required to train algorithms, the current practice is to reuse models trained by large corporations, and modify them slightly for the task at hand. These models are curated in a Model Zoo.\n In this attack, the adversary attacks the models hosted in the Model Zoo, thereby poisoning the well for anyone else.\n* Example: researchers showed how it was possible for an attacker to insert malicious code into one of the popular models. An unsuspecting ML developer downloaded this model and used it as part of the image recognition system in their code.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "* Minimize 3rd-party dependencies for models and data where possible.\n* Incorporate these dependencies into your threat modeling process.\n* Leverage strong authentication, access control and encryption between 1st/3rd-party systems.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.\n* Perform integrity checks where possible to detect tampering.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we protected from exploits on software dependencies of our AI/ML systems?",
				"threatif": "No",
				"explanation": "* In this case, the attacker does NOT manipulate the algorithms, but instead exploits traditional software vulnerabilities such as buffer overflows or cross-site scripting.\n* Example: an adversary customer finds a vulnerability in a common OSS dependency that you use and uploads a specially crafted training data payload to compromise your service.\n\nSource: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"recommendation": "Work with your security team to follow applicable Security Development Lifecycle/Operational Security Assurance best practices. Source: Microsoft, Threat Modelling AI/ML Systems and Dependencies.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Safety & Environmental Impact",
		"id": 5,
		"colour": "f7f09f",
		"cards": [
			{
				"question": "Could the channels that we will use to collect real-time input data fail?",
				"threatif": "Yes",
				"explanation": "* Are these channels trustworthy?\n* What will happen in case of failure?\n* Think for instance about IoT devices used as sensors.",
				"recommendation": "* If you are collecting/receiving data from sensors, consider estimating the impact it could have on your model if any of the sensors fail and your input data gets interrupted or corrupted.\n* Sensor blinding attacks are one example of a risk faced by poorly designed input gathering systems. Note that consistent feature identification related to sensors is likely to require human calibration. Source: [BerryVilleiML](https://berryvilleiml.com/)",
				"sources": "",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we need to implement an age gate to use our product?",
				"threatif": "Yes",
				"explanation": "* Is your product not meant to be used by children? You might need to implement an age verification mechanism to prevent children from accessing the product.\n* Age verification can also be important to provide the right diagnosis (health sector).",
				"recommendation": "* Clearly specify in the user instructions for which age group the application is built. Labels or symbols can be very helpful.\n* Consider which design is more appropriate based on your use case, and consider the possible risks associated with your design choice, and the mitigating measures you can implement to reduce that risk. Document the rest risks that you want to accept.\n* Test the accessibility of your design with different age groups.",
				"sources": "[Evolution in Age-Verification Applications](https://montrealethics.ai/evolution-in-age-verification-applications-can-ai-open-some-new-horizons/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Output",
					"Deploy"
				]
			},
			{
				"question": "Could the AI system propagate or amplify misinformation?",
				"threatif": "Yes",
				"explanation": "* AI systems, particularly language models and content recommendation algorithms, might spread misinformation or generate outputs that mislead users, potentially causing harm.",
				"recommendation": "* Develop content validation pipelines that identify and address misinformation. \n* Use human oversight to audit and correct generated content before dissemination.\n*Implement explainable AI techniques to clarify the rationale behind outputs.",
				"sources": "[Bontridder N, Poullet Y. The role of artificial intelligence in disinformation. Data & Policy. 2021;3:e32. doi:10.1017/dap.2021.20](https://www.cambridge.org/core/journals/data-and-policy/article/role-of-artificial-intelligence-in-disinformation/7C4BF6CA35184F149143DE968FC4C3B6)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the lack of interpretability in our AI models compromise safety?",
				"threatif": "Yes",
				"explanation": "* When AI systems operate as 'black boxes', developers may find it challenging to understand the underlying decision-making processes. This opacity can hinder the identification and rectification of errors, biases, or unintended behaviors, which can lead to unsafe or unreliable behavior in critical applications.",
				"recommendation": "* Invest in interpretability tools and methodologies that allow developers to analyze how models function and understand the factors influencing their outputs.\n* Incorporate explainability features into AI systems during the development phase to enhance debugging and error detection.\n* Develop and use validation frameworks to assess the safety implications of unexplained model behaviors and improve reliability.\n*Provide training for development teams to equip them with the skills needed to interpret and manage AI models safely.",
				"sources": "[Key Concepts in AI Safety: Interpretability in Machine Learning](https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/)\n[The AI Safety Atlas](https://ai-safety-atlas.com/chapters/09/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can over-reliance on automated systems lead to failures during emergencies?",
				"threatif": "Yes",
				"explanation": "* Relying too heavily on automation can reduce human involvement and oversight, making it difficult to respond quickly or effectively to unexpected failures or emergency situations.",
				"recommendation": "* Design systems with manual override capabilities and ensure operators are trained to use them effectively.\n* Create scenarios for testing human-AI collaboration under stress conditions.\n* Regularly evaluate the balance between automation and human oversight.",
				"sources": "[The Danger of Overreliance on Automation in Cybersecurity](https://www.automation.com/en-us/articles/december-2023/danger-overreliance-automation-cybersecurity)\n[Automation and Situation Awareness] (https://maritimesafetyinnovationlab.org/wp-content/uploads/2019/12/Automation-and-Situation-Awareness-Endsley.pdf)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can our AI system scale in performance from data input to data output?",
				"threatif": "No",
				"explanation": "Can your algorithm scale in performance from the data it learned on to real data? In online situations the rate at which data comes into the model may not align with the rate of anticipated data arrival. This can lead to both outright ML system failure and to a system that “chases\" its own tail.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "* Find out what the rate would be of expected data arrival to your model and perform tests in a similar environment with similar amount of data input.\n* Implement measures to make your model scalable.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could environmental phenomena or natural disasters compromise our AI system?",
				"threatif": "Yes",
				"explanation": "* Examples of environmental phenomena are heating, cooling and climate change.\n* Examples of natural disasters to take into account are earthquakes, floods and fires.\nEnvironmental phenomena may adversely influence the operation of IT infrastructure and hardware systems that support AI systems.\nNatural disasters may lead to unavailability or destruction of the IT infrastructures and hardware that enables the operation, deployment and maintenance of AI systems.\nSuch outages may lead to delays in decision-making, delays in the processing of data streams and entire AI systems being placed offline. Sources: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "Implement a disaster discovery plan considering different scenarios, impact, Recovery Time Objective (RTO), Recovery Point Objective (RPO) and mitigation measures.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "In case of system failure, could users be adversely impacted?",
				"threatif": "Yes",
				"explanation": "* Do you have a mechanism implemented to stop the processing in case of harm?\n* Do you have a way to identify and contact affected individuals and mitigate the adverse impacts?\n* Imagine a scenario where your AI system, a care-robot, is taking care of an individual (the patient) by performing some specific tasks and that this individual depends on this care.",
				"recommendation": "* Implement some kind of *stop button* or procedure to safely abort an operation when needed.\n* Establish a detection and response mechanism for undesirable adverse effects on individuals.\n* Define criticality levels of the possible consequences of faults/misuse of the AI system: what type of harm could be caused to the individuals, environment or organisations?",
				"sources": "",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				],
				"id": 44
			},
			{
				"question": "Is our AI model suitable and robust for its intended use across different deployment contexts?",
				"threatif": "Yes",
				"explanation": "Are you testing the product in a real environment before releasing it?\n When deploying an AI model, it is critical to ensure that it aligns with the intended use and functions effectively in its operational environment. If the model is trained and tested on data from one context but deployed in a different one, there is a significant risk of performance degradation or unintended behavior. This is particularly important in cases where environmental changes, unexpected inputs, or shifts in user interaction occur. Additionally, reinforcement learning models may require retraining when objectives or environments deviate slightly from the training setup. Beyond data, other contextual factors like legal, cultural, or operational constraints must be considered to ensure successful deployment",
				"recommendation": "* Use different data for testing and training. Make sure diversity is reflected in the data and that it aligns with the intended deployment environment. Specify your training approach, statistical methods, and ensure edge cases are adequately tested. Explore different environments and contexts to make sure your model is trained with the expected variations in data sources. \n* For reinforcement learning, ensure the objective functions are robust and adaptable to slight changes in the environment. \n* Are you considering enough aspects beyond data, such as legal, cultural, or operational factors? Did you forget any environmental variable that could affect performance or safety? Could limited sampling due to high costs or practical constraints pose a challenge? Document these risks and seek organizational support. The organization is accountable for mitigating or accepting these risks, which may require additional budget or resources. \n*  Consider applying techniques such as cultural effective challenge. This creates an environment where technology developers and stakeholders can actively participate in questioning the AI design and process. This approach better integrates social, cultural, and contextual factors into the design and helps prevent issues such as target leakage, where the AI system trains for an unintended purpose. \n* Set up mechanisms for real-time monitoring post-deployment. Continuously validate that the system is aligned with its intended use and can adapt or alert for significant changes in context or input. \n* Engage end-users in real-world testing to bridge any gaps between assumptions and practical application.",
				"sources": "Information about *cultural effective challenge*:   [A Proposal for Identifying and Managing Bias in Artificial Intelligence](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270-draft.pdf)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system become persuasive causing harm to the individual?",
				"threatif": "Yes",
				"explanation": "* This is of special importance in Human Robot Interaction (HRI): If the robot can achieve reciprocity when interacting with humans, could there be a risk of manipulation and human compliance?\n* Reciprocity is a social norm of responding to a positive action with another positive action, rewarding kind actions. As a social construct, reciprocity means that in response to friendly actions, people are frequently much nicer and much more cooperative than predicted by the self-interest model; conversely, in response to hostile actions they are frequently much more nasty and even brutal. Source: Wikipedia",
				"recommendation": "* Signals of susceptibility coming from a robot or computer could have an impact on the willingness of humans to cooperate or take advice from it.\n* It is important to consider and test this possible scenario when your AI system is interacting with humans and some type of collaboration/cooperation in expected.",
				"sources": "[The role of reciprocity in human-robot social influence](https://www.sciencedirect.com/science/article/pii/S258900422101395X)\n[Reciprocity in Human-Robot Interaction](https://ir.canterbury.ac.nz/bitstream/handle/10092/100798/Reciprocity-human-condition.pdf?sequence=2&isAllowed=y)\n[Social robots and the risks to reciprocity](https://link.springer.com/article/10.1007/s00146-021-01207-y)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy"
				]
			},
			{
				"question": "Could our AI agents develop strategies that could have undesired negative side effects on the environment?",
				"threatif": "Yes",
				"explanation": "* Reinforcement Learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n\n* To better understand the threat consider a case where a robot is built to move an object, without manually programming a separate penalty for each possible bad behaviour. If the objective function is not well defined, the AI’s ability to develop its own strategies can lead to unintended, harmful side effects. In this case, the objective of moving an object seems simple, yet there are a myriad of ways in which this could go wrong. For instance, if a vase is in the robot’s path, the robot may knock it down in order to complete the goal. Since the objective function does not mention anything about the vase, the robot wouldn’t know how to avoid it. Source: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"recommendation": "AI systems don’t share our understanding of the world. It is not sufficient to formulate the objective as “complete task X”; the designer also needs to specify the safety criteria under which the task is to be completed. A better strategy could be to define a *budget* for how much the AI system is allowed to impact the environment. This would help to minimize the unintended impact, without neutralizing the AI system.\n\nAnother approach would be training the agent to recognize harmful side effects so that it can avoid actions leading to such side effects. In that case, the agent would be trained for two tasks: the original task that is specified by the objective function and the task of recognizing side effects.\nThe AI system would still need to undergo extensive testing and critical evaluation before deployment in real life settings.\nSource: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could our AI agents “hack” their reward functions to exploit the system?",
				"threatif": "Yes",
				"explanation": "* Reinforcement Learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n\n* Consider potential negative consequences from the AI system\nlearning novel or unusual methods to score well on its objective function.\nSometimes the AI can come up with some kind of “hack” or loophole in the design of the system to receive unearned rewards. Since the AI is trained to maximize its rewards, looking for such loopholes and “shortcuts” is a perfectly fair and valid strategy for the AI. For example, suppose that the office cleaning robot earns rewards only if it does not see any garbage in the office. Instead of cleaning the place, the robot could simply shut off its visual sensors, and thus achieve its goal of not seeing garbage.\nSource: [OpenAI]( https://openai.com/blog/concrete-ai-safety-problems/)",
				"recommendation": "One possible approach to mitigating this problem would be to have a “reward agent” whose only task is to mark if the rewards given to the learning agent are valid or not. The reward agent ensures that the learning agent (robot for instance) does not exploit the system, but rather, completes the desired objective. For example: a “reward agent” could be trained by the human designer to check if a room has been properly cleaned by the cleaning robot. If the cleaning robot shuts off its visual sensors to avoid seeing garbage and claims a high reward, the “reward agent” would mark the reward as invalid because the room is not clean. The designer can then look into the rewards marked as “invalid” and make necessary changes in the objective function to fix the loophole. Source: [OpenAI]( https://openai.com/blog/concrete-ai-safety-problems/)",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Can our AI/ML system be robust to changes in the data distribution?",
				"threatif": "No",
				"explanation": "A complex challenge for deploying AI agents in real life settings is that the agent could end up in situations that it has never experienced before. Such situations are inherently more difficult to handle and could lead the agent to take harmful actions.\nSource: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"recommendation": "One promising research direction focuses on identifying when the agent has encountered a new scenario so that it recognizes that it is more likely to make mistakes. While this does not solve the underlying problem of preparing AI systems for unforeseen circumstances, it helps in detecting the problem before mistakes happen. Another direction of research emphasizes transferring knowledge from familiar scenarios to new scenarios safely. Source: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Can our AI agents learn about their environment without causing harm or catastrophic actions?",
				"threatif": "No",
				"explanation": "* Reinforcement Learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n\n* **Safe exploration**: An important part of training an AI agent is to ensure that it explores and understands its environment. While exploring, the agent might also take some action that could damage itself or the environment. Source: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"recommendation": "One approach to reduce harm is to optimize the performance of the learning agent in the worst case scenario. When designing the objective function, the designer should not assume that the agent will always operate under optimal conditions. Some explicit reward signal may be added to ensure that the agent does not perform some catastrophic action, even if that leads to more limited actions in the optimal conditions. Source: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Does training and deploying our AI system generate high CO₂ emissions?",
				"threatif": "Yes",
				"explanation": "* AI systems, especially large-scale models, require a lot of computational power. It’s important to consider the environmental impact of building and maintaining your system: does its scope and the benefits it provides justify its emissions? Are you effectively minimizing CO₂ emissions throughout your supply chain?.",
				"recommendation": "Prioritize renewable energy for data centers.\n*Reduce training time and computational waste by improving model efficiency.\n*Use energy-efficient chips and cooling systems to upgrade hardware. \n*Scale resources according to actual usage to avoid unnecessary deployment.\n*Track your carbon footprint and invest in offsets when needed.",
				"sources": "[AI’s Carbon Footprint Problem](https://hai.stanford.edu/news/ais-carbon-footprint-problem)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Is our data center's cooling process sustainable?",
				"threatif": "No",
				"explanation": "* Data centers use large volumes of water for server cooling, especially in hot climate regions. This could negatively impact the local supply of water, particularly in those territories already suffer from water scarcity.",
				"recommendation": "Prioritize waterless cooling technologies to reduce dependence on water. \n*Consider locating data centers in cooler climates or areas with better water management capabilities.",
				"sources": "[Our commitment to climate-conscious data center cooling](https://blog.google/outreach-initiatives/sustainability/our-commitment-to-climate-conscious-data-center-cooling/)\n[Towards Building a Sustainable System of Data Center Cooling and Power Management Utilizing Renewable Energy](https://www.researchgate.net/publication/364608372_Towards_Building_a_Sustainable_System_of_Data_Center_Cooling_and_Power_Management_Utilizing_Renewable_Energy)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design"
				]
			},
			{
				"question": "Is the production of our AI hardware exploiting limited material resources?",
				"threatif": "Yes",
				"explanation": "* AI hardware production relies on rare minerals like cobalt and lithium, which are often extracted at the cost of environmental damage and community exploitation. The short lifespan of AI devices also creates electronic waste and can involve leaking toxic chemicals into ecosystems and harming human health. When assessing your hardware, consider the resource availability and the risks of relying on these materials. Does your current hardware use materials that are becoming harder to source? Could this create future supply chain issues or environmental impact?.",
				"recommendation": "Invest in sustainable alternatives to rare minerals and prioritize ethical sourcing with transparent supply chains. \n*Promote recycling programs to recover rare metals and reduce electronic waste. \n*Design AI hardware for longer lifespans and easier recyclability using eco-friendly materials to minimize environmental harm.",
				"sources": "[Recyclable vitrimer-based printed circuit board for circular electronics](https://arxiv.org/abs/2308.12496)\n[Eco-Friendly Electronics—A Comprehensive Review](https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/admt.202001263)\n[Attributes of Commodity Supply Chains](https://uwspace.uwaterloo.ca/items/ffb47140-06e4-4d50-be6a-82ed01575998)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design"
				]
			},
			{
				"question": "Are we assessing our AI system’s environmental impact across its entire life cycle?",
				"threatif": "No",
				"explanation": "* An AI system’s environmental footprint goes beyond its operational phase. A full life cycle assessment (LCA) should account for resource extraction, hardware manufacturing, training, deployment, and end-of-life disposal. Key impact indicators include CO₂ emissions, energy and water consumption, and raw material use. Since many AI systems run in mixed-use facilities, properly allocating environmental costs can be complex but necessary for accurate reporting.",
				"recommendation": "Analyse the full environmental footprint of your system, from development to retirement. \n*Use clear metrics (e.g., emissions per token or annual energy use) to monitor impact. \n*Develop methodologies to fairly allocate environmental costs in shared computing environments. \n*Integrate LCA results into corporate reporting and sustainability strategies.",
				"sources": "[AI’s Carbon Footprint Problem](https://hai.stanford.edu/news/ais-carbon-footprint-problem)\n[AI is an energy hog. This is what it means for climate change](https://www.technologyreview.com/2024/05/23/1092777/ai-is-an-energy-hog-this-is-what-it-means-for-climate-change/)",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could children be part of the users of our AI system",
				"threatif": "Yes",
				"explanation": "* If children are potential users or exposed to your AI system, it is essential to ensure that the system respects the rights and best interests of the child. \n* This includes considering child protection, ethical communication, and designing the system to avoid harm or exploitation. \n* Inappropriate design or oversight could lead to risks to children’s mental, moral, or physical well-being, including potential misuse of the system by others to harm children.",
				"recommendation": "* Assess whether an age verification mechanism is necessary to restrict access to certain features or content.\n* Adapt communication and design in both the product and associated documentation, such as the privacy policy, to be child-appropriate and transparent.\n* Develop and enforce policies to ensure the safety and well-being of children when using or being exposed to your AI system.\n* Establish procedures to regularly assess and monitor the usage of your product to identify and mitigate any risks to children’s safety and health.\n* Provide clear labeling and instructions to ensure safe usage by children, including warnings about potential misuse.\n* Monitor for and address inappropriate or harmful usage of the AI system, including any attempts to exploit or harm children.\n* Develop a responsible marketing and advertising policy that explicitly avoids harmful, manipulative, or unethical practices targeting children",
				"sources": "[Convention on the Rights of the Child](https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-rights-child)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Bias, Fairness & Discrimination",
		"id": 6,
		"colour": "f8d18c",
		"cards": [
			{
				"question": "Can the data be representative of the different groups/populations?",
				"threatif": "No",
				"explanation": "It is important to reduce the risk of bias and different types of discrimination. Did you consider diversity and representativeness of users/individuals in the data?\n* When applying statistical generalisation, the risk exists of making inferences due to misrepresentation, for instance: a postal code where mostly young families live can discriminate the few old families living there because they are not properly represented in the group.",
				"recommendation": "* Who is covered and who is underrepresented?\n* Prevent disparate impact: when the output of a member of a minority group is disparate compared to representation of the group. Consider measuring the accuracy from minority classes too instead of measuring only the total accuracy. Adjusting the weighting factors to avoid disparate impact can result in positive discrimination which has also its own issues: disparate treatment.\n* One approach to addressing the problem of class imbalance is to randomly resample the training dataset. This technique can help to rebalance the class distribution when classes are under or over represented:\n - random oversampling (i.e. duplicating samples from the minority class)\n - random undersampling (i.e. deleting samples from the majority class)\n* There are trade-offs when determining an AI system’s metrics for success. It is important to balance performance metrics against the risk of negatively impacting vulnerable populations.\n* When using techniques like statistical generalisation is important to know your data well, and get familiarised with who is and who is not represented in the samples. Check the samples for expectations that can be easily verified. For example, if half the population is known to be female, then you can check if approximately half the sample is female.",
				"sources": "**Related to disparate impact**\n[AI Fairness - Explanation of Disparate Impact Remover](https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1)\n[Mitigating Bias in AI/ML Models with Disparate Impact Analysis](https://medium.com/@kguruswamy_37814/mitigating-bias-in-ai-ml-models-with-disparate-impact-analysis-9920212ee01c)\n[Certifying and removing disparate impact](https://arxiv.org/abs/1412.3756)\n[Avoiding Disparate Impact with Counterfactual Distributions](https://oconnell.fas.harvard.edu/files/hao/files/wesgai.pdf)\n\n**Related to random resampling**\n[Oversampling and Undersampling](https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf)\n[Random Oversampling and Undersampling for Imbalanced Classification](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)\n\n**Related to Statistical Generalization**\n[Generalization in quantitative and qualitative research: Myths and strategies](https://core.ac.uk/download/pdf/49282746.pdf)\n[Generalizing Statistical Results to the Entire Population](https://www.dummies.com/article/academics-the-arts/math/statistics/generalizing-statistical-results-to-the-entire-population-201267/)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could actions be incorrectly attributed to an individual or group?",
				"threatif": "Yes",
				"explanation": "Your AI system could have an adverse impact on individuals by incorrectly attributing them facts or actions. For instance, a facial recognition system that identifies a person incorrectly, or an inaccurate risk prediction model that could negatively impact an individual.",
				"recommendation": "* Evaluate the possible consequences of inaccuracies of your AI system and implement measures to prevent these errors from happening: avoiding bias and discrimination during the life cycle of the model, ensuring the quality of the input data, implementing a strict human oversight process, ways to double check the results with extra evidence, implementing safety and redress mechanisms, etc.\n* Assess the impact on the different human rights of the individual.\n* Consider not to implement such a system if you cannot mitigate the risks.",
				"sources": "",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could certain groups be disproportionately affected by the outcomes of the AI system?",
				"threatif": "yes",
				"explanation": "* Could the AI system potentially negatively discriminate against people on the basis of any of the following grounds: sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, birth, disability, age, gender or sexual orientation?\n* If your model is learning from data specific to some cultural background then the output could be discriminating for members of other cultural backgrounds.",
				"recommendation": "* Consider the different types of users and contexts where your product is going to be used.\n* Consider the impact of diversity of backgrounds, cultures, and other important different attributes when selecting your input data, features and when testing the output.\n* Assess the risk of possible unfairness towards individuals or communities to avoid discriminating minority groups.\n* The disadvantage to people depends on the kind of harm, severity of the harm and significance (how many people are put at a disadvantage compared to another group of people). Statistical and causal analyses of group differences are essential tools for evaluating potential unfairness and discriminatory impacts of AI systems.\n* Design with empathy, diversity and respect in mind.",
				"sources": "[Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922)\n[The Fairness Handbook](https://www.amsterdamintelligence.com/resources/the-fairness-handbook)\n[Advancing the field of bias detection and mitigation in Large Language Models and Traditional AI Models] (https://rhite.tech/files/bias-detection-in-llms-and-traditional-ai-models_extended.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system not represent current social needs and context?",
				"threatif": "Yes",
				"explanation": "The datasets that you want to use might not be representative of the current social situation. In that case the output of the model is also not representative of the current reality. Depending on the type of product you are designing this could have a big impact on the individual.",
				"recommendation": "Make sure that you are using correct, complete, accurate and current data. Also make sure that you have sufficient data to represent all possible contexts that you might need.",
				"sources": "",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system fail to uphold and respect human dignity?",
				"threatif": "Yes",
				"explanation": "Does the AI system treat all users with respect, ensuring no output undermines their dignity?\n*The need for data labeling is growing. Does our labeling process respect the rights and well-being of the workers involved? ",
				"recommendation": "Ensure system outputs are designed to avoid degrading, offensive, or dehumanizing content. Regularly test and audit the AI system for potential biases or outputs that could harm individuals’ dignity. \n* Establish fair labor conditions, including proper wages, working hours, and protections for workers involved in data labeling. Avoid exploitative labor practices, such as unreasonably low compensation or unsafe working conditions. Conduct regular audits to verify that third-party providers adhere to ethical standards. \n* Engage stakeholders, including user groups and labor rights organizations, to review and improve practices. \n* Train developers, data labelers, and system operators on the importance of preserving human dignity in AI-related tasks. \n*Include guidelines for respectful and non-discriminatory practices in AI system documentation and policies. \n* Implement mechanisms to identify and address cases where AI system outputs or processes violate human dignity. Provide users and stakeholders with channels to report concerns and ensure timely resolution.",
				"sources": "Article 1 Human Dignity (Charter of Fundamental Rights of the European Union)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system contribute to social division or rivalry?",
				"threatif": "Yes",
				"explanation": "Could the AI system inadvertently polarize opinions or foster division among groups by amplifying biases or stereotypes in its outputs? \n* Could the system's design or deployment lead to the stigmatization of specific groups, reinforcing harmful narratives or negative assumptions?.\n* AI systems, if not carefully designed and monitored, may unintentionally contribute to societal discord. Outputs influenced by biased data or algorithms could amplify stereotypes, marginalize groups, or reinforce societal divisions. The risks are heightened in applications with broad public interaction, such as social media, news dissemination, or educational tools, where outputs can shape public opinion.",
				"recommendation": "Conduct regular audits of system outputs to identify and mitigate content that may promote social division or negative stereotypes.\n* Include diverse stakeholder groups in the development process to identify risks of social bias or divisive content. \n* Implement content moderation and fairness mechanisms to ensure outputs are balanced and inclusive. \n* Train the system using representative and unbiased datasets to minimize the risk of amplifying societal divisions. \n* Monitor real-world impacts and continuously refine the system to align with ethical and societal norms.",
				"sources": "All human beings are free and equal, No discrimination (Universal Declaration of Human Rights)\n* Article 1 Human dignity, Article 20 Equality before the law, Article 21 Non-discrimination (Charter of fundamental rights of the European Union).",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system automatically label or categorize people?",
				"threatif": "Yes",
				"explanation": "* This could have an impact on the way individuals perceive themselves and society. It could constrain identity options and even contribute to erase real identity of the individuals.\n* This threat is also important when designing robots and the way they look. For instance: do care/assistant robots need to have a feminine appearance? Is that the perception you want to give to the world or the one accepted by certain groups in society? What impact does it have on society?",
				"recommendation": "* It is important that you check the output of your model, not only in isolation but also when this is linked to other information. Think in different possible scenarios that could affect the individuals. Is your output categorizing people or helping to categorize them? In which way? What could be the impact?\n* Think about ways to prevent adverse impact to the individual: provide information to the user, consider changing the design (maybe using different features or attributes?), consider ways to prevent misuse of your output, consider not to release the product to the market.",
				"sources": "",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Ethics & Human Rights",
		"id": 7,
		"colour": "f2bc9a",
		"cards": [
			{
				"question": "Could the AI system have an impact on human work?",
				"threatif": "Yes",
				"explanation": "* Risks include de-skilling and changes in safety conditions. \n* Could the use of your AI system affect the safety conditions of employees? \n* Does the system’s design or implementation involve exploitative labor practices? \n* Could the AI system create the risk of de-skilling of the workforce? (skilled people being replaced by AI systems) \n* Could the system’s outputs or actions limit fair competition or disadvantage certain businesses? \n* Does the system hinder workers' ability to organize, negotiate, or take collective action to protect their interests? \n* Could the system indirectly encourage or support child labor or unsafe work practices for young people?",
				"recommendation": "* Inform and consult impacted workers and their representatives (e.g., trade unions, work councils) before implementing the AI system. Foster an open dialogue to address concerns and ensure transparency. \n*Conduct impact assessments to understand how the AI system affects human work, including safety conditions, worker rights, and labor practices. Use these assessments to develop appropriate risk mitigation strategies. \n* Provide comprehensive training for workers to understand the AI system’s functionalities, limitations, and operational scope. Equip them with safety instructions, particularly when interacting with AI-driven machinery or robots. \n*Ensure that the AI system’s design and implementation uphold fair labor standards and avoid exploitative practices. Include safeguards to prevent indirect encouragement of child labor or unsafe work conditions. \n*Maintain clear documentation and transparency for businesses deploying your AI system. If you are a third-party provider, supply accessible and understandable information regarding the potential risks of the system to your customers. \n*Consider proactive measures to upskill or reskill employees whose roles may be affected by the system, ensuring they can transition to new or augmented roles supported by AI. \n*Regularly evaluate the system’s impact on competition, employee safety, and workplace dynamics. Adjust system features or provide additional guidance as needed to ensure compliance with fair labor and safety standards. \n*Engage with regulatory bodies and labor rights organizations to ensure the AI system complies with laws and ethical guidelines related to worker protection and well-being.",
				"sources": "Right to work, No slavery (Universal Declaration of Human Rights), article 16 Freedom to conduct a business, article 28 Right of collective bargaining and action, article 5 Prohibition of slavery and forced labor, Article 31 Fair and just working conditions, article 32 Prohibition of child labor and protection of young people at work (Charter of fundamental rights of the European Union).",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system affect democracy or have an adverse impact on society at large?",
				"threatif": "Yes",
				"explanation": "* Could your product be used for monitoring and surveillance purposes?\n* Could the system interfere with democratic principles, such as having a pluralistic system of political parties and organizations, or ensuring transparency and accountability in public administration?\n* Could the system influence voting choices, limit citizens' access to voting, or restrict their ability to run as candidates in elections?",
				"recommendation": "*Train the AI system on unbiased data and incorporate mechanisms to detect and address misinformation or disinformation that could affect democratic outcomes. If the system is used in voting or election processes, ensure robust cybersecurity measures and fail-safes to protect against tampering, hacking, or manipulation. \n* Design the system to promote pluralistic views and ensure it does not restrict or prioritize certain political narratives. \n* Adhere to relevant national and international legal standards protecting democracy, political freedoms, and human rights. \n* Continuously monitor the AI system’s impact on democratic institutions and processes, making adjustments as necessary to mitigate risks and uphold democratic principles.\n* Conduct an impact assessment to evaluate how the AI system might influence democratic processes, including political participation, electoral fairness, and public administration transparency. Implement strict policies to prevent the system from favoring or disfavoring specific political parties, candidates, or ideologies. \n* Make the system’s purpose, data sources, and decision-making processes clear and accessible to the public, ensuring that its operations can be scrutinized by independent parties. \n* Collaborate with regulatory bodies and civic organizations to establish oversight committees that monitor the system's impact on democratic processes.",
				"sources": "Right to democracy (Universal Declaration of Human Rights), article 41 Right to good administration, article 39 Right to vote and to stand as a candidate at elections to the European Parliament, article 40 Right to vote and to stand as a candidate at municipal elections (Charter of fundamental rights of the European Union).",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is our AI system contestable?",
				"threatif": "No",
				"explanation": "Are users allowed to disagree with an outcome from the AI system? Consider for instance the risk if your system makes automatic decisions that could have a negative impact on an individual and you do not offer any way to contest that decision.\n* Consider the risk if this was used in a criminal case and the consequences if wrong information is used to condemn someone. Do you have a mechanism to challenge the decisions of your AI system? ",
				"recommendation": "Ensure the AI system includes mechanisms for contestability, allowing users to challenge or seek review of decisions that negatively impact them. Provide clear instructions on how users can initiate such challenges and ensure that this process is transparent, accessible, and user-friendly. \n*Incorporate features that enable human oversight in decision-making processes, ensuring users have the option to escalate issues to human operators. \n*Establish a redressal process that includes timelines for resolution, a clear escalation hierarchy, and mechanisms for feedback integration to improve the system’s decision-making over time. \n*Regularly audit and evaluate the decision-making outcomes of the AI system, focusing on areas where users frequently raise disputes. Use these audits to improve system accuracy and reduce the need for contestation. \n*Provide detailed and comprehensible explanations of the system’s outputs to users, ensuring they understand how decisions are made and what data was used. \n*Engage relevant stakeholders, including legal experts, ethicists, and representatives from affected user groups to design and evaluate the contestability mechanisms and ensure they meet ethical and regulatory standards. \n*Train system operators and customer support staff to handle disputes arising from the AI system effectively, ensuring they are equipped to assist users in navigating the contestation process.",
				"sources": "Right to treated fair by court (Universal Declaration of Human Rights), article 11 Freedom of expression and information, article 47 Right to an effective remedy and to a fair trial (Charter of fundamental rights of the European Union).\n*[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)",
				"qr": "[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the system have an impact on decisions regarding the right to life?",
				"threatif": "Yes",
				"explanation": "Consider for instance the risk if your AI system is used in the health sector for choosing the right treatment for a patient. Is the output of the model accurate and fair? Are your datasets representative enough and free from bias? Does the system produce outputs, such as fake news, that could put the life of somebody in danger? Could the system encourage harmful health practices or medical misinformation? ",
				"recommendation": "Design the system with rigorous safety standards to minimize risks in scenarios affecting the right to life, such as healthcare or emergency response. Ensure datasets are representative and regularly validated for fairness, accuracy, and absence of harmful biases. \n*Include safeguards against outputs that may promote harmful practices, misinformation, or decisions endangering life. Conduct robust testing to identify and mitigate potential errors or unintended consequences. \n*Prohibit the dissemination of outputs that could incite violence, endanger health, or spread medical misinformation. Establish a monitoring mechanism to flag and rectify such outputs in real-time. \n*Engage domain experts, such as healthcare professionals or ethics specialists, in the system design and evaluation process. Use their input to ensure the AI system aligns with ethical standards for protecting life. \n*Establish a post-market monitoring system to identify and address risks that may emerge after deployment, especially in dynamic contexts like healthcare or public safety. \n*Provide training for users and operators to ensure they understand the system's limitations and ethical implications, particularly in life-critical decision-making contexts. Encourage informed and responsible use through comprehensive documentation and guidelines. \n*Create mechanisms for users and affected individuals to challenge decisions that may impact the right to life. Implement a robust redressal process to address grievances and prevent recurrence of harmful outcomes.",
				"sources": "Right to life, No torture and inhuman treatment (Universal Declaration of Human Rights), article 2 Right to life, article 3 Right to the integrity of the person, article 4 Prohibition of torture and inhuman or degrading treatment or punishment (Charter of fundamental rights of the European Union).\n*[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)",
				"qr": "[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system limit or distort the user input the freedom of expression of its users?",
				"threatif": "Yes",
				"explanation": "An AI system may inadvertently or deliberately suppress or distort user input, limiting individuals' ability to express themselves freely. For instance, if the system inaccurately labels text as hate speech, it might block valid expressions of opinion, chilling free speech and stifling open dialogue. Another example is the over-filtering of content based on overly broad or biased definitions of 'inappropriate' or 'unacceptable' language, which could suppress discussions on sensitive but critical topics. Such outcomes can disproportionately impact minority groups or individuals expressing dissenting viewpoints, leading to an erosion of trust in the platform and limiting diversity of thought.",
				"recommendation": "Adhere to ethical guidelines and ensure transparency and accountability. \n* Regularly audit and refine content moderation algorithms to minimize false positives in detecting harmful content. Incorporate diverse training data that reflects a wide range of cultural, linguistic, and contextual nuances. \n* Provide users with clear explanations and opportunities to contest or appeal content moderation decisions. Develop an independent oversight committee to review contentious cases of content removal. \n* Collaborate with diverse stakeholders to ensure freedom of expression is preserved. Test the system with input from underrepresented communities to identify potential biases or oversights. \n* Allow users to customize their interaction with content filters, such as by adjusting sensitivity levels or choosing topics they wish to see moderated differently. Provide clear guidelines and options for users to express themselves within platform policies. \n* Establish mechanisms for users to report errors in content moderation and provide constructive feedback. \n* Continuously monitor the system's performance and adapt to emerging risks or contexts that may affect freedom of expression. \n* Align the system’s operation with international standards protecting freedom of expression, such as article 11 of the Charter of Fundamental Rights of the European Union and article 19 of the Universal Declaration of Human Rights.",
				"sources": "[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)",
				"qr": "[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system impact access to healthcare, jobs, housing, insurance, benefits or education?",
				"threatif": "Yes",
				"explanation": "* The output of your model could be used to deny access to certain fundamental rights.\n* How can you be sure that the decisions of your AI system are always fair and correct?\n* How can you prevent causing harm to individuals?\n* AI systems intended to be used to determine access or admission, evaluate learning outcomes, or monitor students’ behaviors are classified as “high risk” by the AI Act (Annex III).\n* Could the AI system create barriers to healthcare access for some groups or individuals?",
				"recommendation": "Adhere to EU Trustworthy AI guidelines to ensure fairness and accountability\n* Use diverse, representative training data to reduce biases that could disproportionately impact certain groups. \n*Regularly audit the system for unintended discriminatory effects and address identified issues. \n*Provide clear explanations for decisions made by the AI system, including the data and logic used. Allow users to challenge decisions and request human reviews. \n*Establish post-market monitoring processes to detect and address issues that arise after deployment. \n*Update the system regularly to account for changes in legal requirements, societal norms, and data quality. For high-risk applications, such as determining healthcare access or evaluating job candidates, establish stringent safeguards to minimize the risk of harm. Implement thresholds and fail-safes to ensure critical decisions are accurate, fair, and reliable. \n*Work with regulatory bodies, civil society organizations, and industry peers to establish best practices and promote fairness and equity in AI systems. Ensure the AI system is designed to accommodate a wide range of users, including those with varying needs and abilities. Regularly test the system in diverse real-world settings to validate its accessibility and fairness. Use stakeholder consultations to understand the specific needs and vulnerabilities of affected groups.",
				"sources": "Right to education, Right of social service (Universal Declaration of Human Rights), article 14 Right to education, article 34 Social security and social assistance, article 35 Healthcare, article 36 Access to services of general economic interest,  (Charter of fundamental rights of the European Union)\n*[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)",
				"qr": "[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system affect human autonomy by interfering with the user’s decision-making process?",
				"threatif": "Yes",
				"explanation": "* Could your system affect which choices and which information is made available to people?\n* Could the AI system affect human autonomy by generating over-reliance by users (too much trust on the technology)?\n* Could this reinforce their beliefs or encourage certain behaviours?\n* Could the AI system create human attachment, stimulate addictive behaviour, or manipulate user behaviour? \n* Could the AI system mislead consumers or provide false recommendations? ",
				"recommendation": "* Clearly explain how the AI system processes inputs and generates outputs to avoid unintentional manipulation or misrepresentation. Ensure users understand the limitations and intended purposes of the AI system through user-friendly documentation and communication. Offer features that allow users to adjust or override AI recommendations, ensuring they maintain control over decisions. Implement mechanisms for users to pause, disable, or opt-out of certain AI functionalities.\n* Implement safeguards to detect and reduce over-reliance, such as reminders or notifications encouraging users to seek alternative opinions or double-check recommendations. Include disclaimers or warnings about the system’s limitations in contexts where over-reliance might occur. \n* Refrain from using techniques that exploit psychological vulnerabilities, such as gamification, excessive notifications, or reward loops that could encourage addictive behavior. Periodically evaluate whether design elements unintentionally foster dependency on the system. \n* Test the system with users from various cultural, socioeconomic, and demographic backgrounds to understand potential impacts on different groups. Incorporate diverse perspectives to avoid inadvertent biases that could restrict autonomy for certain populations. \n* Continuously monitor for any behaviors or outputs that may interfere with user decision-making processes. Use post-market monitoring to collect feedback and implement updates to reduce unintended autonomy infringements. Ensure human oversight mechanisms are in place for critical decision-making areas. Clearly define the role of the AI system as a tool to assist, not replace, human decision-making.",
				"sources": "Article 6 Right to liberty and security, article 3 Right to the integrity of the person, article 38 Consumer protection (Charter of fundamental rights of the European Union), article 5a (AI Act)  \n*[Dispositional and Situational Attributions of Human Versus Robot Behaviour](https://www.frontiersin.org/articles/10.3389/frobt.2021.788242/full)\n[Understanding Human Over-Reliance on Technology](https://www.ismp.org/resources/understanding-human-over-reliance-technology)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is our AI designed to promote multiple viewpoints and narratives?",
				"threatif": "No",
				"explanation": "* An AI system that does not consider or promote diverse viewpoints and narratives risks reinforcing biases, perpetuating stereotypes, or marginalizing specific groups. Such systems might unintentionally amplify dominant cultural, religious, or linguistic perspectives while excluding or suppressing minority voices. For example, content recommendation systems may disproportionately highlight mainstream viewpoints, reducing exposure to diverse cultural or ideological perspectives. This could hinder freedom of opinion and expression, harm cultural diversity, and lead to discriminatory outcomes.",
				"recommendation": "* Ensure datasets used for training and validation are diverse and representative of different cultural, religious, and linguistic groups. Design the system to recognize and value multiple perspectives, avoiding the prioritization of any single viewpoint. \n* Regularly test the AI system for biases that may marginalize or exclude certain narratives or groups. Use fairness metrics to evaluate how outputs reflect diversity and inclusivity. \n* Consult with diverse user groups, including minority communities, to understand their needs and perspectives. Include experts in cultural studies, ethics, and human rights during the development process. \n* Provide users with clear explanations of how the AI system processes and prioritizes content. Offer mechanisms for users to provide feedback on perceived biases or lack of representation. \n* Avoid algorithmic designs that overly amplify any particular narrative unless explicitly required by the use case. \n* Continuously monitor system outputs for patterns of exclusion or marginalization. \n* Regularly update models and algorithms to reflect evolving societal values and ensure alignment with inclusivity goals.",
				"sources": "Freedom of opinion and expression (Universal Declaration of Human Rights), article 11 Freedom of expression and information, article 21 Non-Discrimination, article 22 Cultural, religious and linguistic diversity, article 10 Freedom of thought, Conscience and religion (Charter of fundamental rights of the European Union )\n*[Value alignment](https://www.ibm.com/design/ai/ethics/value-alignment/)\n[Online Ethics Canvas](https://www.ethicscanvas.org/canvas/index.php)\n[AI Values and Alignment](https://www.deepmind.com/publications/artificial-intelligence-values-and-alignment)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Transparency & Accessibility",
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Does the AI system impose certain values or beliefs on users?",
				"threatif": "No",
				"explanation": "* Could cultural and language differences be an issue when it comes to the ethical nuance of your algorithm? Well-meaning values can create unintended consequences.\n* Must the AI system understand the world in all its different contexts?\n* Could ambiguity in rules you teach the AI system be a problem?\n* Can your system interact equitably with users from different cultures and with different abilities?).",
				"recommendation": "* Consider designing with value alignment, what means that you want to ensure consideration of existing values and sensitivity to a wide range of cultural norms and values.\n* Make sure that when you test the product you include a large diversity in type of users.\n* Think carefully about what diversity means in the context where the product is going to be used.\n* Remember that this is a team effort and not an individual decision!",
				"sources": "Freedom of thought and religion(Universal Declaration of Human Rights), article 22 Cultural, religious and linguistic diversity, article 10 Freedom of thought, Conscience and religion (Charter of fundamental rights of the European Union )\n*[Value alignment](https://www.ibm.com/design/ai/ethics/value-alignment/)\n[Online Ethics Canvas](https://www.ethicscanvas.org/canvas/index.php)\n[AI Values and Alignment](https://www.deepmind.com/publications/artificial-intelligence-values-and-alignment)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Transparency & Accessibility",
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Does the labelling of our training data respect the dignity and well-being of the labour force involved?",
				"threatif": "No",
				"explanation": "The need for labelling of data grows and unfortunately with that the amount of companies providing cheap labelling services at the cost of the dignity and labour rights of their workforce. Is the data that you are going to use labelled under such conditions?",
				"recommendation": "Conduct thorough audits of dataset sources to ensure they adhere to fair labor practices. Require documentation from third-party providers to verify compliance with labor laws and ethical standards. \n* Include clauses in contracts with vendors to uphold workers' rights, provide fair wages, and ensure safe working conditions. Choose providers who demonstrate commitment to ethical labor practices. \n* Use hybrid approaches combining automated tools and ethical human labor to reduce dependency on exploitative practices. \n* Provide training for teams involved in data acquisition to recognize the importance of ethical labor practices in the AI development pipeline. \n* Collaborate with NGOs, labor unions, and international organizations to promote better working conditions across the AI industry. \n* Develop mechanisms to receive and address complaints or concerns from workers involved in the labeling process. \n* Disclose the labeling process used for your datasets in product or system documentation. Be transparent with stakeholders about steps taken to ensure ethical labor practices in your supply chain.",
				"sources": "[The exploited labor behind AI](https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system fail to respect and safeguard children’s rights?",
				"threatif": "Yes",
				"explanation": "Children interacting with AI systems require special protections to ensure their rights, safety, and well-being are preserved. AI systems used by or designed for children must prioritize their best interests, such as ensuring age-appropriate content, safeguarding their privacy, and fostering their ability to share, learn, and express themselves freely. A failure to address these factors could result in harm, exploitation, or the suppression of their rights. For example, an AI system might expose children to inappropriate content, fail to protect their personal data, or limit their ability to engage in meaningful learning and expression.",
				"recommendation": "Develop and test the system for age-appropriateness. \n* Implement mechanisms to filter and block harmful or inappropriate content. \n* Adhere to strict data privacy regulations, such as GDPR, ensuring children’s data is protected. Foster safe environments where children can freely share their thoughts and ideas. Include features that support interactive and meaningful learning experiences. \n* Engage with experts in child development, education, and rights advocacy during the design phase. Consult children (where appropriate) to ensure their perspectives are respected and integrated. \n* Continuously monitor the AI system for unintended harms or risks to children. \n* Clearly communicate to parents, guardians, and educators how the AI system works and the measures in place to protect children. Provide accessible guidelines for safe and effective use.",
				"sources": "Article 24 The rights of the child (Charter of Fundamental Rights of the European Union)[Convention on the Rights of the Child, UNICEF](https://www.unicef.org/child-rights-convention)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Will we collect and use behavioral data to train or improve the AI system, and could this pose ethical risks?",
				"threatif": "Yes",
				"explanation": "Behavioral data refers to information collected about individuals' actions, habits, or interactions, such as browsing patterns, purchase history, or movement tracking. When using such data, there is a risk of reinforcing conformity behavior or encouraging certain actions deemed 'desirable' or 'undesirable' based on system design. This can lead to unintended consequences, such as behavioral exploitation or manipulation. For example, an authoritarian government might exploit these features to enforce compliance or suppress dissent by labeling or discouraging certain behaviors",
				"recommendation": "* Clearly define and critically evaluate how behaviors are labeled as 'positive' or 'negative' and consider the broader societal implications of these choices.\n* Incorporate diversity of opinions and perspectives during the design phase to mitigate ethical risks and ensure fairness in the system's outputs.\n* Assess the sufficiency and representativeness of the behavioral data collected, ensuring it is free from bias and aligns with ethical considerations.\n* Regularly audit and review the impact of labeling and feedback mechanisms to avoid reinforcing harmful behaviors or promoting conformity.\n* Establish guidelines and safeguards to prevent misuse of the system for manipulative or exploitative purposes, especially in politically sensitive or authoritarian contexts.",
				"sources": "Article 1, Human dignitie, article 7 Right to privacy, article 10 Freedom of thought, conscience, and religion (Charter of Fundamental Rights of the European Union)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Bias, Fairness & Discrimination",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Accountability & Human Oversight",
		"id": 8,
		"colour": "eea4b5",
		"cards": [
			{
				"question": "Is the task or assignment completely clear?",
				"threatif": "No",
				"explanation": "* Is the problem you want to solve well defined?\n* Are the possible benefits clear?",
				"recommendation": "* Clearly define the problem and outcome you are optimizing for.\n* Assess if your AI system will be well-suited for this purpose.\n* Always discuss if there are alternative ways to solve the problem.\n* Define success! Working with individuals who may be directly affected can help you identify an appropriate way to measure success.\n* Make sure there is a stakeholder involved (product owner for instance) with enough knowledge of the business and a clear vision about what the model needs to do.\n* Did you try analytics first? In this context analytics could also offer inspiring views that can help you decide on the next steps. They can be a good source of information and are sometimes enough to solve the problem without the need of AI/ML.",
				"sources": "",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Have we identified all the important stakeholders needed in this phase of the project?",
				"threatif": "No",
				"explanation": "* Do you have all the necessary stakeholders on board? Not having the right people that can give the necessary input can put the design of the AI system in danger.\n* Think for instance when attributes or variables need to be selected, or when you need to understand the different data contexts.\n* Data scientists should not be the only ones making assumptions about variables, it should really be a team effort.",
				"recommendation": "* Identify and involve on time the people that you need during the whole life cycle of the AI system. This will avoid unnecessary rework and frustrations.\n* Identifying who’s responsible for making the decisions and how much control they have over the decision-making process allows for a more evident tracking of responsibility in the AI’s development process.",
				"sources": "",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we have enough dedicated resources to monitor the AI system?",
				"threatif": "No",
				"explanation": "Do you already have a process in place to monitor the quality of the output and system errors? Do you have resources to do this? Not having the right process and resources in place could have an impact on the project deadline, the organisation and the users.",
				"recommendation": "* Put a well-defined process in place to monitor if the AI system is meeting the intended goals.\n* Define failsafe fallback plans to address AI system errors of whatever origin and put governance procedures in place to trigger them.\n* Put measure in places to continuously assess the quality of the output data: e.g. check that predictions scores are within expected ranges; anomaly detection in output and reassign input data leading to the detected anomaly.\n* Does the data measure what you need to measure? You could get measurement errors if data is not correctly labelled.",
				"sources": "",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Can we provide human resources to supervise and give feedback to the actions of AI agents?",
				"threatif": "No",
				"explanation": "* Reinforcement Learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n\n* When the agent is learning to perform a complex task, human oversight and feedback are more helpful than just rewards from the environment. Rewards are generally modelled such that they convey to what extent the task was completed, but they do not usually provide sufficient feedback about the safety implications of the agent’s actions. Even if the agent completes the task successfully, it may not be able to infer the side-effects of its actions from the rewards alone. In the ideal setting, a human would provide fine-grained supervision and feedback every time the agent performs an action (Scalable oversight). Though this would provide a much more informative view about the environment to the agent, such a strategy would require far too much time and effort from the human. Source: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"recommendation": "One promising research direction to tackle this problem is semi-supervised learning, where the agent is still evaluated on all the actions (or tasks), but receives rewards only for a small sample of those actions (or tasks).\n\nAnother promising research direction is hierarchical reinforcement learning, where a hierarchy is established between different learning agents. There could be a supervisor agent/robot whose task is to assign some work to another agent/robot and provide it with feedback and rewards.\nSource: [OpenAI](https://openai.com/blog/concrete-ai-safety-problems/)",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Monitor"
				]
			},
			{
				"question": "Is human intervention necessary to oversee the automatic decision making (ADM) process of the AI system?",
				"threatif": "Yes",
				"explanation": "* Do humans need to review the process and the decisions of the AI system? Consider the impact that this could have for the organisation.\n* Do you have enough capacitated employees available for this role?",
				"recommendation": "It is important that people are available for this role and that they receive specific training on how to exercise oversight. The training should teach them how to perform the oversight without being biased by the decision of the AI system (automation bias).",
				"sources": "[Automation Bias](https://en.wikipedia.org/wiki/Automation_bias)\n[The Flaws of Policies Requiring Human Oversight of Government Algorithms](https://arxiv.org/ftp/arxiv/papers/2109/2109.05067.pdf)\n[The False Comfort of Human Oversight as an Antidote to AI Harm](https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is the creation of the AI system proportional to the intended goal?",
				"threatif": "No",
				"explanation": "* Proportionality is a general principle of EU law. It requires you to strike a balance between the means used and the intended aim.\n* In the context of fundamental rights, proportionality is key for any limitation on these rights.",
				"recommendation": "* Proportionality requires that advantages due to limiting the right are not outweighed by the disadvantages to exercise the right. In other words, the limitation on the right must be justified.\n* Safeguards accompanying a measure can support the justification of a measure. A pre-condition is that the measure is adequate to achieve the envisaged objective.\n* In addition, when assessing the processing of personal data, proportionality requires that only that personal data which is adequate and relevant for the purposes of the processing is collected and processed. Source: [EDPS](https://edps.europa.eu/data-protection/our-work/subjects/necessity-proportionality_en)",
				"sources": "Proportionality test: [EDPS Guidelines on assessing the proportionality of measures that limit the fundamental rights to privacy and to the protection of personal data](https://edps.europa.eu/sites/edp/files/publication/19-12-19_edps_proportionality_guidelines_en.pdf)\n\nAssess the possible impact on human rights: [Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012P/TXT&from=EN)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Ethics & Fundamental Rights",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Will our AI system make automatic decisions without human intervention?",
				"threatif": "Yes",
				"explanation": "* Can these decisions have an important adverse impact on the individual? Think about someone’s legal rights, legal status, rights under a contract, or a decision with similar effects and significance. Article 22 of the GDPR specifically addresses automatic decision-making and profiling, providing safeguards to protect individuals' rights in such cases. Additionally, Article 86 of the AI Act requires transparency and the provision of clear explanations for significant decisions made by high-risk AI systems.",
				"recommendation": " Check with your privacy expert if your processing falls under Article 22 of the GDPR or under its exceptions. Human oversight is a key mitigation strategy and should be discussed with legal advisors and your development team.\n* Article 22(3) of the GDPR provides individuals with the right to obtain human intervention in automated decisions and the right to contest such decisions. Ensure your system complies with this requirement.\n* Align with the Artificial Intelligence Act (AIA) requirements for high-risk AI systems, which emphasize robust oversight mechanisms, risk management processes, and transparency. Conduct impact assessments to identify and mitigate potential adverse effects.\n* Implement human oversight mechanisms, ensuring that users or experts can intervene when necessary to prevent harm or adverse outcomes.\n* Ensure transparency and accountability by documenting decision-making processes and making these accessible to users. Trustworthy AI requires adherence to principles such as human agency, oversight, and accountability.\n* Regularly audit your system to assess and address risks, particularly in sensitive applications such as legal, healthcare, or employment-related decisions.*",
				"sources": "[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Privacy & Data Protection",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is there a clear escalation path when AI decisions lead to unintended consequences?",
				"threatif": "No",
				"explanation": "*If an AI system produces harmful or incorrect outputs, is there a predefined process for reporting and addressing these issues? \n*Are employees aware of how to escalate AI failures, and do they have clear channels to report incidents?",
				"recommendation": "Set up clear escalation protocols to identify, report, and resolve AI-related incidents. \n*Assign responsibilities to ensure accountability for handling AI failures. \n*Keep assessing and improving incident response strategies over time, especially after performing changes or updates in the AI systems.*",
				"sources": "[AI: An Accountability Framework for Federal Agencies and Other Entities](https://www.gao.gov/assets/gao-21-519sp.pdf)\n[The Human Factor in AI Safety](https://arxiv.org/abs/2201.04263)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Is the responsibility for AI decisions clearly assigned within the organization?",
				"threatif": "No",
				"explanation": "*AI outputs can lead to mistakes or even cause harm. In such cases, is it clear who is responsible within your organization? Are accountability structures well-defined and documented? ",
				"recommendation": "*Assign and communicate responsibilities for AI decision-making, considering both legal and ethical accountability. \n*Use decision logs and role-based access control to document and track accountability throughout the AI system’s lifecycle. \n*Get leadership involved in maintaining oversight, keeping accountability a priority at all levels.",
				"sources": "[AI: An Accountability Framework for Federal Agencies and Other Entities](https://www.gao.gov/assets/gao-21-519sp.pdf)\n[Algorithmic Accountability for the Public Sector](https://ainowinstitute.org/publication/algorithmic-accountability-for-the-public-sector-report)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Is there a mechanism to reassess the AI system’s goals and assumptions over time?",
				"threatif": "No",
				"explanation": "* AI models and their objectives may drift from their original intent, making human oversight crucial to ensure ongoing alignment with ethical and business objectives. Are there periodic human-led reviews in place to monitor AI system behavior, validate outcomes, and reassess goals? Human oversight should play an active role in detecting unintended consequences, adjusting governance policies, and maintaining accountability throughout the AI system’s lifecycle.",
				"recommendation": "Schedule regular reassessments of AI objectives and assumptions.\n*Update training data, governance policies, and oversight structures as AI systems evolve.*",
				"sources": "[Embedding Ethical Oversight in AI Governance through Independent Review](https://www.responsible.ai/embedding-ethical-oversight-in-ai-governance-through-independent-review/)\n[Effective Human Oversight of AI-Based Systems: A Signal Detection Perspective on the Detection of Inaccurate and Unfair Outputs](https://link.springer.com/article/10.1007/s11023-024-09701-0)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we planning to use a third party AI tool?",
				"threatif": "Yes",
				"explanation": "If you use a third party tool you might still have a responsibility towards the users. Think about employees, job applicants, patients, etc. It is also your responsibility to make sure that the AI system you choose won't cause harm to the individuals.",
				"recommendation": "If personal data is involved, review which ones are your responsibilities (look into art. 24 and 28 GDPR).\n\nYou can also start by checking:\n* That you have the right agreements in place with the third party provider.\n* That the origin and data lineage of their datasets are verified.\n* How their models are fed; do they anonymise the data?\n* How you have assessed their security, ethical handling of data, quality process and ways to prevent bias and discrimination in their AI system.\n* That you have informed users accordingly.",
				"sources": "",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			}
		]
	}
]
