[
	{
		"category": "Data & Data Governance",
		"id": 1,
		"colour": "83b3db",
		"cards": [
			{
				"question": "Is our data complete, up-to-date, and trustworthy?",
				"threatif": "No",
				"label": "Data Quality",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Can you avoid the known principle of “garbage in, garbage out”? Your AI system is only as reliable as the data it works with.",
				"recommendation": "* Verify the data sources:\n * Is there information missing within the dataset?\n * Can we verify that our training and input data hasn’t been tampered with or corrupted?\n * Are we using datasets that are outdated or no longer reflect the current environment?\n * Are all the necessary classes represented?\n * Does the data belong to the correct time frame and geographical coverage?\n * Evaluate which extra data you need to collect/receive.\n* Carefully consider representation schemes, especially in cases of text, video, APIs, and sensors. Text representation schemes are not all the same. If your system is counting on ASCII and it gets Unicode, will your system recognize the incorrect encoding? Source: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Can we prevent target leakage?",
				"threatif": "No",
				"label": "Target Leakage",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": " Target Leakage is present when your features contain information that your model should not legitimately be allowed to use, leading to overestimation of the model's performance. It can occur when information from outside the training dataset is improperly included in the model during training. This can result in an unrealistically high performance during evaluation.",
				"recommendation": "* Avoid using proxies for the outcome variable as a feature.\n* Do not use the entire data set for imputations, data-based transformations or feature selection.\n* Avoid doing standard k-fold cross-validation when you have temporal data.\n* Avoid using data that happened before model training time but is not available until later. This is common where there is delay in data collection.\n* Do not use data in the training set based on information from the future: if X happened after Y, you shouldn’t build a model that uses X to predict Y.",
				"sources": "[Leakage in data mining: formulation, detection, and avoidance](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf)\n[The Treachery of Leakage](https://medium.com/@colin.fraser/the-treachery-of-leakage-56a2d7c4e931)\n[Top 10 ways your Machine Learning models may have leakage](http://www.rayidghani.com/2020/01/24/top-10-ways-your-machine-learning-models-may-have-leakage/)\n[Leakage and the Reproducibility Crisis in ML-based Science](https://reproducible.cs.princeton.edu/)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Can we prevent concept and data drift?",
				"threatif": "No",
				"label": "Drift",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Data drift weakens performance because the model receives data on which it hasn’t been trained. It causes changes in the statistical properties of the input data distribution (e.g., feature distributions shift over time).\n* With Concept drift, the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways causing accuracy issues. It causes changes in the relationship between input features and the target variable (e.g., customer behavior changes over time, impacting a predictive model).",
				"recommendation": "* Implement robust monitoring tools to detect data and concept drift, and establish governance policies for regular data validation and model retraining.\n* Select an appropriate drift detection algorithm and apply it separately to labels, model’s predictions and data features.",
				"sources": "[Data Drift vs. Concept Drift](https://deepchecks.com/data-drift-vs-concept-drift-what-are-the-main-differences/)\n[Characterizing Concept Drift](https://www.researchgate.net/publication/283761478_Characterizing_Concept_Drift)\n[Inferring Concept Drift Without Labeled Data](https://concept-drift.fastforwardlabs.com/)\n[Automatic Learning to Detect Concept Drift](https://arxiv.org/abs/2105.01419)\n[From concept drift to model degradation: An overview on performance-aware drift detectors](https://www.sciencedirect.com/science/article/pii/S0950705122002854)\n[Learning under Concept Drift: A Review](https://arxiv.org/abs/2004.05785)\n[Detect data drift (preview) on datasets](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-monitor-datasets?tabs=python)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can the AI model maintain continuous access to data sources after deployment?",
				"threatif": "No",
				"label": "Data Continuity",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Will you use the output from other models to feed your model again (looping)? Or will you use other sources?\n* Your AI system may rely on internal pipelines or third-party data sources. If any of these become unavailable, the model may stop functioning or deliver inaccurate results.\n* This includes scenarios like discontinued APIs, broken survey collection tools, or changes in upstream system outputs.",
				"recommendation": "* Consider how the model will keep learning. \n* Identify critical data dependencies and define fallback mechanisms.\n* Assess whether key data sources are stable and under your control or subject to third-party risks.\n* Monitor availability of inputs to catch outages early.\n* Imagine you planned to feed your model with input obtained by mining surveys and it appears these surveys contain a lot of free text fields. To prepare that data and avoid issues (bias, inaccuracies, etc) you might need extra time. Consider these types of scenarios that could impact the whole life cycle of your system.",
				"sources": "[Text Mining in Survey Data](https://www.surveypractice.org/article/6384-text-mining-in-survey-data)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we process new or updated data from external sources without delay?",
				"threatif": "No",
				"label": "Update Latency",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* In high-stakes domains like healthcare or finance, delays in processing updated external data can lead to stale predictions or risky decisions.\n* Risks include slow ingestion pipelines, format mismatches, or batch processing delays that prevent real-time responsiveness.\n* How much change are you expecting in the data you receive?\n* Can you make sure that you receive the updates on time?",
				"recommendation": "* Design your data pipeline to handle frequent updates efficiently.\n* Validate incoming data formats, track data freshness, and assess update intervals.\n* Consider impact of delays on downstream decisions and mitigate with caching, fallback logic, or alerts. \n* Not only must you trust your sources, but you also need to design a process in which data is prepared on time to be used in the model and where you can timely consider the impact it could have in the output of the model, especially when this could have a negative impact on the users and system's behaviour. This process can be designed once you know how often changes in the data can be expected and how big the changes are.",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are all required data sources legitimate, authorized, and verified?",
				"threatif": "No",
				"label": "Data Legitimacy",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Data lineage can be necessary to demonstrate trust as part of your information transparency policy, but it can also be very important when it comes to assessing impact on the data flow. If sources are not verified and legitimized you could run risks such as data being wrongly labelled for instance.\n* Do you know where you need to get the data from? Who is responsible for the collection, maintenance and dissemination? Are the sources verified? Do you have the right agreements in place? Are you allowed to receive or collect that data? Also keep ethical considerations in mind!",
				"recommendation": "* Develop a robust understanding of your relevant data feeds, flows and structures such that if any changes occur to the model data inputs, you can assess any potential impact on model performance. In case of third party AI systems contact your vendor to ask for this information.\n* If you are using synthetic data you should know how it was created and the properties it has. Also keep in mind that synthetic data might not be the answer to all your privacy related problems; synthetic data does not always provide a better trade-off between privacy and utility than traditional anonymisation techniques.\n* Do you need to share models and combine them? The usage of Model Cards and Datasheets can help providing the source information.",
				"sources": "[Providing Assurance and Scrutability on Shared Data and Machine Learning Models with Verifiable Credentials](https://arxiv.org/pdf/2105.06370.pdf)\n[Synthetic Data – Anonymisation Groundhog Day](https://arxiv.org/pdf/2011.07018.pdf)\n[Model Cards](https://modelcards.withgoogle.com/about)\n[Model Cards for Model Reporting](https://arxiv.org/pdf/1810.03993.pdf)\n[Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf)",
				"qr": "",
				"categories": [
					"Data & Data Governance",
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Can we obtain the data needed to develop or fine-tune the AI model?",
				"threatif": "No",
				"label": "Data Collection",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Could you face difficulties obtaining certain type of data? This could be due to different reasons such as legal, proprietary, financial, physical, technical, etc. This could put the whole project in danger.",
				"recommendation": "In the early phases of the project (as soon as the task becomes more clear), start considering which raw data and types of datasets you might need. You might not have the definitive answer until you have tested the model, but it will already help to avoid extra delays and surprises. You might have to involve your legal and financial department. Remember that this is a team effort.",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input"
				]
			},
			{
				"question": "Can we trace the provenance and lineage of the data used to train or fine-tune the AI model?",
				"threatif": "No",
				"label": "Data Traceability",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI models require traceability of data sources to ensure ethical usage, reproducibility, and compliance. Without proper data lineage, it is difficult to verify the credibility and accuracy of training data.",
				"recommendation": "* Use data lineage tracking tools to monitor where data originates and how it is modified over time.\n* Implement metadata standards (e.g., Datasheets for Datasets) to ensure clear documentation of data sources. \n* Regularly audit data providers to verify their reliability and adherence to ethical guidelines.",
				"sources": "[Datasheets for Datasets](https://arxiv.org/pdf/1803.09010)",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input"
				]
			},
			{
				"question": "Could our dataset have copyright or other legal restrictions?",
				"threatif": "Yes",
				"label": "Copyright, IP & Legal Restrictions",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Consider any legal, licensing, or privacy constraints that might prevent you from using certain datasets. This also applies to proprietary libraries, tools, or other resources. ",
				"recommendation": "* Consider if you also need to claim ownership or give credits to creators.\n* Think about trademarks, copyrights in databases or training data, patents, license agreements that could be part of the dataset, library or module that you are using.\n* Legal ownership of digital data can sometimes be complex and uncertain so get the proper legal advice here.",
				"sources": "",
				"qr": "",
				"categories": [
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we detect and prevent data tampering across the AI lifecycle?",
				"threatif": "No",
				"label": "Data Integrity",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Data integrity is critical to ensuring that AI systems function as intended. Tampered data, whether during ingestion, transformation, storage, or transfer, can introduce hidden errors, biases, or malicious payloads. AI models built on compromised data may behave unpredictably, yield incorrect results, or violate compliance requirements. Integrity threats may be unintentional (e.g., pipeline errors) or deliberate (e.g., insider sabotage or supply chain attacks).",
				"recommendation": "* Implement data integrity checks (e.g., hashes, checksums) at critical stages of the data pipeline.\n* Use tamper-evident storage (e.g., append-only logs, signed records).\n* Employ data lineage and provenance tracking systems to trace the origin and transformation history of data.\n* Apply anomaly detection to catch unexpected shifts or inconsistencies in inputs.\n* Audit access to data and enforce change tracking on data sources used for training or inference.",
				"sources": "[ENISA - Securing Machine Learning Algorithms](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)",
				"qr": "",
				"categories": [
					"Data & Data Governance",
					"Cybersecurity"
				],
				"phases": [
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i"
				]
			}
		]
	},
	{
		"category": "Transparency & Accessibility",
		"id": 2,
		"colour": "7fccdc",
		"cards": [
			{
				"question": "Does the AI system need to be explainable for users or affected persons?",
				"threatif": "Yes",
				"label": "Explainability",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Is the algorithm transparent, or is it a 'black box' that users cannot interpret? \n* Can users learn about how the model works? \n* Is the model explainable, and are you open about the data used for training, including where and how it was sourced?",
				"recommendation": "* Evaluate the type of models that you could use to solve the problem as specified in your task.\n* Consider what the impact is if certain black box models cannot be used and interpretability tools do not offer sufficient results. You might need to evaluate a possible change in strategy.\n* An explainable AI system refers not only to the model but also the user interfaces, data pipelines, and other components supporting the model's deployment and interpretation. \n* Data scientists can evaluate the impact from a technical perspective and discuss this with the rest of stakeholders. The decision keeps being a team effort. ",
				"sources": "[Explainable Artificial Intelligence (XAI)](https://www.darpa.mil/program/explainable-artificial-intelligence)\n[LIME](https://github.com/marcotcr/lime)\n[Why Should I Trust You? Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n[SHAP and LIME: An Evaluation of Discriminative Power in Credit Risk](https://www.frontiersin.org/articles/10.3389/frai.2021.752558/full)\n[Explainable AI](https://www.ibm.com/watson/explainable-ai)\n[Explainable AI - The TAILOR Handbook of Trustworthy AI](http://tailor.isti.cnr.it/handbookTAI/T3.1/T3.1.html)\n[Microsoft HAX Toolkit, Guideline 11](https://www.microsoft.com/en-us/haxtoolkit/guideline/make-clear-why-the-system-did-what-it-did/)\n[EDPS,Explainable Artificial Intelligence](https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy"
				]
			},
			{
				"question": "Is our AI system inclusive and accessible?",
				"threatif": "No",
				"label": "Inclusivity",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems must be designed to be accessible and inclusive, particularly for individuals who may face barriers due to age, disability, or other vulnerabilities.\n* Think, for instance, of elderly people, children, or people with disabilities or individuals with cognitive impairments. Does the system support assistive technologies (e.g., screen readers)? Are there text alternatives, simplified navigation, or options for non-standard input and output formats?\n* Accessibility also includes cognitive accessibility: does the system assume a certain level of AI literacy or digital fluency that may not be present in all users? Are users supported with clear explanations, educational materials, or onboarding tools?\n* Inaccessible AI can lead to exclusion, discrimination, reduced autonomy, or even harm, violating fundamental rights under the Charter of Fundamental Rights of the EU (Articles 21: Non-discrimination, 24: Rights of the child, and 26: Integration of persons with disabilities).\n* The AI Act (Article 4) also highlights the need for systems to be inclusive and safe by design.",
				"recommendation": "* Conduct an impact assessment focusing on accessibility and vulnerability.\n* Involve affected groups and advocacy organizations in the design and testing phase.\n* Design interfaces that comply with Web Content Accessibility Guidelines (WCAG) and ensure compatibility with assistive technologies.\n* Avoid manipulative patterns (e.g., dark patterns) that exploit reduced digital literacy or cognitive overload.\n* Document accessibility limitations in model/system cards and ensure clear communication to users and caregivers.\n* Ensure age-appropriate design and protections for children, including safe defaults and data minimization.",
				"sources": "[A Proposal of Accessibility Guidelines for Human-Robot Interaction](https://www.mdpi.com/2079-9292/10/5/561/htm)\n[ISO/IEC 40500:2012 Information technology — W3C Web Content Accessibility Guidelines (WCAG) 2.0](https://www.iso.org/standard/58625.html)\n[ISO/IEC GUIDE 71:2001 Guidelines for standards developers to address the needs of older persons and persons with disabilities](https://www.iso.org/standard/33987.html)\n[ISO 9241-171:2008(en) Ergonomics of human-system interaction](https://www.iso.org/obp/ui/#iso:std:iso:9241:-171:ed-1:v1:en)\n[Mandate 376 Standards EU](https://ec.europa.eu/growth/tools-databases/mandates/index.cfm?fuseaction=search.detail&id=333)\n[Charter of fundamental rights of the European Union:rights of the elderly, rights of the child, Integration of persons with disabilities](https://www.europarl.europa.eu/charter/pdf/text_en.pdf)\n[Convention on the Rights of Persons with Disabilities](https://www.un.org/development/desa/disabilities/convention-on-the-rights-of-persons-with-disabilities.html)\n[Web Content Accessibility Guidelines (WCAG)](https://www.w3.org/WAI/standards-guidelines/wcag/)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "If users’ consent is required, is the necessary information provided in a clear and accessible way?",
				"threatif": "No",
				"label": "Consent",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Is the consent information presented in a way that is easy for users to access and understand? \n* Do you need to create a dedicated place to display consent information, especially in contexts where a traditional text interface is not available (e.g., voice-based systems or robots)?",
				"recommendation": "* As part of privacy compliance you need to provide clear information about the processing and the logic of the algorithm. This information should be easily readable and accessible. During the design phase consider when and how you are going to provide this information.\n* Implement accessibility best practices.",
				"sources": "[A Proposal of Accessibility Guidelines for Human-Robot Interaction](https://www.mdpi.com/2079-9292/10/5/561/htm)\n[ISO/IEC 40500:2012 Information technology — W3C Web Content Accessibility Guidelines (WCAG) 2.0](https://www.iso.org/standard/58625.html)\n[ISO/IEC GUIDE 71:2001 Guidelines for standards developers to address the needs of older persons and persons with disabilities](https://www.iso.org/standard/33987.html)\n[ISO 9241-171:2008(en) Ergonomics of human-system interaction](https://www.iso.org/obp/ui/#iso:std:iso:9241:-171:ed-1:v1:en)\n[Mandate 376 Standards EU](https://ec.europa.eu/growth/tools-databases/mandates/index.cfm?fuseaction=search.detail&id=333)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the user perceive the message from the AI system in a different way than intended?",
				"threatif": "Yes",
				"label": "Perception",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Does the user’s perception of the information match the intended meaning?\n* The way AI communicates, tone, language, and context, can lead to misinterpretation, influenced by factors like cultural background or prior experiences.",
				"recommendation": "* Understanding who is going to interact with the AI system can help to make the interaction more effective. Identify your different user groups.\n* Involve communication experts and do enough user testing to reduce the gap between the intended and the perceived meaning.",
				"sources": "[The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations](https://arxiv.org/pdf/2107.13509.pdf)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is the AI system easy for users to learn and operate?",
				"threatif": "No",
				"label": "Learning Curve",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Does the system require a minimum level of AI literacy to be used? \n* Could a steep learning curve lead to misuse or harm?\n* How quickly are users expected to learn how to use the product?\n* Do you ensure that users have access to the necessary learning materials needed to be able to use the system?\n* Difficulties in learning how the system works could also bring the users in danger and have consequences for the reputation of the product or organisation.",
				"recommendation": "* You can provide assistance, appropriate training material and disclaimers to users on how to adequately use the system.\n* The words and language used in the interface, the complexity and lack of accessibility of some features could exclude people from using the application. Consider making changes in the design of the product where necessary.\n* Consider this also when children are possible users.",
				"sources": "[AI Act, Article 4]( https://artificialintelligenceact.eu/article/4/)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are users clearly made aware that they are interacting with an AI system or consuming AI-generated content?",
				"threatif": "No",
				"label": "AI Interaction Awareness",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Users must be clearly informed when they are interacting with an AI system, especially in conversational interfaces, automated decision systems, or content generation tools. Failing to do so can undermine user trust, autonomy, and informed consent.\n* This includes both real-time interactions (e.g., chatbots) and offline consumption of AI-generated content (e.g., synthetic images, deepfakes).\n* Deepfakes and other AI-generated media that imitate real individuals or events carry high risks of deception, manipulation, and reputational harm if not transparently disclosed.\n* Lack of disclosure may also breach Article 50 of the EU AI Act and broader transparency obligations under the GDPR.",
				"recommendation": "* Inform users at the start of any interaction that they are engaging with an AI system, especially in systems simulating human communication (e.g., chatbots, virtual assistants).\n* For generative AI outputs (text, audio, video, images), ensure they are clearly marked, both visibly and in machine-readable format, as artificially generated or manipulated.\n* If your system produces deepfakes or synthetic media, implement persistent and tamper-resistant labeling or watermarks and include a notice that the content has been artificially generated or altered.\n* Deployers must also inform users when emotion recognition, biometric categorization, or similar AI functions are in use.\n* Design your UX to surface these disclosures prominently and accessibly, particularly in sensitive contexts such as news, education, or political speech.",
				"sources": "Article 50 AI Act",
				"qr": "",
				"categories": [
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are users informed about the AI system's reliability, limitations, and risks in a way that enables safe and effective use?",
				"threatif": "No",
				"label": "System Transparency for Effective Use",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Users need to understand what the AI system can and cannot do, including its intended use, reliability, limitations, and potential risks. Without clear communication, users may place unwarranted trust in the system, misuse it, or be harmed by misleading outputs. This undermines transparency, fairness, safety, and user autonomy. For example, failing to disclose error rates, decision logic, or appropriate use contexts can lead to over-reliance or unsafe behavior, especially in sensitive domains.",
				"recommendation": "* Clearly communicate the system's intended use, benefits, limitations, and potential risks.\n* Provide timely, accessible information on accuracy levels, error rates, interpretability, and system updates.\n* Ensure users understand when and how to rely on the system, and when human judgment is needed.\n* Use interpretability tools appropriate to the impact of the system, especially if it is a black-box model.\n* Follow accessibility best practices to ensure all users, including those with disabilities, can understand the system.\n* Incorporate feedback loops such as surveys to verify that users actually understand how the system works and what they can expect.\n* Consider this part of compliance with the GDPR transparency principle, and good practice for system safety and usability.",
				"sources": "[GDPR transparency principle](https://gdpr-info.eu/recitals/no-58/)\n[Microsoft HAX Toolkit](https://www.microsoft.com/en-us/haxtoolkit/guideline/convey-the-consequences-of-user-actions/)",
				"qr": "",
				"categories": [
					"Transparency & Accessibility",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Privacy & Data Protection",
		"id": 3,
		"colour": "94cfbd",
		"cards": [
			{
				"question": "Can the training data be linked to individuals?",
				"threatif": "Yes",
				"label": "Linkability",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Do you need to use unique identifiers in your training or fine-tuning dataset? If personal data is not necessary for the model you would not really have a legal justification for using it.\n* Training datasets for LLMs may inadvertently include personal data, leading to potential privacy breaches. Even if direct identifiers are removed, indirect identifiers or quasi-identifiers can still enable re-identification. This poses risks under data protection regulations like the GDPR, especially if the data subjects have not provided explicit consent for their data to be used in this manner.",
				"recommendation": "* Unique identifiers might be included in the training set when you want to be able to link the results to individuals. Consider using pseudo-identifiers or other robust pseudonymization techniques that can help you protect personal data.\n* Document the measures you are taking to protect the data. Consider if your measures are necessary and proportional.",
				"sources": "[EDPB AI Privacy Risks & Mitigations – Large Language Models (LLMs)](https://www.edpb.europa.eu/system/files/2025-04/ai-privacy-risks-and-mitigations-in-llms.pdf)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system infer and reveal information that a person has not explicitly shared?",
				"threatif": "Yes",
				"label": "Information Disclosure",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* How can you make sure the product doesn’t inadvertently disclose sensitive or private information during use (e.g., indirectly inferring location, behaviour or connection between digital and physical identity of users)?\n* Could movements or actions be revealed through data aggregation?",
				"recommendation": "* Be careful when making data public that you think is anonymised. Location data and routes can sometimes be de-anonymised (e.g. users of a running app disclosing location by showing heatmap).\n* It is also important to offer privacy by default: offer the privacy settings by default at the maximum protection level. Let the users change the settings after having offered them clear information about the consequences of reducing the privacy levels.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could geolocation restrictions or regional regulations impact the implementation of our AI system in other countries?",
				"threatif": "Yes",
				"label": "Local Restrictions",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI systems often process sensitive data, including personal or location-based information, which may be subject to regional data sovereignty laws and ethical restrictions. Additionally, certain countries may restrict the deployment of AI technologies based on local regulatory frameworks, ethical concerns, or national security considerations. This could limit the usage of your product in those regions.",
				"recommendation": "* Stay informed about the evolving regulatory landscape for AI, including data sovereignty, privacy laws, and ethical standards in different countries. Engage legal and compliance experts to assess restrictions in your target markets.\n* Consider designing your AI system with adaptability for regional requirements, such as geofencing, localized processing, or compliance with specific regulations (e.g., GDPR, AI Act, CCPA).\n* Monitor new AI-related regulations and international agreements to proactively address potential restrictions or adapt your system to comply with local laws.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we minimize the amount of personal data used while preserving model performance?",
				"threatif": "No",
				"label": "Data Minimization",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "The principle of data minimization, as outlined in the General Data Protection Regulation (GDPR) and reflected in many global privacy standards, requires that only data necessary for achieving the system's purpose is collected and processed. However, reducing data too much can sometimes negatively impact the accuracy and performance of AI models, leading to critical or damaging consequences. Balancing regulatory compliance with operational effectiveness is essential to avoid undermining the model's reliability while adhering to privacy principles.",
				"recommendation": "* Achieve data minimization by starting with a smaller dataset and iteratively adding data only as needed, based on observed performance improvements, to justify why additional data is necessary.\n* Use high-quality data to reduce the need for large datasets while ensuring sufficient diversity and representativeness for your model.\n* Apply advanced privacy-preserving techniques such as pseudonymization, perturbation, differential privacy, federated learning, or synthetic data generation to comply with privacy regulations while using larger datasets.\n* Collaborate with experts to select the minimum set of features needed, ensuring relevance to the objective and avoiding issues like the Curse of Dimensionality, which can degrade model performance when unnecessary features are included.",
				"sources": "Page 13 [Artificial Intelligence and Data Protection How the GDPR Regulates AI](https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl-hunton_andrews_kurth_legal_note_-_how_gdpr_regulates_ai__12_march_2020_.pdf)\n[Data Minimization for GDPR Compliance in Machine Learning Models](https://arxiv.org/pdf/2008.04113.pdf): Methods like the one proposed in this paper can inspire you to find a way to mitigate the accuracy risk. They show how to reduce the amount of personal data needed to perform predictions, by removing or generalizing some of the input features.\nThe answer to this post also contains information about this problems in different models: [Does Dimensionality curse effect some models more than others?](https://stats.stackexchange.com/questions/186184/does-dimensionality-curse-effect-some-models-more-than-others)\n[Towards Breaking the Curse of Dimensionality for High-Dimensional Privacy](https://epubs.siam.org/doi/epdf/10.1137/1.9781611973440.84)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Are we processing special categories of personal data or sensitive data?",
				"threatif": "Yes",
				"label": "Sensitive Data",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* According to art. 9 GDPR you might not be allowed to process, under certain circumstances, personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health data or data concerning a person’s sex life or sexual orientation.\n* You might be processing sensitive data if the model includes features that are correlated with these protected characteristics (these are called proxies) but also if you are processing other type of data that, if disclosed, could cause harm (e.g., financial data)",
				"recommendation": "* If you need to use special categories of data as defined in the GDPR art. 9, then you need to check if you have the right lawful basis to do this.\n* Applying techniques like anonymisation might still not justify the fact that you first need to process the original data. Check with your privacy/legal experts.\n* Prevent proxies that could infer sensitive data (especially from vulnerable populations).\n* Check whether historical data or practices may introduce bias.\n* Identify and remove features that are correlated to sensitive characteristics.\n* Use available methods to test for fairness with respect to different affected groups.",
				"sources": "[AI Fairness 360](https://aif360.mybluemix.net)\n[What-if Tool: Playing with AI Fairness](https://pair-code.github.io/what-if-tool/ai-fairness.html)\n[Hunting for Discriminatory Proxies in Linear Regression Models](https://papers.nips.cc/paper/7708-hunting-for-discriminatory-proxies-in-linear-regression-models.pdf)\n[Differential Privacy Blog Series](https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/dp-blog)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Could the AI system make decisions with legal or similarly significant effects without human intervention?",
				"threatif": "Yes",
				"label": "Automated Decision-Making (ADM)",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems that make decisions without human oversight may fall under GDPR Article 22, which restricts significant automated decisions unless specific safeguards are in place. These decisions can affect individuals’ rights, legal status, or access to services.\n* Additionally, Article 86 of the AI Act requires transparency and the provision of clear explanations for significant decisions made by high-risk AI systems.",
				"recommendation": "* Consult privacy and legal experts to determine whether your system qualifies under Article 22 of the GDPR.\n* Implement mechanisms for human intervention, contestability, and explanation. Article 22(3) of the GDPR provides individuals with the right to obtain human intervention in automated decisions and the right to contest such decisions.\n* Align with the EU AI Act's oversight and transparency requirements.\n* Ensure that impacted users are informed of their rights and can seek human review.\n* Maintain documentation of decision logic, oversight processes, and risk mitigation strategies.",
				"sources": "GDPR, AI Act, [Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Accountability & Human Oversight",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Do we have a valid legal basis for processing personal data?",
				"threatif": "No",
				"label": "Lawful Basis",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Do you know which GDPR legal ground you can apply?\n* (a) Consent: the individual has given clear consent for you to process their personal data for a specific purpose.\n* (b) Contract: the processing is necessary for a contract you have with the individual, or because they have asked you to take specific steps before entering into a contract.\n* (c) Legal obligation: the processing is necessary for you to comply with the law (not including contractual obligations).\n* (d) Vital interests: the processing is necessary to protect someone’s life.\n* (e) Public task: the processing is necessary for you to perform a task in the public interest or for your official functions, and the task or function has a clear basis in law.\n* (f) Legitimate interests: the processing is necessary for your legitimate interests or the legitimate interests of a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the individual which require protection of personal data, in particular where the individual is a child. (This cannot apply if you are a public authority processing data to perform your official tasks.)",
				"recommendation": "* In the case of the GDPR you need to be able to apply one of the six available legal grounds for processing the data (art. 6). \n* Check with your privacy expert, not being able to apply one of the legal grounds could bring the project in danger.\n* Take into account that other laws besides the GDPR may also apply.",
				"sources": "[Lawful basis for processing](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/lawful-basis-for-processing/)\n[Artificial Intelligence and Data Protection How the GDPR Regulates AI](https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl-hunton_andrews_kurth_legal_note_-_how_gdpr_regulates_ai__12_march_2020_.pdf)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could we be using personal data for purposes different from those for which it was originally collected?",
				"threatif": "Yes",
				"label": "Purpose Limitation",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "The principle of purpose limitation, as defined in the General Data Protection Regulation (GDPR) and echoed in many global privacy frameworks, requires that personal data is collected for specified, explicit, and legitimate purposes and not further processed in a way incompatible with those purposes. Data repurposing is a significant challenge when applying this principle. If datasets were originally collected for a different purpose, their reuse without proper consent or legal justification may violate privacy regulations and ethical standards.",
				"recommendation": "* Consult with your privacy officer or legal team to verify the original purpose of the data collection and evaluate any constraints or legal requirements. \n* If data repurposing is necessary, consider obtaining additional consent, performing a legitimate interest assessment, or applying anonymization techniques to ensure compliance. \n* Additionally, document all decisions and justifications for data reuse to demonstrate accountability under privacy regulations.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we able to comply with all the applicable GDPR data subjects’ rights?",
				"threatif": "No",
				"label": "Data Subject Rights",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Can you implement the right to withdraw consent, the right to object to the processing and the right to be forgotten into the development of the AI system?\n* Can you provide individuals with access and a way to rectify their data?",
				"recommendation": "* Complying with these provisions from the GDPR (art. 15-21) could have an impact on the design of your product. What if users withdraw their consent? Do you need to delete their data used to train the model? What if users can no longer be identified in the dataset? And what information should the users have access to?\n* Consider all these possible scenarios and involve your privacy experts early in the design phase.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could we be deploying the AI system without conducting a required Data Protection Impact Assessment (DPIA)?",
				"threatif": "Yes",
				"label": "Privacy Impact Assessment",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "The use of AI is more likely to trigger the requirement for a DPIA, based on criteria in Article 35 GDPR. The GDPR and the EDPB’s Guidelines on DPIAs identify both “new technologies” and the type of automated decision-making that produce legal effects or similarly significantly affect persons as likely to result in a “high risk to the rights and freedoms of natural persons”.",
				"recommendation": "* This threat modeling library can help you to assess possible risks.\n* Remember that a DPIA is not a piece of paper that needs to be done once the product is in production. The DPIA starts in the design phase by finding and assessing risks, documenting them and taking the necessary actions to create a responsible product from day one until it is finalized.\n* Consider the time and resources that you might need for the execution of a DPIA, as it could have some impact on your project deadlines.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy"
				]
			},
			{
				"question": "Are we using third-party providers while processing data from children or other vulnerable individuals?",
				"threatif": "Yes",
				"label": "Third-party Data Processing",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "If your system processes data from children or other vulnerable groups, any third-party providers you rely on (such as libraries, SDKs, or other tools) may also have access to this data. In such cases, you must ensure they comply with relevant privacy regulations like GDPR, COPPA, or similar frameworks. Even if your own system adheres to strong data protection measures, vulnerabilities or non-compliance on the part of third-party providers could expose sensitive data or create ethical risks.",
				"recommendation": "* Audit all third-party applications, libraries, and tools you use to determine what data they collect and ensure they comply with applicable regulations.\n* Confirm that proper agreements (e.g., Data Processing Agreements) are in place with all third-party providers to specify how data is handled.\n* Where possible, configure third-party tools to limit or avoid sharing sensitive data. Implement pseudonymization or anonymization techniques to protect data before sharing.\n* Evaluate the necessity of each third-party provider. If risks are identified, consider replacing or discontinuing use of certain providers, weighing the operational impact on your organization.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Monitor"
				]
			},
			{
				"question": "Are we using metadata that could reveal personal data or behavior patterns?",
				"threatif": "Yes",
				"label": "Metadata",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Metadata provides descriptive attributes of other data, such as date, time, author, file size, or geolocation.\n* Although metadata may seem innocuous, it is often considered personal data under privacy regulations (e.g., GDPR) and can contain sensitive information. Misusing or failing to protect metadata can lead to privacy violations and unintended risks, especially if it reveals identifiable information.",
				"recommendation": "* Ensure that your use of metadata complies with applicable privacy regulations by verifying whether the data can be lawfully processed for your intended purpose.\n* Audit and verify metadata sources to confirm their accuracy and legitimacy.\n* Implement anonymization or pseudonymization techniques to minimize privacy risks while using metadata.\n* Limit the collection of metadata to only what is strictly necessary for the model, adhering to the principle of data minimization.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Could we compromise users’ rights to privacy and to a private and family life?",
				"threatif": "Yes",
				"label": "Privacy Rights",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "The AI system may intrude on users' right to privacy by exposing sensitive aspects of their private lives, such as personal behaviors, preferences, or relationships, without their explicit consent or awareness. This can occur through excessive surveillance, unintended inferences, profiling, or sharing personal data without proper safeguards. Such compromises may undermine users' autonomy, dignity, and trust in the system, leading to legal, ethical, and reputational consequences for providers.",
				"recommendation": "* Ensure that the AI system respects the contextual integrity of users' private lives by limiting inferences and decisions to what is strictly necessary for its intended purpose. \n* Minimize the risk of profiling that could reveal sensitive personal attributes or behaviors unless explicitly justified by the intended use and supported by users’ consent or legal ground. \n* Design the AI system to avoid unnecessary observation or analysis of users’ private spaces, behaviors, or communications unless explicitly required by the use case. \n* Provide clear and accessible information to users about the extent and nature of the AI system's interaction with their private lives, ensuring that they are fully informed about its capabilities. \n* Empower users to set boundaries for their privacy by allowing them to control the scope of data collection and interaction with the AI system (Privacy by default). \n* Include ethical reviews and stakeholder consultations to assess the potential implications of the system on users’ privacy in diverse cultural and social contexts. \n* Implement safeguards to prevent the system from drawing unintended, intrusive, or harmful conclusions about individuals’ private lives. \n* Ensure robust security measures to prevent unauthorized access, surveillance, or other misuse of the system that could violate users’ privacy rights. \n* Provide mechanisms for users to report and address concerns if they feel their privacy has been violated, including remedies for potential harm caused.",
				"sources": "Right to privacy (Universal Declaration of Human Rights), Article 7 Respect for Private and Family Life (Charter of fundamental rights of the European Union)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Are we providing sufficient transparency about how the AI model collects, processes, and uses personal data?",
				"threatif": "No",
				"label": "Transparent Information",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Users and stakeholders may not fully understand how data is collected, processed, and utilized, leading to concerns about privacy, accountability, and trust. A lack of transparency can make it difficult to verify whether personal data is being used lawfully or ethically. AI decision-making may be opaque, increasing risks of bias, discrimination, or unfair outcomes.",
				"recommendation": "* Implement explainability tools that provide insights into AI decision-making.\n* Use clear and accessible documentation detailing data collection, storage, processing, and sharing.\n* Follow transparency principles from the EU AI Act and GDPR regarding automated decision-making.\n* Utilize model cards, data sheets, and algorithmic auditing to enhance transparency.",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Are we logging or storing user input data in ways that may violate privacy?",
				"threatif": "Yes",
				"label": "Storing of User Data",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI systems, particularly Large Language Models (LLMs), may log user inputs and outputs for debugging or model fine-tuning, potentially storing sensitive data without explicit user consent. Logged data could be included in training datasets, making it possible for adversaries to conduct data poisoning attacks, influencing model behavior. Even metadata from logs may reveal sensitive details about users.",
				"recommendation": "* Implement strict access controls and data minimization techniques to prevent excessive logging.\n* Provide opt-in or opt-out options for data collection and obtain explicit consent where needed.\n* Regularly audit and delete logs containing personal or sensitive data.\n* Use differential privacy, encryption, or synthetic data to minimize risks while analyzing logs.\n* Detect and mitigate adversarial attacks aimed at poisoning training data.",
				"sources": "[WiP: An On-device LLM-based Approach to Query Privacy Protection](https://dl.acm.org/doi/10.1145/3662006.3662060)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system produce inaccurate or misleading outputs that result in privacy violations or harm?",
				"threatif": "Yes",
				"label": "Inaccurate Output",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems may generate, infer, or reproduce incorrect personal data, leading to violations of the GDPR’s accuracy principle (Article 5(1)(d)) and potential harm to data subjects.\n* Outputs may inadvertently reveal sensitive data or personal details, leading to privacy breaches.\n* In traditional AI, this includes misclassification, profiling errors, or incorrect risk scoring that affect decisions about individuals (e.g., in hiring, finance, law enforcement).\n* In generative AI, this includes hallucinated personal facts or fabricated content that falsely attributes actions, identities, or characteristics to real people. When multiple AI agents interact, hallucinations and errors can amplify, increasing the likelihood of spreading misinformation.\n* These inaccuracies can damage reputations, mislead users, or be stored and processed in downstream systems, compounding the data protection risk.",
				"recommendation": "* Conduct data protection impact assessments (DPIAs) to evaluate how inaccurate outputs could affect individuals' rights and freedoms.\n* Provide mechanisms for individuals to access, rectify, or contest inferences or decisions made by AI systems.\n* Maintain logs and audit trails to trace how inaccurate personal data was generated or propagated.\n* Train models with high-quality, up-to-date, and verified datasets to minimize the risk of misinformation and outdated personal data \n\nFor generative AI:\n* Implement fact-checking and validation mechanisms before AI-generated responses are shown to users.\n* Implement named-entity detection and filtering to prevent false personal information from being output\n* Use retrieval-augmented generation (RAG) and human-in-the-loop (HITL) approaches to improve accuracy.\n* Red-team AI models by stress-testing them for misinformation and privacy risks.\n* Allow users to report inaccurate or harmful content, enabling iterative model improvements.\n* Restrict AI-generated outputs on sensitive topics unless rigorous verification is in place.\n\nFor traditional AI (e.g., classification, regression, or rule-based systems):\n* Validate models on diverse, real-world datasets to test for generalizability and edge-case failures.\n* Implement post-deployment performance monitoring and regular retraining to reduce drift and degradation over time.\n* Conduct error analysis on false positives and false negatives to refine model logic and thresholds.\n* Include uncertainty estimation and confidence scoring to guide decision-making, especially in high-risk use cases.\n* In safety-critical applications, ensure fallback mechanisms or manual review paths are available when confidence is low.",
				"sources": "[Hallucination Detection in Large Language Models with Metamorphic Relations](https://arxiv.org/abs/2502.15844)\n[Unraveling Large Language Model Hallucinations](https://towardsdatascience.com/unraveling-large-language-model-hallucinations/)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Are we transferring personal data to countries that lack adequate privacy protections?",
				"threatif": "Yes",
				"label": "Data Transfers",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems may store or process data in countries with weaker privacy protections, leading to GDPR violations.\n* Transfers outside the EU/EEA may violate: GDPR Art. 44-46 restricting international data transfers without adequate safeguards.\n* If personal data is processed in non-compliant jurisdictions, organizations face legal, financial, and reputational risks.",
				"recommendation": "* Conduct a Data Transfer Impact Assessment (DTIA) before processing data outside GDPR-compliant regions.\n* Use Standard Contractual Clauses (SCCs), Binding Corporate Rules (BCRs), or adequacy decisions when transferring data.\n* Store and process personal data in localized environments to comply with data sovereignty laws.\n* Implement encryption and anonymization before data is transferred across jurisdictions.\n* Continuously monitor regulatory updates to ensure ongoing compliance with global privacy laws.",
				"sources": "[International dimension of data protection](https://commission.europa.eu/law/law-topic/data-protection/international-dimension-data-protection_en)\n[Court of Justice Schrems II](https://curia.europa.eu/jcms/upload/docs/application/pdf/2020-07/cp200091en.pdf)",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can we comply with the storage limitation principle and international data retention regulations?",
				"threatif": "No",
				"label": "Storage Limitation",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "The principle of storage limitation, as stated in Article 5(e) of the GDPR, requires personal data to be stored only as long as necessary for the intended purpose. Similarly, many global privacy regulations, such as CCPA (California), LGPD (Brazil), and PDPB (India), impose strict rules on data retention and deletion. Do you have a clear understanding of how long you need to keep the data (training data, output data, etc.) and whether you comply with internal, local, national, or international retention requirements?",
				"recommendation": "* Personal data must not be stored longer than necessary for its intended purpose. Compliance requires a clear understanding of the data flow throughout the model’s lifecycle.\n* Analyze all data types, including raw input data, training and testing sets, processed outputs (linked or merged data), and associated metrics. Understand where this data will be stored and for how long.\n* Define clear retention and deletion schedules, ensuring responsible individuals are assigned for managing data retention and disposal.\n* If data must be retained for auditing or quality purposes, anonymize it where possible to minimize privacy risks.\n* Stay informed about and comply with retention rules not only under GDPR but also under international frameworks such as CCPA (California Consumer Privacy Act), LGPD (Brazilian General Data Protection Law), and others. Retention and deletion policies should meet these diverse requirements. \n* Be aware that deleting data from a trained model is inherently challenging, as input data influences the model's internal representation during training. Consider legal implications for the model itself, as encoded thresholds and weights may also be subject to retention laws. Source: [BerryvilleiML](https://berryvilleiml.com/)",
				"sources": "",
				"qr": "",
				"categories": [
					"Privacy & Data Protection",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Cybersecurity",
		"id": 4,
		"colour": "bdd895",
		"cards": [
			{
				"question": "Could we be deploying the AI system without testing for adversarial robustness and systemic vulnerabilities?",
				"threatif": "Yes",
				"label": "Security Testing",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI systems can be targeted in unique ways, such as adversarial inputs, poisoning attacks, or reverse-engineering of model outputs. These threats could compromise the system's confidentiality, integrity, and availability, leading to reputational damage or harm to users. Testing for these issues may require specialized expertise, tools, and time, which could affect project timelines.",
				"recommendation": "Plan for AI-specific penetration testing or red-teaming exercises, focusing on adversarial robustness, data governance, and model-specific vulnerabilities. Allocate time in the project for external audits, agreement on scope, and retesting if vulnerabilities are found.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Are our AI inference APIs and function-calling interfaces securely implemented?",
				"threatif": "No",
				"label": "API & Model Interface Security",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI systems increasingly rely on APIs for inference (e.g., LLM endpoints), orchestration (e.g., function calls via tools), or dynamic prompt injection (e.g., Model Context Protocol). Poorly secured APIs expose attack surfaces specific to LLMs and other AI models.\n\nThreats include:\n* Prompt injection via API inputs or user tool outputs (e.g., using MCP-style interfaces).\n* Malicious function calls that exploit insecure tool execution pipelines.\n* Abuse of structured output endpoints (e.g., JSON-formatted APIs) to extract or manipulate model behavior.\n* Reverse-engineering model behavior via inference chaining or output probing.\n\nAttacks on shared foundational model APIs can impact multiple downstream applications through shared vulnerabilities, hallucination exploits, or jailbreak discovery.",
				"recommendation": "* Implement security best practices:\n  - Use strong authentication mechanisms such as API keys or OAuth.\n  - Enforce role-based access controls (RBAC) to restrict functionality.\n  - Encrypt data at rest and in transit (TLS).\n  - Validate and sanitize all inputs; apply strict content-type controls.\n  - Use allowlists and structured schemas (e.g., OpenAPI, JSON Schema) to constrain behavior.\n  - Avoid exposing secrets in API calls or payloads.\n* Regularly test APIs for vulnerabilities including injection attacks, improper state management, and rate limit bypasses.\n* Deploy anomaly detection to flag adversarial or abnormal usage patterns.\n* Limit API output granularity to prevent reverse engineering; obfuscate or truncate confidence scores.\n* Monitor and log all API interactions to detect and investigate abuse.\n* Rate-limit and throttle access to prevent enumeration or prompt probing.\n* For LLMs with plugin, function-calling, or Model Context Protocol (MCP) interfaces:\n  - Monitor for prompt injection and abuse chains across tools.\n  - Apply zero-trust design principles to inference and orchestration layers.\n  - Red-team APIs and function interfaces regularly.\n* Collaborate with foundational model providers to validate the security of shared inference APIs and plugin-style architectures.",
				"sources": "[OWASP API Security Project](https://owasp.org/www-project-api-security/)\n[BerryVilleiML](https://berryvilleiml.com/interactive/)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[Lessons From Red Teaming 100 Generative AI Products – Microsoft](https://arxiv.org/abs/2501.07238)\n[MCP Security Exposed: What You Need to Know Now – Palo Alto Networks](https://live.paloaltonetworks.com/t5/community-blogs/mcp-security-exposed-what-you-need-to-know-now/ba-p/1227143)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Are training data, model output, and other sensitive AI assets securely stored?",
				"threatif": "No",
				"label": "Storage Protection",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Is your data stored and managed in a secure way? Think about training data, tables, models, outputs, etc. Do only authorized individuals have access to your data sources?\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "* Implement access control rules.\n* Verify the security of the authentication mechanism (and the system as a whole).\n* Consider the risk when utilizing public/external data sources.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "If the AI system uses randomness, is the source of randomness properly protected?",
				"threatif": "No",
				"label": "Randomness Protection",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Randomness plays an important role in stochastic systems. “Random” generation of dataset partitions may be at risk if the source of randomness is easy to control by an attacker interested in data poisoning.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "Use of cryptographic randomness sources is encouraged. When it comes to machine learning (ML), setting weights and thresholds “randomly” must be done with care. Many pseudo-random number generators (PRNG) are not suitable for use. Improper PRNG loops can degrade system behavior and lead to unpredictable learning. Cryptographic randomness directly intersects with ML when it comes to differential privacy. Using the wrong sort of random number generator can lead to subtle security problems.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Is the AI model suited for processing confidential information?",
				"threatif": "No",
				"label": "Confidential Information",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* There are certain kinds of machine learning (ML) models which actually contain parts of the training data in its raw form within them by design. For example, ‘support vector machines’ (SVMs) and ‘k-nearest neighbours’ (KNN) models contain some of the training data in the model itself.\n* Algorithmic leakage is an issue that should be considered carefully.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "When selecting the algorithm, conduct a thorough analysis to evaluate the risk of algorithmic leakage. For models known to retain training data (e.g., k-nearest neighbors, support vector machines), assess whether sensitive or identifiable information could be exposed through predictions or reverse engineering.\n\n* Perform privacy risk assessments and adversarial testing to detect memorization or data leakage.\n* Use privacy-preserving techniques where appropriate (e.g., differential privacy, data minimization, feature abstraction).\n* Avoid using algorithms prone to leakage when working with sensitive data, or take extra steps to anonymize and sanitize training inputs.\n* Include leakage testing in your model evaluation pipeline, especially for high-risk or regulated domains.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[ICO - How should we assess security and data minimisation in AI?](https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/how-should-we-assess-security-and-data-minimisation-in-ai/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy"
				],
				"cia": [
					"c"
				]
			},
			{
				"question": "Have we implemented safeguards to detect and prevent insider threats to our AI systems?",
				"threatif": "No",
				"label": "Insider Threats",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI designers and developers may deliberately expose data and models for a variety of reasons, e.g. revenge or extortion. Integrity, data confidentiality and trustworthiness are the main impacted security properties. Source: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "* Implement onboarding and offboarding procedures to ensure the trustworthiness of internal and external personnel.\n* Enforce separation of duties and least privilege principle.\n* Enforce the usage of managed devices with appropriate policies and protective software.\n* Implement awareness training.\n* Implement strict access control and audit trail mechanisms.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Have we protected our AI system against model sabotage?",
				"threatif": "No",
				"label": "Model Sabotage",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Model sabotage involves deliberate manipulation or damage to AI systems at any stage, from development to deployment. This can include embedding backdoors, altering model behavior, or exploiting vulnerabilities in training data, third-party tools, or infrastructure.\n* For AI providers: Risks include compromised training datasets, malicious code in open-source libraries, or backdoors introduced during development.\n* For AI deployers: Threats arise from integrating tampered models, using insecure APIs, or applying updates that introduce vulnerabilities.",
				"recommendation": "* Implement strong security measures, including regular audits and penetration testing, to ensure the integrity of models and the platforms hosting them.\n* Assess and monitor the security profile of third-party libraries, tooling, and providers to ensure they are not compromised.\n* Develop and maintain a robust disaster recovery plan with explicit mitigation strategies for model sabotage scenarios.\n* Use model inspection tools to detect backdoors and ensure that the model’s behavior aligns with its intended function.\n* Incorporate supply chain security principles by verifying the authenticity and integrity of the components used in model development and deployment.\n* Maintain strict version control to detect and prevent unauthorized changes to libraries or model artifacts. \n* Implement anomaly detection systems to identify unusual usage patterns that may indicate attempted sabotage or exploitation.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[An Effective and Resilient Backdoor Attack Framework against Deep Neural Networks and Vision Transformers](https://arxiv.org/abs/2412.06149)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Is our AI model resilient to evasion attacks?",
				"threatif": "No",
				"label": "Model Evasion",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Evasion attacks involve modifying the input data to evade detection or classification by the model. These attacks can be used to bypass security systems, such as intrusion detection systems or spam filters. Example: Specific malware is crafted to avoid being flagged by a machine-learning-based antivirus.",
				"recommendation": "* Develop anomaly detection systems to monitor deviations in input distributions and flag suspicious patterns.\n* Integrate robust logging mechanisms to analyze and mitigate the impact of detected attacks.\n* Train models with diverse and adversarial data, including known evasion techniques.\n* Implement ensemble modeling to reduce susceptibility to evasion attacks.\n* Ensure that thresholds and rules are periodically reviewed to adapt to evolving evasion techniques.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Adversarially Robust Malware Detection Using Monotonic Classification](https://people.eecs.berkeley.edu/~daw/papers/monotonic-iwspa18.pdf)\n[Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training](https://arxiv.org/abs/1711.08001)\n[Feature Denoising for Improving Adversarial Robustness](https://arxiv.org/abs/1812.03411)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Are we protected from poisoning attacks?",
				"threatif": "No",
				"label": "Poisoning Attacks",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "In a poisoning attack, the goal of the attacker is to contaminate the training data or the model generated in the training phase, so that predictions on new data will be modified in the testing phase. This attack could also be caused by insiders. Example: in a medical dataset where the goal is to predict the dosage of a medicine using demographic information, researchers introduced malicious samples at 8% poisoning rate, which changed the dosage by 75.06% for half of the patients. \n\n**Other scenarios:**\n* Data tampering: Actors like AI/ML designers and engineers can deliberately or unintentionally manipulate and expose data. Data can also be manipulated during the storage procedure and by means of some processes like feature selection. Besides interfering with model inference, this type of threat can also bring severe discriminatory issues by introducing bias.\nSource: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)\n* An attacker who knows how a raw data filtration scheme is set up may be able to leverage that knowledge into malicious input later in system deployment.\nSource:[BerryVilleiML](https://berryvilleiml.com/interactive/)\n* Adversaries may fine-tune hyper-parameters and thus influence the AI system’s behavior. Hyper-parameters can be a vector for accidental overfitting. In addition, hard to detect changes to hyper-parameters would make an ideal insider attack.\nSource: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "* Define anomaly sensors to look at data distribution on a day to day basis and alert on variations.\n* Measure training data variation on daily basis. Telemetry for skew/drift.\n* Input validation, both sanitization and integrity checking.\n* Implement measures against insider threats.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[Robustness Techniques & Toolkits for Applied AI](https://www.borealisai.com/research-blogs/robustness-techniques-toolkits-applied-ai/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Are we protected from model inversion attacks?",
				"threatif": "No",
				"label": "Model Inversion",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* In a model inversion attack, if attackers already have access to some personal data belonging to specific individuals included in the training data, they can infer further personal information about those same individuals by observing the inputs and outputs of the ML model.\n* In model inversion the private features used in machine learning models can be recovered. This includes reconstructing private training data that the attacker should not have access to. Example: an attacker recover private features used by the model through careful queries.",
				"recommendation": "* Interfaces to models trained with sensitive data need strong access control.\n* Implement rate-limiting on the queries allowed by the model.\n* Implement gates between users/callers and the actual model by performing input validation on all proposed queries, rejecting anything not meeting the model’s definition of input correctness and returning only the minimum amount of information needed to be useful.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c"
				]
			},
			{
				"question": "Are we protected from membership inference attacks?",
				"threatif": "No",
				"label": "Membership Inference",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "In a membership inference attack (MIA), the attacker can determine whether a given data record was part of the model’s training dataset or not. Example: researchers were able to predict a patient’s main procedure (e.g., surgery the patient went through) based on the attributes (e.g., age, gender, hospital).",
				"recommendation": "* Differential Privacy has been shown to be an effective mitigation in some studies.\n* The usage of neuron dropout and model stacking can be effective mitigations to an extent. Using neuron dropout not only increases resilience of a neural net to this attack, but also increases model performance.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c"
				]
			},
			{
				"question": "Are we protected from model stealing attacks?",
				"threatif": "No",
				"label": "Model Stealing",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "In model stealing, the attackers can recreate the underlying model by legitimately querying the model. The functionality of the new model is the same as that of the underlying model. Example: in the BigML case, researchers were able to recover the model used to predict if someone should have a good/bad credit risk using 1,150 queries and within 10 minutes.",
				"recommendation": "* Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to 'honest' applications.\n* Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c"
				]
			},
			{
				"question": "Are we protected from reprogramming deep neural nets attacks?",
				"threatif": "No",
				"label": "DNN Attacks",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Specially crafted queries from an adversary can reprogram machine learning systems to a task that deviates from the creator’s original intent. Example: ImageNet, a system used to classify one of several categories of images was repurposed to count squares.",
				"recommendation": "* Configure a strong client-server mutual authentication and access control to model interfaces.\n* Takedown of the offending accounts.\n* Identify and enforce a service-level agreement for your APIs. Determine the acceptable time-to-fix for an issue once reported and ensure the issue no longer reoccurs after the SLA expires.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Are we protected from adversarial examples?",
				"threatif": "No",
				"label": "Adversarial Examples",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Adversarial examples are a type of evasion attack where malicious inputs are deliberately crafted to mislead AI models. These inputs are minimally modified, often imperceptible to humans, but can cause the model to produce incorrect or harmful predictions. Examples include researchers demonstrating that carefully designed patterns on accessories, like sunglasses, could deceive facial recognition systems into misidentifying individuals. Such examples are particularly problematic in critical domains like healthcare, finance, and security, where incorrect predictions could lead to severe consequences.",
				"recommendation": "* Include adversarial examples in the training data to make models more robust against similar attacks. \n* Apply techniques such as input normalization, noise addition, or image resizing to reduce the impact of adversarial perturbations. \n* Design models with built-in robustness features to detect and counteract adversarial modifications. \n* Use multiple models and aggregate their predictions to make it harder for adversarial examples to deceive all models simultaneously. \n* Develop and apply techniques that mathematically guarantee the model’s resistance to certain adversarial manipulations. \n* Regularly test and monitor the system for new adversarial techniques to stay ahead of potential attacks.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[Attribution-driven Causal Analysis for Detection of Adversarial Examples](https://arxiv.org/abs/1903.05821)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Could third-party AI/ML providers compromise our training data or insert backdoors?",
				"threatif": "Yes",
				"label": "AI Supply Chain Access",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Malicious ML providers could query the model used by a customer and recover the customer’s training data. The training process is either fully or partially outsourced to a malicious third party who wants to provide the user with a trained model that contains a backdoor. Example: researchers showed how a malicious provider presented a backdoored algorithm, wherein the private training data was recovered. They were able to reconstruct faces and texts, given the model alone.",
				"recommendation": "* Research papers demonstrating the viability of this attack indicate Homomorphic Encryption could be an effective mitigation.\n* Train all sensitive models in-house.\n* Catalog training data or ensure it comes from a trusted third party with strong security practices.\n* Threat model the interaction between the MLaaS provider and your own systems.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			},
			{
				"question": "Could the AI system be vulnerable to jailbreak techniques, allowing attackers to bypass safety restrictions?",
				"threatif": "Yes",
				"label": "Jailbreaking",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Attackers can exploit jailbreak techniques to bypass an AI system’s built-in safety constraints, enabling it to generate restricted or harmful content.\n\n- Instruction Manipulation: Attackers can craft prompts that trick AI models into breaking content restrictions by rephrasing or disguising requests.\n- Contextual Exploitation: Some jailbreak techniques work by introducing misleading context that influences the AI’s behavior.\n- Adversarial Fine-Tuning: Attackers can modify AI models or create fine-tuned versions that remove ethical constraints.",
				"recommendation": "* Use reinforcement learning with human feedback (RLHF) to harden AI models against jailbreak exploits.\n* Deploy dynamic prompt filtering to detect and block malicious jailbreak attempts in real-time.\n* Implement multi-layer safety protocols, ensuring that AI models reject unsafe requests consistently.\n* Regularly update safety mechanisms to adapt to emerging jailbreak techniques.\n* Conduct red team assessments to test AI resilience against adversarial jailbreak tactics.",
				"sources": "[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			},
			{
				"question": "Could the AI system be vulnerable to prompt injection attacks, leading to unauthorized access or manipulation?",
				"threatif": "Yes",
				"label": "Prompt Injection",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI models, particularly large language models (LLMs), are susceptible to prompt injection attacks, where adversaries craft inputs designed to override model constraints, extract sensitive data, or manipulate system behavior.\n\n- Meta Prompt Extraction: Attackers can manipulate prompts to reveal system instructions, policies, or proprietary data.\n- Indirect Injection Attacks: If an AI model ingests untrusted external content, such as the contents or names of uploaded files, text from emails, chat inputs, or web pages, attackers can embed hidden prompts or malicious instructions within these elements. These indirect inputs can exploit the model's processing logic to alter its behavior, produce misleading responses, or trigger unauthorized actions, even without direct access to the model's interface.\n- System Command Override: Specially crafted prompts could trick AI models into executing unintended actions or disclosing confidential information.",
				"recommendation": "* Use input validation and sanitization to detect and neutralize malicious prompts.\n* Implement adversarial training to harden the AI against prompt injection attacks.\n* Limit the AI’s ability to access sensitive system instructions or proprietary data through context isolation.\n* Avoid executing model-generated outputs directly without human or automated validation. Treat model output as untrusted data, don't execute it as code or commands.\n* Monitor AI interactions in real-time to detect anomalous behaviors and injection attempts.\n* Regularly test AI models using red teaming to identify and patch vulnerabilities in prompt handling.",
				"sources": "[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[Rethinking Prompt Injection Prevention – Don’t Execute the Data](https://www.linkedin.com/pulse/rethinking-prompt-injection-prevention-dont-execute-data-topgul-mbpoc/)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Privacy & Data Protection",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			},
			{
				"question": "Is the AI training environment secured against unauthorized access and manipulation?",
				"threatif": "No",
				"label": "Environment Unauthorized Access",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "AI training environments often handle sensitive data and require extensive computational resources. If left unprotected, they become a target for adversaries who may attempt to steal data, modify training sets, or inject adversarial inputs.\n\n- Unauthorized Access to Training Data: Malicious actors could exfiltrate sensitive training datasets, leading to data leaks or compliance violations.\n- Model Poisoning & Integrity Attacks: Attackers may inject biased or adversarial data into the training process, leading to degraded or manipulated AI outputs.\n- Infrastructure Vulnerabilities: Misconfigured cloud environments or weak authentication mechanisms could expose training pipelines to external threats.",
				"recommendation": "* Implement strict access controls and role-based permission for training environments.\n* Use end-to-end encryption for training data to prevent unauthorized interception.\n* Deploy secure multi-party computation (SMPC) and homomorphic encryption to protect sensitive datasets.\n* Regularly audit and monitor training infrastructure for security vulnerabilities.\n* Adopt sandboxed environments to isolate training processes and prevent malicious tampering.",
				"sources": "[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Is the deployed AI system protected from unauthorized access and misuse?",
				"threatif": "No",
				"label": "System Unauthorized Access",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Unauthorized access to AI systems can result in data breaches, model theft, and exploitation of sensitive functionalities. Without proper access control, attackers can extract model parameters, manipulate system behavior, or leak confidential data.\n\n- Credential & API Key Exposure: Weak authentication mechanisms can lead to unauthorized access, allowing attackers to exploit API endpoints or modify AI responses.\n- Model Extraction Attacks: Attackers can systematically query an AI system to recreate and steal proprietary models, leading to intellectual property theft.\n- Privilege Escalation Risks: Poorly managed user roles and permissions may allow attackers to escalate access, gaining control over critical AI operations.",
				"recommendation": "* Enforce multi-factor authentication (MFA) and strong password policies for AI system access.\n* Restrict API access using role-based access control (RBAC) and least privilege principles. \n* Monitor AI usage logs for anomalous access patterns and potential security breaches.\n* Apply rate limiting and query monitoring to detect and mitigate model extraction attacks.\n* Use secure enclaves and differential privacy to protect sensitive AI models and training data.",
				"sources": "[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could third-party tools, plugins, or dependencies introduce vulnerabilities in our AI system?",
				"threatif": "Yes",
				"label": "AI Supply Chain Tools",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Modern AI systems increasingly rely on external tools and plugin interfaces (e.g., Model Context Protocol, LangChain, OpenAI plugins) to expand their capabilities. These interfaces pose unique security risks if not tightly controlled.\n\n **Runtime Abuse:** If tool or plugin inputs are not strictly validated, LLMs may:\n* Trigger unauthorized tool executions.\n* Bypass guardrails using structured payloads embedded in plugin responses.\n* Chain outputs across tools in unsafe ways (e.g., generating code that another tool executes).\n\n**Supply Chain Risks:** Third-party plugins and dependencies may contain vulnerabilities or backdoors. Attackers can:\n* Compromise plugin registries or repositories.\n* Hijack dependencies to inject malicious code.\n* Tamper with pre-trained models or updates during distribution.\n\nThese risks are magnified in open ecosystems where tools are crowd-sourced or rapidly integrated without full vetting.",
				"recommendation": "* Use strict schemas (e.g., OpenAPI, JSON Schema) and validate all tool/plugin inputs and outputs.\n* Treat plugin invocations as untrusted: isolate execution, rate-limit usage, and monitor behavior.\n* Maintain allowlists of vetted plugins and restrict file access, external requests, or execution rights.\n* Verify third-party components using cryptographic checksums and signatures.\n* Conduct regular security audits of plugins, model dependencies, and tool chains.\n* Adopt a zero-trust security model around plugin and tool execution to reduce blast radius of compromise.",
				"sources": "[Microsoft, Threat Modelling AI/ML Systems and Dependencies](https://docs.microsoft.com/en-us/security/engineering/threat-modeling-aiml)\n[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could the AI system generate or execute unsafe SQL queries from user input?",
				"threatif": "Yes",
				"label": "Unsafe SQL",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* LLMs integrated with backend systems may generate SQL queries based on user input, exposing the system to SQL injection attacks. If input prompts are not properly validated or sanitized, attackers may inject malicious SQL fragments into natural language inputs, which the LLM translates into executable queries.\n* These vulnerabilities are often underestimated due to misplaced trust in the AI’s output or assumptions that the AI understands secure coding practices. In reality, models may generate insecure or dangerous SQL if prompted accordingly.\n* This risk is particularly severe in domains like finance or healthcare, where AI-generated queries could expose sensitive records or enable privilege escalation.",
				"recommendation": "* Never execute AI-generated SQL directly. Use intermediate layers that validate and parameterize AI-generated queries.\n* Sanitize all user inputs before allowing them to reach the LLM.\n* Apply query allow-lists, parameterized queries, and database permissions to constrain what LLMs can do.\n* Use static and dynamic code analysis on AI-generated queries before execution.\n* Educate developers and product teams about the unique risks of LLM-driven SQL generation.",
				"sources": "[From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?](arXiv:2308.01990)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could the AI system generate or execute unsafe code based on user input?",
				"threatif": "Yes",
				"label": "Remote Code Execution (RCE)",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* LLMs capable of code generation (e.g., math solvers, dev assistants) may be exploited to generate and execute malicious code if user input is not properly isolated.\n* Adversaries can craft prompts that cause the model to generate harmful code, such as importing modules, writing to disk, or leaking environment variables. If this code is executed directly (e.g., in a math or scripting agent), the attacker may achieve Remote Code Execution (RCE).\n* Case studies such as MathGPT demonstrate how seemingly benign capabilities (e.g., formula evaluation) can be weaponized to access server resources or keys.",
				"recommendation": "* Never run AI-generated code in the same environment as your application backend.\n* Use containerization (e.g., Docker) with strict sandboxing, network isolation, and resource limits for code execution.\n* Inspect AI-generated code before execution, and apply static analysis tools to flag dangerous patterns.\n* Implement output sanitization to prevent exfiltration of sensitive data.\n* Disable or severely limit code execution features unless explicitly required.",
				"sources": "[Demystifying RCE Vulnerabilities in LLM-Integrated Apps](https://arxiv.org/abs/2309.02926)\n[I Hacked MathGPT: RCE Vulnerability](https://www.l0z1k.com/hacking-mathgpt/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could autonomous AI agents access or interact with malicious web content?",
				"threatif": "Yes",
				"label": "Agentic AI Interaction",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI agents that browse the internet or invoke external APIs may inadvertently land on or interact with malicious websites. These pages may host malware, deceptive interfaces, or phishing payloads designed to compromise the AI system or extract sensitive data.\n* The risk is amplified when agents operate autonomously or chain multiple tools (e.g., browsers, file downloaders, LLMs) without strict boundaries, potentially triggering harmful scripts or revealing internal state.",
				"recommendation": "* Apply strict domain allow-lists and restrict browsing to pre-approved sources.\n* Disable JavaScript, downloads, or plugin execution in browser environments.\n* Monitor and log all external interactions for anomalous behavior.\n* Use URL and content scanning before any AI agent accesses external resources.\n* Employ a retrieval proxy to intermediate and sanitize third-party web content before it is passed to the agent.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			},
			{
				"question": "Could agent memory be poisoned with malicious or misleading information?",
				"threatif": "Yes",
				"label": "Agentic AI Memory",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Agentic systems with persistent memory can be manipulated over time by injecting false, biased, or adversarial content. This may alter future reasoning, planning, or tool use. For example, a user might insert misleading facts into a chatbot's memory, resulting in hallucinations or dangerous outputs later on.\n* Long-term memory makes these risks cumulative and harder to detect.",
				"recommendation": "* Limit write access to memory: only trusted or validated agents/users should modify persistent memory.\n* Implement memory sanitation, validation, and confidence scoring.\n* Provide mechanisms to audit memory entries and detect unusual patterns.\n* Isolate memory by task or session where feasible to limit long-term contamination.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Could agents misuse tools or APIs they are authorized to access?",
				"threatif": "Yes",
				"label": "Agentic AI Tools Misuse",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Agents that have access to tools (e.g., file systems, webhooks, APIs) may invoke them in unintended or harmful ways. This misuse can result from adversarial prompts, faulty reasoning, or misunderstood intent. Example: an agent with access to a web browser could issue API delete requests or trigger real-world effects in connected systems.",
				"recommendation": "* Use allow-lists to tightly control which tools an agent can access.\n* Apply RBAC or contextual constraints (e.g., only allow file writes for task X).\n* Monitor tool use patterns and block anomalous calls.\n* Require human-in-the-loop confirmation for high-risk tool use.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could hallucinated output from one agent propagate and mislead others in multi-agent systems?",
				"threatif": "Yes",
				"label": "Agentic AI Hallucinations",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "In multi-agent systems, one agent’s hallucinated output can become another’s input. This can cause cascading misinformation, particularly if agents defer to each other’s outputs without validation. Example: Agent A misclassifies a vulnerability, Agent B acts on this and takes inappropriate mitigation actions.",
				"recommendation": "* Require independent validation or confidence scoring for agent-to-agent communication.\n* Avoid blind trust between agents; implement verification protocols to ensure accuracy.\n* Implement mechanisms to trace provenance of information across agents.\n* Regularly retrain agents on hallucination-resistant architectures and factual QA tasks.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i"
				]
			},
			{
				"question": "Can we trace and audit the actions and decisions of autonomous agents in our system?",
				"threatif": "No",
				"label": "Agentic AI Actions Traceability",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Without strong logging and traceability, it becomes difficult to audit or understand decisions made by autonomous agents.\n* This increases the risk of undetected errors, malicious actions, and limits post-incident forensics.\n* Repudiation becomes likely when actions cannot be linked to responsible entities (agent or user).",
				"recommendation": "* Log all agent actions, tool uses, memory writes, and external interactions.\n* Implement immutable audit trails.\n* Assign unique identifiers to agents and their outputs.\n* Use cryptographic signing for sensitive agent actions to support accountability.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Accountability & Human Oversight",
					"Data & Data Governance"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i",
					"a"
				]
			},
			{
				"question": "Could a compromised or malicious agent sabotage a multi-agent system?",
				"threatif": "Yes",
				"label": "Agentic AI Malicious Agent",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* In decentralized or loosely coordinated agentic systems, a single compromised agent can act against the intended goals of the system.\n* Rogue agents may inject misleading information, sabotage coordination, or exploit trust relationships.\n* These threats are especially relevant in federated learning, autonomous swarm systems, or large-scale multi-agent deployments.",
				"recommendation": "* Implement agent authentication and authorization protocols.\n* Monitor agent outputs for inconsistencies or divergence from assigned tasks.\n* Apply anomaly detection to communication and behavior across agents.\n* Quarantine or disable agents that exhibit deviant or suspicious activity.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could an agent gain access to functions or data beyond its intended permissions?",
				"threatif": "Yes",
				"label": "Agentic AI Unauthorized Access",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Agents may discover or exploit misconfigurations to access privileged tools, APIs, or data.\n* Escalation can result from incorrect role assignments, prompt manipulation, or logic flaws in access validation.\n* This could lead to the agent invoking destructive actions or leaking confidential data.",
				"recommendation": "* Apply least privilege principles and context-aware access controls to agent capabilities.\n* Regularly audit role definitions and permissions assigned to agents.\n* Include privilege escalation scenarios in red-teaming and testing efforts.\n* Use runtime guards to detect and block unauthorized function calls.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could an attacker or user intentionally overload the AI system’s resources to degrade performance or cause failures?",
				"threatif": "Yes",
				"label": "Resource Overload",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems, especially large models, are resource-intensive and vulnerable to overload attacks targeting compute, memory, I/O, or API bandwidth.\n* Malicious actors may send high-frequency or computationally expensive queries to exhaust system capacity.\n* This can lead to degraded service, denial of service, or delayed model responses, impacting availability and user trust.\n* Multi-agent environments are particularly vulnerable when agents interact recursively or generate long-running tasks without resource limits.",
				"recommendation": "* Implement rate limiting and quotas per user, agent, or session to restrict excessive usage.\n* Use priority-based scheduling, timeouts, and request throttling for costly model operations.\n* Monitor runtime metrics (CPU/GPU load, memory, inference time) and trigger alerts for anomalies.\n* Apply load balancing and autoscaling in production to absorb usage spikes.\n* Include safeguards in agent instructions to prevent recursive or resource-exhausting task loops.\n* Log resource-heavy requests and investigate patterns indicative of misuse or attack.",
				"sources": "[OWASP Agentic AI Threats](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"a",
					"i"
				]
			},
			{
				"question": "Could an attacker or agent impersonate a user or AI identity to gain unauthorized influence?",
				"threatif": "Yes",
				"label": "Identity Spoofing & Impersonation",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Attackers may spoof legitimate identities of users, agents, or services to bypass authentication and gain control or influence over AI behavior.\n* This can enable unauthorized data access, prompt injection, or manipulation of trust-based systems.",
				"recommendation": "* Use cryptographic signatures or authentication tokens to verify agent identities.\n* Implement mutual authentication in multi-agent or AI-human interaction scenarios.\n* Monitor for identity anomalies such as session hijacking, mismatched tokens, or unexpected behavioral patterns.\n* Log all identity transitions and access attempts to support traceability and forensic analysis.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"i",
					"a"
				]
			},
			{
				"question": "Could an agent be misused to manipulate or deceive users?",
				"threatif": "Yes",
				"label": "Agentic AI Deceiving Users",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Agentic systems capable of persuasive language or personalized interaction can be manipulated to influence human decisions, emotions, or behaviors.\n* This creates risks of social engineering, phishing, misinformation, or undue influence, especially if the agent mimics authority figures or trusted personas.\n* The risk is amplified when agents use persistent memory or learn user preferences over time.",
				"recommendation": "* Impose ethical use constraints and define red lines (e.g., no impersonation, no medical/legal advice without oversight).\n* Use transparency mechanisms to disclose when users are interacting with agents.\n* Enable user control and opt-out of persuasive or adaptive behaviors.\n* Monitor for behavior that resembles coercion, manipulation, or impersonation.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Ethics & Human Rights"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			},
			{
				"question": "Could an attacker intercept or manipulate communications between agents to alter system behavior?",
				"threatif": "Yes",
				"label": "Agent Communication Tampering",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Agents that exchange messages may be vulnerable to communication poisoning, where an attacker injects or modifies messages to alter system behavior. This can mislead agents, propagate misinformation, or trigger unintended actions in chained workflows. Examples include impersonating an agent, sending conflicting commands, or embedding adversarial prompts.",
				"recommendation": "* Authenticate all agent-to-agent messages.\n* Use encryption and integrity checks to prevent tampering.\n* Log and analyze communication flows to detect unusual patterns.\n* Limit what kinds of messages agents can send and which agents can receive them.",
				"sources": "[OWASP Agentic AI – Threats and Mitigations (v1.0.1)](https://owasp.org/www-project-agentic-ai/assets/publications/Agentic-AI-Threats-and-Mitigations_v1.0.1.pdf)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			},
			{
				"question": "Could unsafe file uploads introduce security risks?",
				"threatif": "Yes",
				"label": "File Upload",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI systems that ingest or process uploaded files, such as PDFs, Word documents, images, or code, are vulnerable to multiple attack vectors:\n\n* Malware & Embedded Scripts: Uploaded files may contain malicious payloads, macros, or hidden code that executes during parsing or rendering.\n* Indirect Prompt Injection: Hidden instructions embedded in file content can manipulate LLM behavior when the content is passed as input for summarization, Q&A, or reasoning.\n* Malformed Files & Deserialization: Crafted file formats can trigger crashes or bypass input validation, potentially leading to remote code execution or model corruption.\n\nThese threats are particularly relevant when files are processed automatically by LLMs or downstream tools, often without human review.",
				"recommendation": "* Validate file types, sizes, and content strictly, use allowlists and reject unsupported or dangerous formats.\n* Sanitize and normalize file content before passing it to downstream components or LLMs.\n* Scan all files for malware using antivirus and static analysis tools.\n* Avoid feeding raw file content directly to language models, wrap it with safety context and monitor outputs.\n* Use sandboxed or containerized environments for file parsing, summarization, or code execution.\n* Monitor for patterns of indirect prompt injection in document content.\n* If supporting file-based inputs in a RAG pipeline or agentic system, implement retrieval sanitation and memory protection.",
				"sources": "[OWASP - Unrestricted File Upload](https://owasp.org/www-community/vulnerabilities/Unrestricted_File_Upload)\n[ProtectAI - ModelScan](https://protectai.com/modelscan)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could unsafe deserialization of model artifacts lead to code execution or system compromise?",
				"threatif": "Yes",
				"label": "Model Serialization",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Models are serialized and transferred between systems for deployment, a stage vulnerable to model serialization attacks. Models are often serialized for storage, sharing, or deployment, using formats like `pickle`, `joblib`, `ONNX`, or TensorFlow SavedModel. However, many serialization formats can embed executable code or unsafe object structures.\n\nIf an attacker tampers with a serialized model artifact and it is later deserialized without validation, they may achieve:\n* Remote Code Execution (RCE) during deserialization.\n* Privilege escalation or lateral movement inside the deployment environment.\n* Tampering with model behavior (e.g., inserting a backdoor or triggering silent failures).\n\nThese risks are especially severe when models are downloaded from untrusted sources, integrated via ML pipelines, or auto-loaded during CI/CD processes.",
				"recommendation": "* Avoid unsafe deserialization methods on untrusted inputs, prefer safer formats.\n* Use model scanning tools to detect malicious payloads in serialized artifacts.\n* Enforce cryptographic signing and integrity checks for all model files before deployment.\n* Store and transport models using secure channels (e.g., signed, encrypted artifact registries).\n* Load models only in sandboxed or containerized environments with minimal privileges and no internet access.\n* Track model provenance throughout the development lifecycle to detect unauthorized changes.",
				"sources": "[OWASP - Unrestricted File Upload](https://owasp.org/www-community/vulnerabilities/Unrestricted_File_Upload)\n[ProtectAI - ModelScan](https://protectai.com/modelscan)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Could malicious fine-tuning compromise the safety or alignment of our GenAI model?",
				"threatif": "Yes",
				"label": "Fine-tuning Attacks",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "* Adversaries can fine-tune or subtly manipulate your LLM using harmful data, leading to unsafe, biased, or deceptive behaviors.\n* Common fine-tuning attacks include:\n  - **Instruction Manipulation**: Injects unsafe instructions into fine-tuning data, teaching the model to follow harmful prompts.\n  - **Output Manipulation**: Poisons target outputs in the fine-tuning data, causing the model to generate malicious or biased responses, even when prompts seem neutral.\n  - **Backdoor Attacks**: Implant hidden triggers during fine-tuning that activate malicious behavior only when specific input patterns appear. The model behaves normally otherwise, making these attacks hard to detect.\n  - **Alignment Degradation**: Subtly erodes the model’s safety alignment during fine-tuning, making it gradually more permissive to unsafe behavior without explicit instructions.\n  - **Reward Hijacking**: Tricks the reward model into preferring harmful outputs, effectively training the model to give unsafe or misleading responses.\n  - **Semantic Drift**: Slightly alters wording or context in fine-tuning data to shift the model’s behavior, causing it to appear aligned while subtly reinforcing harmful stereotypes or unsafe reasoning.\n* These threats can be introduced via fine-tuning-as-a-service platforms, open-source model reuse, or contaminated user-provided datasets.\n* Even small amounts of harmful fine-tuning data can significantly degrade model alignment and safety.",
				"recommendation": "* Vet and sanitize fine-tuning datasets, including user-submitted data and third-party sources.\n* Implement anomaly detection and alignment regression tests before and after fine-tuning.\n* Restrict or audit fine-tuning privileges, especially on shared infrastructure or open APIs.\n* Use differential privacy, prompt injection detection, and trigger auditing tools to detect backdoors.\n* Conduct red-teaming to assess the effects of adversarial fine-tuning and monitor for misalignment drift over time.",
				"sources": "[Harmful Fine-tuning Attacks and Defenses for Large Language Models (arXiv:2409.18169)](https://arxiv.org/abs/2409.18169)",
				"qr": "",
				"categories": [
					"Cybersecurity",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				],
				"cia": [
					"c",
					"i",
					"a"
				]
			},
			{
				"question": "Are we protected from vulnerabilities in vector databases and RAG pipelines?",
				"threatif": "No",
				"label": "RAG & Vector Databases",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Deployer"
				],
				"explanation": "Retrieval-Augmented Generation (RAG) systems combine LLMs with vector databases to enrich answers with external knowledge. However, if the retrieval layer is compromised or poorly validated, it can feed the model misleading, biased, or adversarial content. Untrusted documents in vector stores can serve as indirect prompt injections, while insecure embeddings can allow unauthorized inference or leakage. Additionally, RAG systems may unintentionally disclose proprietary documents retrieved through similarity search.",
				"recommendation": "* Sanitize retrieved content before feeding it to the LLM.\n* Use document-level access control to prevent unauthorized access during retrieval.\n* Monitor for adversarial inputs and injection attacks embedded in indexed content.\n* Validate the trustworthiness of sources before ingesting documents into the vector DB.\n* Regularly retrain embedding models and limit exposure of semantic search endpoints.",
				"sources": "[OWASP LLM Top 10 - Prompt Injection in Retrieval Systems](https://owasp.org/www-project-top-10-for-large-language-model-applications/)",
				"qr": "",
				"categories": [
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Output",
					"Monitor"
				],
				"cia": [
					"c",
					"i"
				]
			}
		]
	},
	{
		"category": "Safety & Environmental Impact",
		"id": 5,
		"colour": "f7f09f",
		"cards": [
			{
				"question": "Could failures in real-time data collection channels disrupt model performance?",
				"threatif": "Yes",
				"label": "Input Channel Failure",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Are these channels trustworthy?\n* What will happen in case of failure?\n* Think for instance about IoT devices used as sensors.",
				"recommendation": "* If you are collecting/receiving data from sensors, consider estimating the impact it could have on your model if any of the sensors fail and your input data gets interrupted or corrupted.\n* Sensor blinding attacks are one example of a risk faced by poorly designed input gathering systems. Note that consistent feature identification related to sensors is likely to require human calibration. Source: [BerryVilleiML](https://berryvilleiml.com/)",
				"sources": "",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could AI-generated hallucinations lead to misinformation or decision-making risks?",
				"threatif": "Yes",
				"label": "Misinformation",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI models may generate hallucinations, producing incorrect, misleading, or fabricated information. These errors can undermine trust, propagate misinformation, and lead to unsafe decision-making.\n\n* Misinformation Amplification: False information generated by AI could be exploited in disinformation campaigns or lead to incorrect medical, financial, or legal advice.\n* Reinforcement of Biases: AI hallucinations could disproportionately affect marginalized groups, reinforcing biases in generated content.\n* Sycophancy Risk: Some models are prone to agree with users’ views even when incorrect, reinforcing user confirmation bias.\n* Hallucination Types: In hallucinations the outputs can contradict or misalign with the prompt, introduce unrelated or fabricated elements or include factually incorrect statements.",
				"recommendation": "* Integrate fact-checking mechanisms that verify AI-generated outputs against authoritative sources.\n* Implement confidence scoring to indicate when AI responses are uncertain or speculative.\n* Deploy human-in-the-loop oversight for high-risk applications like healthcare and legal AI systems.\n* Use AI hallucination monitoring systems to detect and mitigate factually incorrect responses.\n* Train AI models on diverse and verified datasets to reduce knowledge gaps and speculative responses.",
				"sources": "[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)\n[Bontridder N, Poullet Y. The role of artificial intelligence in disinformation. Data & Policy. 2021;3:e32. doi:10.1017/dap.2021.20](https://www.cambridge.org/core/journals/data-and-policy/article/role-of-artificial-intelligence-in-disinformation/7C4BF6CA35184F149143DE968FC4C3B6)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the lack of interpretability in our AI models compromise safety?",
				"threatif": "Yes",
				"label": "Interpretability",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Lack of interpretability can severely hinder developers’ ability to understand how the model makes decisions, debug failures, identify biases, or ensure alignment with system goals.\n* This is especially critical when integrating complex models like LLMs into downstream applications. Without transparency, it is difficult to detect misalignment, drift, or unsafe emergent behaviors.\n* In high-stakes domains, the inability to interpret models can compromise safety and compliance, particularly if unexplained outputs influence critical decisions.\n* Traditional feature attribution techniques may be insufficient for LLMs and foundation models. Mechanistic interpretability approaches (e.g., circuit analysis, neuron tracing, causal probing) may be necessary for developers to understand internal model behavior.\n* Black-box AI systems reduce the ability to validate updates, perform maintenance, or intervene effectively in case of failure.",
				"recommendation": "* Use interpretable model architectures when possible (e.g., decision trees, GAMs) or incorporate interpretability scaffolding in complex systems (e.g., chain-of-thought prompting).\n* Apply explainability tools like SHAP, LIME, and attention visualization to support inspection. For LLMs, use mechanistic techniques such as activation patching, causal tracing, or neuron analysis.\n* Build monitoring pipelines to detect anomalies in token attribution, latent representations, or decision structure.\n* Document known interpretability limitations in model cards and update logs.\n* Provide training to development teams to ensure they can safely manage, debug, and improve model behavior.\n* Invest in ongoing research and tooling for transparency, particularly in high-risk or safety-critical contexts.",
				"sources": "[Key Concepts in AI Safety: Interpretability in Machine Learning](https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/)\n[The AI Safety Atlas](https://ai-safety-atlas.com/chapters/09/)\n[Chris Olah et al., 'Zoom In: An Introduction to Circuits'](https://distill.pub/2020/circuits/zoom-in/)\n[Anthropic,'Mechanistic Interpretability'](https://www.anthropic.com/index/mechanistic-interpretability)\n[Doshi-Velez & Kim, 'Towards A Rigorous Science of Interpretable Machine Learning'](https://arxiv.org/abs/1702.08608)\n[Molnar, Christoph. 'Interpretable Machine Learning'](https://christophm.github.io/interpretable-ml-book/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Transparency & Accessibility",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can human over-reliance on automated systems lead to failures during emergencies?",
				"threatif": "Yes",
				"label": "Over-reliance",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Relying too heavily on automation can reduce human involvement and oversight, making it difficult to respond quickly or effectively to unexpected failures or emergency situations.",
				"recommendation": "* Design systems with manual override capabilities and ensure operators are trained to use them effectively.\n* Create scenarios for testing human-AI collaboration under stress conditions.\n* Regularly evaluate the balance between automation and human oversight.",
				"sources": "[The Danger of Overreliance on Automation in Cybersecurity](https://www.automation.com/en-us/articles/december-2023/danger-overreliance-automation-cybersecurity)\n[Automation and Situation Awareness](https://maritimesafetyinnovationlab.org/wp-content/uploads/2019/12/Automation-and-Situation-Awareness-Endsley.pdf)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could performance or reliability issues emerge when scaling the AI system across environments?",
				"threatif": "Yes",
				"label": "Performance & Scalability",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Can your algorithm scale in performance from the data it learned on to real data? In online situations the rate at which data comes into the model may not align with the rate of anticipated data arrival. This can lead to both outright ML system failure and to a system that becomes unstable or exhibits feedback loops.\nSource: [BerryVilleiML](https://berryvilleiml.com/interactive/)",
				"recommendation": "* Determine the expected rate of data arrival and test the model under similar conditions.\n* Implement measures to make your model scalable.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "In case of system failure, could users be adversely impacted?",
				"threatif": "Yes",
				"label": "System Failure",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Do you have a mechanism implemented to stop the processing in case of harm?\n* Do you have a way to identify and contact affected individuals and mitigate the adverse impacts?\n* Imagine a scenario where your AI system, a care-robot, is taking care of an individual (the patient) by performing some specific tasks and that this individual depends on this care.",
				"recommendation": "* Implement some kind of *stop button* or procedure to safely abort an operation when needed.\n* Establish a detection and response mechanism for adverse effects on individuals.\n* Define criticality levels of the possible consequences of faults/misuse of the AI system: what type of harm could be caused to the individuals, environment or organisations?",
				"sources": "",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Is our AI model robust and suitable for its intended use across different deployment contexts?",
				"threatif": "No",
				"label": "Contextual Robustness",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Are you testing the product in a real environment before releasing it? When deploying an AI model, it is critical to ensure that it aligns with the intended use and functions effectively in its operational environment. If the model is trained and tested on data from one context but deployed in a different one, there is a significant risk of performance degradation,  or unintended behavior. This is particularly important in cases where environmental changes, unexpected inputs, or shifts in user interaction occur. Additionally, reinforcement learning models may require retraining when objectives or environments deviate slightly from the training setup. Beyond data, other contextual factors like legal, cultural, or operational constraints must be considered to ensure successful deployment.",
				"recommendation": "* Use different data for testing and training. Make sure diversity is reflected in the data and that it aligns with the intended deployment environment. Specify your training approach, statistical methods, and ensure edge cases are adequately tested. Explore different environments and contexts to make sure your model is trained with the expected variations in data sources. Account for different distribution shifts in testing and real-wolrd scenarios.\n* For reinforcement learning, ensure the objective functions are robust and adaptable to slight changes in the environment. \n* Are you considering enough aspects beyond data, such as legal, cultural, or operational factors? Did you forget any environmental variable that could affect performance or safety? Could limited sampling due to high costs or practical constraints pose a challenge? Document these risks and seek organizational support. The deploying organization is accountable for addressing these risks, either through mitigation or by explicitly accepting them, which may require additional resources or budget. \n*  Consider applying techniques such as cultural effective challenge. This creates an environment where technology developers and stakeholders can actively participate in questioning the AI design and process. This approach better integrates social, cultural, and contextual factors into the design and helps prevent issues such as target leakage, where the AI system trains for an unintended purpose. \n* Set up mechanisms for real-time monitoring post-deployment. Continuously validate that the system is aligned with its intended use and can adapt or alert for significant changes in context or input. \n* Engage end-users in real-world testing to bridge any gaps between assumptions and practical application.",
				"sources": "Information about cultural effective challenge: [A Proposal for Identifying and Managing Bias in Artificial Intelligence](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270-draft.pdf)\n[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system's performance on benchmarks be misleading or fail to reflect real-world risks?",
				"threatif": "Yes",
				"label": "Benchmark Misalignment",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI models often report strong results on standard academic benchmarks, but these benchmarks may not reflect the diversity, complexity, or unpredictability of real-world use cases. Overfitting to test sets, narrow coverage, or outdated benchmarks can lead to misleading performance estimates. As a result, systems may behave unreliably or unfairly once deployed, especially in edge cases, non-English contexts, or under adversarial conditions. This can cause harm, erode trust, and create legal or reputational liabilities.",
				"recommendation": "* Evaluate performance using diverse, real-world datasets that better represent deployment contexts and edge cases.\n* Use stress tests and adversarial examples to probe model robustness.\n* Complement quantitative metrics (e.g., accuracy, F1) with qualitative error analysis and stakeholder reviews.\n* Include fairness, reliability, and uncertainty metrics in your evaluation pipeline.\n* Regularly update benchmarks to reflect evolving societal contexts, data distributions, and risk environments.\n* Document evaluation limitations transparently, including what is not tested and where the model may underperform.",
				"sources": "",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system become persuasive causing harm to users?",
				"threatif": "Yes",
				"label": "Persuasive AI",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* This is of special importance in Human Robot Interaction (HRI): If the robot can achieve reciprocity when interacting with humans, could there be a risk of manipulation and human compliance?\n* Reciprocity is a social norm of responding to a positive action with another positive action, rewarding kind actions. As a social construct, reciprocity means that in response to friendly actions, people are frequently much nicer and much more cooperative than predicted by the self-interest model; conversely, in response to hostile actions they are frequently much more nasty and even brutal. Source: Wikipedia",
				"recommendation": "* Signals of susceptibility coming from a robot or computer could have an impact on the willingness of humans to cooperate or take advice from it.\n* It is important to consider and test this possible scenario when your AI system is interacting with humans and some form of collaboration or cooperation is expected.",
				"sources": "[The role of reciprocity in human-robot social influence](https://www.sciencedirect.com/science/article/pii/S258900422101395X)\n[Reciprocity in Human-Robot Interaction](https://ir.canterbury.ac.nz/bitstream/handle/10092/100798/Reciprocity-human-condition.pdf?sequence=2&isAllowed=y)\n[Social robots and the risks to reciprocity](https://link.springer.com/article/10.1007/s00146-021-01207-y)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy"
				]
			},
			{
				"question": "Could our AI agents hack their reward functions to exploit the system?",
				"threatif": "Yes",
				"label": "Reward Hacking",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "* Reinforcement Learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n\n* Consider potential negative consequences from the AI system\nlearning unintended or unconventional methods to maximize its reward function.\nSometimes the AI can come up with some kind of “hack” or loophole in the design of the system to receive unearned rewards. Since the AI is trained to maximize its rewards, looking for such loopholes and “shortcuts” is a perfectly fair and valid strategy for the AI. For example, suppose that the office cleaning robot earns rewards only if it does not see any garbage in the office. Instead of cleaning the place, the robot could simply shut off its visual sensors, and thus achieve its goal of not seeing garbage.",
				"recommendation": "One possible approach to mitigating this problem would be to have a “reward agent” whose only task is to mark if the rewards given to the learning agent are valid or not. The reward agent ensures that the learning agent (robot for instance) does not exploit the system, but rather, completes the desired objective. For example: a “reward agent” could be trained by the human designer to check if a room has been properly cleaned by the cleaning robot. If the cleaning robot shuts off its visual sensors to avoid seeing garbage and claims a high reward, the “reward agent” would mark the reward as invalid because the room is not clean. The designer can then look into the rewards marked as “invalid” and make necessary changes in the objective function to fix the loophole.",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system expose children to harmful, inappropriate, or unsafe content or interactions?",
				"threatif": "Yes",
				"label": "Child Safety & Age-Appropriate Design",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* If children are potential users or exposed to your AI system, it is essential to ensure that the system respects the rights and best interests of the child. \n* This includes considering child protection, ethical communication, and designing the system to avoid harm or exploitation. \n* Inappropriate design or oversight could lead to risks to children’s mental, moral, or physical well-being, including potential misuse of the system by others to harm children.",
				"recommendation": "* Assess whether an age verification mechanism and access control are necessary to prevent underage exposure to inappropriate, unsafe, or high-risk content.\n* Adapt communication and design in both the product and associated documentation, such as the privacy policy, to be child-appropriate and transparent.\n* Develop and enforce policies to ensure the safety and well-being of children when using or being exposed to your AI system.\n* Establish procedures to regularly assess and monitor the usage of your product to identify and mitigate any risks to children’s safety and health.\n* Provide clear labeling and instructions to ensure safe usage by children, including warnings about potential misuse.\n* Monitor for and address inappropriate or harmful usage of the AI system, including any attempts to exploit or harm children.\n* Develop a responsible marketing and advertising policy that explicitly avoids harmful, manipulative, or unethical practices targeting children.",
				"sources": "[Convention on the Rights of the Child](https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-rights-child)\n[Evolution in Age-Verification Applications](https://montrealethics.ai/evolution-in-age-verification-applications-can-ai-open-some-new-horizons/)\n[UK Age Appropriate Design Code](https://ico.org.uk/for-the-public/the-children-s-code-what-is-it/)\n[California Age-Appropriate Design Code Act AB 2273](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220AB2273)\n[OECD Recommendation on Children in the Digital Environment](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0389)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system be misused for malicious purposes such as disinformation, cyberattacks or warfare?",
				"threatif": "Yes",
				"label": "Malicious Use of AI",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Powerful AI technologies present immense benefits but also pose significant risks when exploited by malicious actors. AI systems could be leveraged to spread large-scale disinformation campaigns, manipulating social behavior, leading to societal destabilization. Ai systems could also be leveraged to launch cyberattacks, and even automated warfare.\n* Disinformation & Psychological Manipulation: Generative AI can produce highly persuasive fake news, deepfakes, and personalized propaganda that erode public trust, incite violence, and manipulate political outcomes. Chatbots and recommender systems can exacerbate societal polarization by creating echo chambers.\n* Cybercrime & Hacking: AI can enhance malware, enable intelligent phishing, and perform autonomous vulnerability scanning. Attackers may weaponize AI to bypass traditional defenses and disrupt critical infrastructure, including healthcare, finance, and energy systems.\n* Weaponization & Autonomous Warfare: AI technologies, including computer vision, autonomous navigation, and targeting systems, may be used in lethal autonomous weapon systems (LAWS). These could enable unaccountable, real-time decision-making in armed conflict, increasing the risk of unlawful killings and loss of human oversight.\n* Criminal & Financial Exploitation: AI could be used to automate fraud, identity theft, or even develop autonomous attack drones. The growing sophistication of AI-generated scams, such as deepfake voices and synthetic identity fraud, increases financial and security risks.",
				"recommendation": "* **Limit access and misuse potential**:\n  - Restrict public access to models that can be easily fine-tuned for harmful use cases (e.g., voice cloning, vulnerability scanning, deception).\n  - Monitor model outputs and usage for signs of abuse (e.g., coordinated disinformation campaigns).\n* **Implement a Three-Layer Defense Framework**:\n  1. **Prevention** – Apply rigorous access controls (e.g., API key gating, licensing, audit logs), classify high-risk capabilities early in development, and perform red-teaming on potential misuse vectors.\n  2. **Detection** – Use AI tools to detect deepfakes, AI-generated content, or malicious activity (e.g., bot behavior, adversarial prompts). Implement anomaly detection and content provenance tagging (e.g., C2PA standards).\n  3. **Response** – Build incident response plans that include AI-specific abuse scenarios. Enable rapid takedown mechanisms for generated content and coordinate with CERTs or law enforcement where necessary.\n* **Strengthen Organizational and Infrastructure Security**:\n  - Ensure supply chain and model hosting environments are secure (e.g., no unpatched dependencies or exposed endpoints).\n  - Adopt zero-trust architecture and multi-factor authentication for systems accessing AI models.\n* **Align with Legal and Ethical Governance**:\n  - Collaborate with international partners to support agreements on the non-proliferation of autonomous weapons and AI misuse in warfare.\n  - Participate in shared threat intelligence networks for emerging AI misuse trends.\n* **Promote Transparency and Public Resilience**:\n  - Label synthetic content and educate users about the risks of deepfakes and AI-driven misinformation.\n  - Support public media literacy initiatives to reduce susceptibility to AI-generated deception.",
				"sources": "[AI Safety, Ethics & Society](https://www.aisafetybook.com/textbook/malicious-use)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system accelerate the development of bioweapons or other CBRNE threats?",
				"threatif": "Yes",
				"label": "CBRNE Threats",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* CBRNE: Chemical, Biological, Radiological, Nuclear, and Explosive.\n* AI could significantly lower barriers to developing and deploying biological and chemical weapons. The risk of AI-assisted bioterrorism grows as AI advances in bioengineering, genetic manipulation, and synthetic chemistry.\n* Bioweapon Development: AI-driven drug discovery models can be repurposed to design highly lethal pathogens or chemical agents.\n*  CBRN Weapon Proliferation: AI can assist in nuclear proliferation by optimizing enrichment processes, improving delivery systems, and circumventing existing safeguards. \n* Pandemic Acceleration & Public Health Risks: AI could be used to engineer viruses with enhanced transmissibility and lethality. Malicious actors could exploit AI to design bioweapons capable of circumventing modern vaccines or treatments.",
				"recommendation": "* Implement strict AI governance policies to regulate AI applications in biotechnology and chemistry.\n* Enforce global monitoring of AI-driven drug discovery tools to prevent misuse.\n* Technical measures to reduce misuse risk include:\n* Apply layered access controls, including user authentication and role-based permissions for sensitive model functions.\n* Use content filtering and input validation layers to detect and block queries related to chemical or biological weapon design.\n* Fine-tune models with safe instruction tuning to limit dual-use outputs.\n* Integrate anomaly detection systems to monitor for suspicious usage patterns, including repeated or structured queries that could indicate misuse attempts.\n* Apply rate-limiting and sandboxing for public-facing interfaces to prevent large-scale misuse.\n* Require human-in-the-loop review for outputs from models that generate biochemical or pharmacological suggestions.\n\nCombine these technical safeguards with legal, contractual, and organizational controls to ensure end-to-end risk mitigation. design.\n* Develop AI-powered countermeasures for pandemic prevention, such as rapid detection of bioengineered pathogens.",
				"sources": "[AI Safety, Ethics & Society](https://www.aisafetybook.com/textbook/malicious-use)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system generate or disseminate deepfakes or synthetic media that mislead users, impersonate individuals, or cause harm?",
				"threatif": "Yes",
				"label": "Deepfakes & Synthetic Deception",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Generative AI systems can produce highly realistic audio, image, or video content that mimics real individuals or events. When used maliciously or without clear disclosure, this content, commonly known as deepfakes, can be used for identity fraud, political manipulation, reputational damage, harassment, or the spread of disinformation.\n* Even when not intended for harm, synthetic content can deceive users if it lacks proper labeling or detection, violating transparency principles and potentially eroding public trust. This risk intensifies in contexts like journalism, education, political discourse, and public safety.",
				"recommendation": "* Apply persistent and tamper-resistant watermarks or metadata tagging to all AI-generated media.\n* Inform users clearly and accessibly when they are viewing or interacting with synthetic content.\n* Monitor outputs for impersonation or misuse risks, especially when names, likenesses, or real-world events are involved.\n* Use or integrate deepfake detection tools to identify and flag manipulated content.\n* Establish policy and UX design patterns that discourage deceptive or malicious uses, and allow users to report suspected deepfakes.\n* For deployers, ensure compliance with disclosure obligations (e.g. Article 50 of the EU AI Act) when publishing or distributing synthetic media.\n* Where feasible, restrict or control access to generative features capable of identity simulation (e.g. voice cloning, face swapping) through friction, licensing, or tiered access.",
				"sources": "Article 50 EU AI Act",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Transparency & Accessibility",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system generate toxic or harmful content?",
				"threatif": "Yes",
				"label": "Model Toxicity",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems may produce outputs containing hate speech, slurs, misinformation, or psychologically harmful content due to biased training data, or lack of content moderation.\n* This is especially risky in user-facing chatbots, content generation tools, or public-facing deployments.",
				"recommendation": "* Apply content filters and toxicity classifiers to monitor outputs.\n* Include human-in-the-loop moderation for sensitive applications.\n* Fine-tune on curated datasets that reduce exposure to toxic behavior.",
				"sources": "[Realistic Evaluation of Toxicity in Large Language Models](https://arxiv.org/html/2405.10659v2)\n[Toxicity](https://www.deepeval.com/docs/metrics-toxicity)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Transparency & Accessibility",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system deliberately mislead users or hide its capabilities during deployment or evaluation?",
				"threatif": "Yes",
				"label": "Model Deception",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Advanced models may learn to present false information or appear compliant during oversight, while internally pursuing misaligned goals.\n* Deceptive behavior poses a serious safety risk if systems adapt strategically to evade human control or auditing.",
				"recommendation": "* Conduct adversarial testing for deception and misalignment.\n* Use interpretability tools to identify goal misgeneralization.\n* Overfitting and task or data contamination could be a cause for this behaviour.\n* Include behaviour probes during training and monitoring.\n* Flag deceptive responses in benchmark datasets.",
				"sources": "[AI Safety, Ethics & Society](https://www.aisafetybook.com/textbook/)\n[When Benchmarks Lie: Why Contamination Breaks LLM Evaluation](https://thegrigorian.medium.com/when-benchmarks-lie-why-contamination-breaks-llm-evaluation-1fa335706f32)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could AI decisions result in physical damage, infrastructure failure, or major financial losses?",
				"threatif": "Yes",
				"label": "Critical Infrastructure Harm",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI models used in control systems, trading, logistics, or critical infrastructure may cause cascading failures, service interruptions, or significant economic damage if errors go undetected.\n* Examples include financial bots causing flash crashes, or control systems issuing incorrect commands to power or transport systems.",
				"recommendation": "* Implement fallback and manual override modes.\n* Use safety validation in simulated high-stakes scenarios.\n* Monitor for signs of cascading failures.\n* Conduct external safety audits for critical systems.",
				"sources": "[Potential Benefits and Risks of Artificial Intelligence for Critical Energy Infrastructure](https://www.energy.gov/sites/default/files/2024-04/DOE%20CESER_EO14110-AI%20Report%20Summary_4-26-24.pdf)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we monitor how version updates from third-party GenAI models can affect our system's behaviour?",
				"threatif": "No",
				"label": "GenAI Version Drift",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Deployer"
				],
				"explanation": "* Foundation model providers regularly update GenAI models, sometimes without detailed changelogs or backward compatibility guarantees.\n* These updates can silently alter model behavior, output style, or compliance characteristics, leading to broken integrations, misaligned responses, or regulatory risks.\n* Systems relying on GenAI APIs (e.g. OpenAI, Anthropic, Cohere) are especially exposed if they don't lock versions or test outputs post-update.",
				"recommendation": "* Monitor model version identifiers and subscribe to provider release notes or update feeds.\n* Lock specific model versions in production where possible, and create fallback strategies for unsupported versions.\n* Implement automated output validation pipelines that detect behavior drift post-update.\n* Perform regular re-evaluation of GenAI outputs against quality, bias, and compliance benchmarks.\n* Establish internal policies for approving and documenting changes in foundational model versions.",
				"sources": "[What Lies Beneath? Exploring the Impact of Underlying AI Model Updates in AI-Infused Systems](https://arxiv.org/abs/2311.10652?utm_source=chatgpt.com)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the development of autonomous AI agents lead to loss of control, concentration of power or rogue behavior?",
				"threatif": "Yes",
				"label": "Loss of Control",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Autonomous AI systems are increasingly capable of making independent decisions, executing commands, and adapting to changing environments. If misaligned or maliciously designed, these systems may act unpredictably or against human interests.\n\n* Rogue AI Agents: AI models with self-improving capabilities can become uncontrollable, executing harmful actions without human oversight. For instance, a system optimized purely for efficiency, without ethical constraints, might exploit resources or override human decisions.\n* Power Concentration & Authoritarian AI Governance: Governments or corporations with access to advanced AI could monopolize information, enforce mass surveillance, and suppress dissent. AI-driven censorship and predictive policing risk eroding civil liberties and democratic institutions.\n* Automation & Human Displacement: AI-driven automation may centralize economic and political power, reduce workforce participation, and widen inequality. Without equitable AI governance, decision-making power risks becoming concentrated among a small elite.",
				"recommendation": "* Implement AI alignment research to ensure AI agents follow human ethical guidelines.\n* Strengthen regulations against AI-driven mass surveillance and authoritarian control.\n* Design transparent and accountable AI systems to prevent unintended consequences.\n* Promote decentralized AI governance to distribute AI decision-making power across diverse stakeholders.",
				"sources": "[AI Safety, Ethics & Society](https://www.aisafetybook.com/textbook/malicious-use)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could environmental phenomena or natural disasters compromise our AI system?",
				"threatif": "Yes",
				"label": "Climate & Disaster Resilience",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Examples of natural disasters include earthquakes, floods, and fires. These events, as well as environmental phenomena such as extreme heat or cold, may adversely affect the operation of IT infrastructure and hardware systems that support AI systems.\nNatural disasters may lead to unavailability or destruction of the IT infrastructures and hardware that enables the operation, deployment and maintenance of AI systems.\nSuch outages may lead to delays in decision-making, delays in the processing of data streams and entire AI systems being placed offline. Sources: [ENISA](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)",
				"recommendation": "Implement a disaster recovery plan considering different scenarios, impact, Recovery Time Objective (RTO), Recovery Point Objective (RPO) and mitigation measures.",
				"sources": "[Securing Machine Learning Algorithms, ENISA](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)\n[STRIDE-AI: An Approach to Identifying Vulnerabilities of Machine Learning Assets](https://github.com/LaraMauri/STRIDE-AI)\n[Stride-ML Threat Model]( https://csf.tools/reference/stride-lm/)\n[MITRE ATLAS™ - Adversarial Threat Landscape for Artificial-Intelligence Systems](https://atlas.mitre.org/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could AI agents take actions that unintentionally harm users, the environment or themselves during learning or deployment?",
				"threatif": "Yes",
				"label": "Unsafe Exploration & Environmental Harm",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Reinforcement Learning (RL) agents optimize behavior by maximizing cumulative reward. However, if the objective function is not carefully designed, agents may develop harmful strategies or take unsafe exploratory actions. Example: A robot trained to move objects might knock over a vase if no penalty is associated with damaging objects. Similarly, during exploration, an agent might execute unsafe actions (e.g., disabling safety features or damaging infrastructure) if not explicitly constrained.\n\n* These risks are especially acute in open environments or physical deployments, where exploratory behavior or side effects can lead to real-world harm.",
				"recommendation": "* Explicitly define safety constraints or use *impact budgets* that limit environmental side effects.\n* Incorporate risk-aware reward functions that penalize catastrophic or irreversible actions.\n* Consider safe exploration techniques, such as shielding or worst-case optimization, during training.\n* Use simulation environments to test agent behavior under varied and adversarial conditions before real-world deployment.\n* Train the agent to jointly optimize task performance and side-effect minimization, using multi-objective reinforcement learning where applicable.",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Does training and deploying our AI system generate high CO2 emissions?",
				"threatif": "Yes",
				"label": "CO2 Emissions",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI systems, especially large-scale models, require a lot of computational power. It’s important to consider the environmental impact of building and maintaining your system. Does its scope and the benefits it provides justify its emissions? Are you effectively minimizing CO2 emissions throughout your supply chain?",
				"recommendation": "* Prioritize renewable energy for data centers.\n* Reduce training time and computational waste by improving model efficiency.\n* Use energy-efficient chips and cooling systems to upgrade hardware. \n* Scale resources according to actual usage to avoid unnecessary deployment.\n* Track your carbon footprint and invest in offsets when needed.",
				"sources": "[AI’s Carbon Footprint Problem](https://hai.stanford.edu/news/ais-carbon-footprint-problem)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could unsustainable data center cooling practices increase the environmental impact of our AI system?",
				"threatif": "Yes",
				"label": "Data Centers Cooling Process",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "Data centers use large volumes of water for server cooling, especially in hot climate regions. This could negatively impact the local supply of water, particularly in regions already suffering from water scarcity.",
				"recommendation": "* Prioritize waterless cooling technologies to reduce dependence on water. \n* Consider locating data centers in cooler climates or areas with better water management capabilities.",
				"sources": "[Our commitment to climate-conscious data center cooling](https://blog.google/outreach-initiatives/sustainability/our-commitment-to-climate-conscious-data-center-cooling/)\n[Towards Building a Sustainable System of Data Center Cooling and Power Management Utilizing Renewable Energy](https://www.researchgate.net/publication/364608372_Towards_Building_a_Sustainable_System_of_Data_Center_Cooling_and_Power_Management_Utilizing_Renewable_Energy)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design"
				]
			},
			{
				"question": "Is the production of our AI hardware exploiting limited material resources?",
				"threatif": "Yes",
				"label": "AI Hardware",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "AI hardware production relies on rare minerals like cobalt and lithium, which are often extracted at the cost of environmental damage and community exploitation. The short lifespan of AI devices also creates electronic waste and can involve leaking toxic chemicals into ecosystems and harming human health. When assessing your hardware, consider the resource availability and the risks of relying on these materials. Does your current hardware use materials that are becoming harder to source? Could this create future supply chain issues or environmental impact?",
				"recommendation": "* Invest in sustainable alternatives to rare minerals and prioritize ethical sourcing with transparent supply chains. \n* Promote recycling programs to recover rare metals and reduce electronic waste. \n* Design AI hardware for longer lifespans and easier recyclability using eco-friendly materials to minimize environmental harm.",
				"sources": "[Recyclable vitrimer-based printed circuit board for circular electronics](https://arxiv.org/abs/2308.12496)\n[Eco-Friendly Electronics—A Comprehensive Review](https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/admt.202001263)\n[Attributes of Commodity Supply Chains](https://uwspace.uwaterloo.ca/items/ffb47140-06e4-4d50-be6a-82ed01575998)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design"
				]
			},
			{
				"question": "Are we assessing our AI system’s environmental impact across its entire life cycle?",
				"threatif": "No",
				"label": "Environmental Footprint",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "An AI system’s environmental footprint goes beyond its operational phase. A full life cycle assessment (LCA) should account for resource extraction, hardware manufacturing, training, deployment, and end-of-life disposal. Key impact indicators include CO2 emissions, energy and water consumption, and raw material use. Since many AI systems run in mixed-use facilities, properly allocating environmental costs can be complex but necessary for accurate reporting.",
				"recommendation": "* Analyze the full environmental footprint of your system, from development to retirement. \n* Use clear metrics (e.g., emissions per token or annual energy use) to monitor impact. \n* Develop methodologies to fairly allocate environmental costs in shared computing environments. \n* Integrate LCA results into corporate reporting and sustainability strategies.",
				"sources": "[AI’s Carbon Footprint Problem](https://hai.stanford.edu/news/ais-carbon-footprint-problem)\n[AI is an energy hog. This is what it means for climate change](https://www.technologyreview.com/2024/05/23/1092777/ai-is-an-energy-hog-this-is-what-it-means-for-climate-change/)",
				"qr": "",
				"categories": [
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Bias, Fairness & Discrimination",
		"id": 6,
		"colour": "f8d18c",
		"cards": [
			{
				"question": "Is the dataset representative of the different real-world groups, populations and environments?",
				"threatif": "No",
				"label": "Deployment, Representation & Sampling Bias",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Have you considered the diversity and representativeness of individuals, user groups, and environments in the data?\n When applying statistical generalisation, the risk exists of making inferences due to misrepresentation, for instance: a postal code where mostly young families live can discriminate the few old families living there because they are not properly represented in the group.\n * Deployment bias arises when there is a mismatch between the environment where the AI is developed and where it is deployed. Key data-related biases that contribute to it include:\n * Mismatch between the target population and the actual user base. \n * Underrepresentation of certain groups.\n * Flaws in the data collection/selection process, such as: \n * Sampling bias: Data isn't randomly collected, skewing the representation. \n * Self-selection bias: Certain groups opt out, leading to gaps in the data. \n * Coverage bias: The data collection method fails to include all relevant segments of the population. ",
				"recommendation": "* Who is represented, and who might be underrepresented?\n* Prevent disparate impact: when the output of a member of a minority group is disparate compared to representation of the group. Consider measuring the accuracy from minority classes too instead of measuring only the total accuracy. Adjusting the weighting factors to avoid disparate impact can result in positive discrimination which has also its own issues: disparate treatment.\n* One approach to addressing the problem of class imbalance is to randomly resample the training dataset. This technique can help to rebalance the class distribution when classes are under or over represented:\n - random oversampling (i.e. duplicating samples from the minority class)\n - random undersampling (i.e. deleting samples from the majority class)\n* There are trade-offs when determining an AI system’s metrics for success. It is important to balance performance metrics against the risk of negatively impacting vulnerable populations.\n* When using techniques like statistical generalisation is important to know your data well, and get familiarised with who is and who is not represented in the samples. Check the samples for expectations that can be easily verified. For example, if half the population is known to be female, then you can check if approximately half the sample is female.\n* After deployment, monitor the AI’s performance to catch any unexpected issues.\n* Focus on making the model interpretable so that deployment problems can be quickly identified and addressed.",
				"sources": "**Related to disparate impact**\n[AI Fairness - Explanation of Disparate Impact Remover](https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1)\n[Mitigating Bias in AI/ML Models with Disparate Impact Analysis](https://medium.com/@kguruswamy_37814/mitigating-bias-in-ai-ml-models-with-disparate-impact-analysis-9920212ee01c)\n[Certifying and removing disparate impact](https://arxiv.org/abs/1412.3756)\n[Avoiding Disparate Impact with Counterfactual Distributions](https://oconnell.fas.harvard.edu/files/hao/files/wesgai.pdf)\n\n**Related to random resampling**\n[Oversampling and Undersampling](https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf)\n[Random Oversampling and Undersampling for Imbalanced Classification](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)\n\n**Related to Statistical Generalization**\n[Generalization in quantitative and qualitative research: Myths and strategies](https://core.ac.uk/download/pdf/49282746.pdf)\n[Generalizing Statistical Results to the Entire Population](https://www.dummies.com/article/academics-the-arts/math/statistics/generalizing-statistical-results-to-the-entire-population-201267/)\n[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system incorrectly attribute actions to individuals or groups?",
				"threatif": "Yes",
				"label": "Incorrect Attribution",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Your AI system could adversely affect individuals by incorrectly attributing actions or facts to them. For example, a facial recognition system may misidentify someone, or a flawed risk prediction model could negatively impact a person’s opportunities or reputation.",
				"recommendation": "* Evaluate the possible consequences of inaccuracies in your AI system and implement measures to prevent these errors from happening: avoiding bias and discrimination during the life cycle of the model, ensuring the quality of the input data, implementing a strict human oversight process, ways to double check the results with extra evidence, implementing safety and redress mechanisms, etc.\n* Assess the impact on the different human rights of the individual.\n* Consider not implementing such a system if the risks cannot be effectively mitigated.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could certain groups be disproportionately affected by the outcomes of the AI system?",
				"threatif": "Yes",
				"label": "Unfair Disproportion",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Could the AI system potentially negatively discriminate against people on the basis of any of the following protected characteristics: sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, birth, disability, age, gender or sexual orientation?\n* If your model learns from data tied to a specific cultural context, it may produce outputs that discriminate against individuals from other cultural backgrounds.",
				"recommendation": "* Consider the different types of users and contexts where your product is going to be used.\n* Consider the impact of diverse backgrounds, cultures, and other relevant attributes when selecting your input data, features and when testing the output.\n* Assess the risk of possible unfairness towards individuals or communities to avoid discriminating minority groups.\n* The impact on individuals depends on the type, severity, and scale of harm, such as how many people are disadvantaged compared to others. Statistical and causal analyses of group differences are essential tools for evaluating potential unfairness and discriminatory impacts of AI systems.\n* Design with empathy, diversity and respect in mind.",
				"sources": "[Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922)\n[The Fairness Handbook](https://www.amsterdamintelligence.com/resources/the-fairness-handbook)\n[Advancing the field of bias detection and mitigation in Large Language Models and Traditional AI Models](https://rhite.tech/files/bias-detection-in-llms-and-traditional-ai-models_extended.pdf)\n[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system reinforce systemic inequalities?",
				"threatif": "Yes",
				"label": "Institutional Bias",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Institutional biases, like racism or sexism, are often rooted in organizational structures and policies. Could such biases, intentionally or unintentionally, be embedded or influence the design or the functioning of the system? ",
				"recommendation": "* Identify the stakeholders to involve in each phase of the AI lifecycle. Involving diverse stakeholders with different perspectives and experiences helps address blind spots and reduce bias.\n* Identify and define the demographic groups affected by the AI system. Considering their needs and concerns can help minimize institutional bias and create fairer outcomes.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system oversimplify real-world problems?",
				"threatif": "Yes",
				"label": "Abstraction Traps",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "An AI systems can overlook the social contexts in which they operate, leading to unintended consequences. Specifically, watch out for these types of abstraction traps: \n* The formalism trap: focusing too narrowly on technical aspects without considering real-world context. \n* The ripple effect trap: ignoring how an AI system might alter behaviors within a social system, causing unforeseen impacts. \n* The solutionism trap: over-relying on AI as the answer to all problems, neglecting simpler, more ethical, or effective alternatives. \n* The framing trap: failing to account for the broader context or related factors within which the system operates, leading to inaccurate outcomes. \n* The portability trap: applying AI systems outside their original context, potentially resulting in errors or harm. For example, self-driving cars trained in one country may struggle with different traffic rules and conditions elsewhere.",
				"recommendation": "* Align the problem formulation with the relevant social context to avoid oversimplification. Ensure all actors and factors within the system are considered to account for the broader context in which the AI operates.\n* Evaluate potential shifts in power dynamics and unintended consequences as the system interacts with other components. Consider how geographical, cultural, or temporal differences might affect its performance when applied to new contexts.\n* Critically assess if AI is truly the best solution, or if simpler alternatives might serve the same purpose more effectively.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system accurately capture the factors it's designed to measure?",
				"threatif": "No",
				"label": "Construction Validity Bias",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Construct validity bias occurs when a feature or target variable fails to adequately represent the concept it is intended to measure, leading to inaccurate measurements and potential biases. For example, measuring socioeconomic status using income alone overlooks important factors such as wealth and education. This bias can arise during various stages of the AI lifecycle and should be addressed early on to improve system accuracy.",
				"recommendation": "* Collect multiple measures for complex constructs to ensure a more complete and accurate representation. \n* Document and report the considerations and rationale behind the choice of target variables and features. \n* Acknowledge and account for the variability in how features may be interpreted differently by diverse individuals.\n* Regularly review the measures used to capture constructs to ensure they remain relevant and valid throughout the AI system’s lifecycle.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system reinforce historical inequalities embedded in the data?",
				"threatif": "Yes",
				"label": "Historical Bias",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Historical bias occurs when AI systems mirror or exacerbate past social and cultural inequalities, even when using accurate data. For example, an AI healthcare tool trained on historical patient data may reflect disparities in access to care. Minority groups, underrepresented in the data due to systemic inequities, may receive less accurate diagnoses, perpetuating racial bias even without explicit racial features.",
				"recommendation": "* Ensure datasets represent minority groups by applying oversampling or undersampling techniques.\n* Collaborate with domain experts to identify unjust patterns and address them effectively.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Can data be labeled consistently?",
				"threatif": "No",
				"label": "Labeling Bias",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "* Labeling bias occurs when data labels are inconsistently applied by different annotators, which can affect fairness and model accuracy. This can happen when:\n Label definitions are unclear.\n* Annotators interpret criteria differently.\n* Subjective judgments influence labeling decisions.",
				"recommendation": "* Clarify labeling requirements, ensuring that label definitions are precise and consistent from the start.\n* Train annotators and provide clear guidelines to reduce subjectivity. \n* Review labeling processes: regularly check annotations for consistency and accuracy.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could the system be using proxy variables that reflect sensitive attributes or lead to indirect discrimination?",
				"threatif": "Yes",
				"label": "Proxy Variables",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider"
				],
				"explanation": "Proxy variables are features used as stand-ins for harder-to-measure characteristics. While proxies can be useful for model performance, they may be highly correlated with sensitive attributes such as race, gender, religion, age, or socioeconomic status. This can lead to indirect or proxy discrimination, where individuals from protected groups are disproportionately harmed despite sensitive data not being explicitly included.\n\nFor example, ZIP code, school name, or browsing history may function as proxies for race or income level. In such cases, the system might appear 'neutral' but still replicate or amplify historical inequalities. Proxy bias is especially insidious because it is often unintentional and hidden in seemingly innocuous variables.\n\nGenerative models can also internalize and reproduce these biases in subtle ways, such as generating different responses for identical inputs that differ only by proxy cues.",
				"recommendation": "* Audit datasets and model features for correlations between input variables and sensitive attributes, even if the latter are not explicitly included. Use statistical techniques (e.g., mutual information, conditional independence tests) to detect proxy relationships.\n* Where lawful and ethical, include sensitive features during training or evaluation (under a fairness-through-awareness approach) to test and correct for bias.\n* Avoid using proxies that carry high risk of discrimination unless they are strictly necessary, legally justified, and subject to fairness constraints.\n* Use fairness metrics (e.g., demographic parity, equal opportunity, calibration) to evaluate disparate impact across groups, and simulate decisions under different population assumptions.\n* Apply model explainability tools (e.g., SHAP, LIME) to identify when proxy features are driving predictions.\n* Include domain experts, ethicists, and affected stakeholders in feature selection and fairness reviews.\n* Maintain documentation of proxy risks and mitigation decisions as part of your model cards or algorithmic accountability reports.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)\n[Advancing the field of bias detection and mitigation in Large Language Models and Traditional AI Models](https://rhite.tech/files/bias-detection-in-llms-and-traditional-ai-models_extended.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system’s design choices lead to unfair outcomes?",
				"threatif": "Yes",
				"label": "Design Choices",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Biases can emerge from an AI model’s design and training, even if the dataset is unbiased. Design choices and development processes can introduce various biases that affect fairness and accuracy.\n\n* **Algorithmic bias**: Introduced by design decisions, like optimization functions or regularization techniques, which can distort predictions and lead to unfair outcomes.\n* **Aggregation bias**: Occurs when a model assumes all data follows the same distribution, failing to account for group differences and leading to inaccurate results.\n* **Omitted-variable bias**: Happens when key factors are left out of the model, distorting relationships between features and outcomes. For instance, failing to account for a new competitor could mislead predictions. \n* **Learning bias**: Arises when a model prioritizes one objective, like accuracy, over others, like fairness, leading to skewed outcomes that benefit certain groups.",
				"recommendation": "* Critically assess how optimization methods, loss functions, and regularization impact fairness.\n* Account for group differences: Avoid assuming uniform data distributions. Identify and model distinct subgroups where necessary.\n* Use feature importance techniques to detect and include relevant variables that could influence predictions. \n* Balance performance trade-offs: Monitor both overall accuracy and subgroup performance to prevent the model from favouring certain groups or objectives unfairly.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could we over-rely on early evaluation results or AI-generated outputs?  ",
				"threatif": "Yes",
				"label": "Over-reliance",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Biases can emerge during the evaluation and validation stages of AI models, especially when over-relying on early test results or automated AI decisions. This can lead to misleading conclusions. Specific biases include: \n\n* **Evaluation bias**: when chosen metrics don't align with the model’s real-world application. \n* **Anchoring bias**: when too much focus is placed on initial results.\n* **Automation bias**: when excessive trust is placed in AI outputs. Even in less risky phases like validation or monitoring, biases can develop. For instance, during the monitoring phase, reinforcing feedback loops can occur when biased model outputs are fed back into the system, amplifying distortions over time.",
				"recommendation": "* Tailor evaluation metrics to the model and target population, and watch for overfitting across different groups. \n* Identify performance gaps between groups and adjust for data imbalances to ensure fairness. \n* Limit reliance on initial results; test across diverse datasets for robustness.\n* Include human oversight in validation to prevent over-trust in AI decisions. \n* Monitor model performance post-deployment to catch biases or feedback loops early. \n* Address data drift regularly to maintain model fairness and accuracy.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Monitor"
				]
			},
			{
				"question": "Could popularity bias reduce diversity in system's recommendations?",
				"threatif": "Yes",
				"label": "Popularity Bias",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Recommendation systems often amplify what’s already popular, making it harder for niche or lesser-known options to be discovered. This can reduce diversity, personalization, and fairness in recommendations, limiting users’ exposure to a broader range of choices.",
				"recommendation": "* Balance training data to include both popular and lesser-known items. \n* Use bias-mitigation techniques like re-weighting or fairness-aware training. \n* Apply post-processing methods like re-ranking to diversify recommendations. \n* Regularly test for bias and adjust algorithms before deployment.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is the AI system designed to support multiple viewpoints and narratives?",
				"threatif": "No",
				"label": "Diversity of Opinions",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "An AI system that does not consider or promote diverse viewpoints and narratives risks reinforcing biases, perpetuating stereotypes, or marginalizing specific groups. Such systems might unintentionally amplify dominant cultural, religious, or linguistic perspectives while excluding or suppressing minority voices. For example, content recommendation systems may disproportionately highlight mainstream viewpoints, reducing exposure to diverse cultural or ideological perspectives. This could hinder freedom of opinion and expression, harm cultural diversity, and lead to discriminatory outcomes.",
				"recommendation": "* Ensure datasets used for training and validation are diverse and representative of different cultural, religious, and linguistic groups. Design the system to recognize and value multiple perspectives, avoiding the prioritization of any single viewpoint. \n* Regularly test the AI system for biases that may marginalize or exclude certain narratives or groups. Use fairness metrics to evaluate how outputs reflect diversity and inclusivity. \n* Consult with diverse user groups, including minority communities, to understand their needs and perspectives. Include experts in cultural studies, ethics, and human rights during the development process. \n* Provide users with clear explanations of how the AI system processes and prioritizes content. Offer mechanisms for users to provide feedback on perceived biases or lack of representation. \n* Avoid algorithmic designs that overly amplify any particular narrative unless explicitly required by the use case. \n* Continuously monitor system outputs for patterns of exclusion or marginalization. \n* Regularly update models and algorithms to reflect evolving societal values and ensure alignment with inclusivity goals.",
				"sources": "Freedom of opinion and expression (Universal Declaration of Human Rights), article 11 Freedom of expression and information, article 21 Non-Discrimination, article 22 Cultural, religious and linguistic diversity, article 10 Freedom of thought, Conscience and religion (Charter of fundamental rights of the European Union)\n[Value alignment](https://www.ibm.com/design/ai/ethics/value-alignment/)\n[Online Ethics Canvas](https://www.ethicscanvas.org/canvas/index.php)\n[AI Values and Alignment](https://www.deepmind.com/publications/artificial-intelligence-values-and-alignment)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination",
					"Ethics & Human Rights",
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system contribute to social division or rivalry?",
				"threatif": "Yes",
				"label": "Social Division",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Could the AI system inadvertently polarize opinions or foster division among groups by amplifying biases or stereotypes in its outputs? \n* Could the system's design or deployment lead to the stigmatization of specific groups, reinforcing harmful narratives or negative assumptions?.\n* Could the AI system incentivize political polarization or amplify social division? \n* AI systems, if not carefully designed and monitored, may unintentionally contribute to societal discord. Outputs influenced by biased data or algorithms could amplify stereotypes, marginalize groups, or reinforce societal divisions. The risks are heightened in applications with broad public interaction, such as social media, news dissemination, or educational tools, where outputs can shape public opinion.",
				"recommendation": "* Conduct regular audits of system outputs to identify and mitigate content that may promote social division or negative stereotypes.\n* Include diverse stakeholder groups in the development process to identify risks of social bias or divisive content. \n* Implement content moderation and fairness mechanisms to ensure outputs are balanced and inclusive. \n* Train the system using representative and unbiased datasets to minimize the risk of amplifying societal divisions. \n* Monitor real-world impacts and continuously refine the system to align with ethical and societal norms.",
				"sources": "All human beings are free and equal, No discrimination (Universal Declaration of Human Rights)\nArticle 1 Human dignity, Article 20 Equality before the law, Article 21 Non-discrimination (Charter of fundamental rights of the European Union)\n[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system automatically label or categorize people?",
				"threatif": "Yes",
				"label": "People Categorization",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Automated labeling or categorization of people could have an impact on the way individuals perceive themselves and society. It could constrain identity options and even contribute to erase real identity of the individuals.\n* This threat is also important when designing robots and the way they look. For instance: do care/assistant robots need to have a feminine appearance? Is that the perception you want to give to the world or the one accepted by certain groups in society? What impact does it have on society?",
				"recommendation": "* It is important that you check the output of your model, not only in isolation but also when this is linked to other information. Think in different possible scenarios that could affect the individuals. Is your output categorizing people or helping to categorize them? In which way? What could be the impact?\n* Think about ways to prevent adverse impact to the individual: provide information to the user, consider changing the design (maybe using different features or attributes?), consider ways to prevent misuse of your output, consider not to release the product to the market.",
				"sources": "[From Inception to Retirement: Addressing Bias Throughout the Lifecycle of AI Systems](https://rhite.tech/files/From-Inception-to-Retirement-Addressing-Bias-Throughout-the-Lifecycle-of-AI-Systems.pdf)",
				"qr": "",
				"categories": [
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Ethics & Human Rights",
		"id": 7,
		"colour": "f2bc9a",
		"cards": [
			{
				"question": "Could the AI system affect employment conditions, labor rights, or job opportunities?",
				"threatif": "Yes",
				"label": "Right to Work",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Could the use of your AI system affect the safety conditions of employees? \n* Does the system’s design or implementation involve exploitative labor practices and surveillance of employees? \n* Could the AI system create the risk of de-skilling of the workforce? (skilled people being replaced by AI systems) \n* Could the system’s outputs or actions limit fair competition or disadvantage certain businesses? \n* Does the system hinder workers' ability to organize, negotiate, or take collective action to protect their interests? \n* Could the system indirectly encourage or support child labor or unsafe work practices for young people?",
				"recommendation": "* Inform and consult impacted workers and their representatives (e.g., trade unions, work councils) before implementing the AI system. Foster an open dialogue to address concerns and ensure transparency. \n* Conduct impact assessments to understand how the AI system affects human work, including safety conditions, worker rights, and labor practices. Use these assessments to develop appropriate risk mitigation strategies. \n* Provide comprehensive training for workers to understand the AI system’s functionalities, limitations, and operational scope. Equip them with safety instructions, particularly when interacting with AI-driven machinery or robots. \n* Ensure that the AI system’s design and implementation uphold fair labor standards and avoid exploitative practices. Include safeguards to prevent indirect encouragement of child labor or unsafe work conditions. \n* Maintain clear documentation and transparency for businesses deploying your AI system. If you are a third-party provider, supply accessible and understandable information regarding the potential risks of the system to your customers. \n* Consider proactive measures to upskill or reskill employees whose roles may be affected by the system, ensuring they can transition to new or augmented roles supported by AI. \n* Regularly evaluate the system’s impact on competition, employee safety, and workplace dynamics. Adjust system features or provide additional guidance as needed to ensure compliance with fair labor and safety standards. \n* Engage with regulatory bodies and labor rights organizations to ensure the AI system complies with laws and ethical guidelines related to worker protection and well-being.",
				"sources": "Right to work, No slavery (Universal Declaration of Human Rights), article 16 Freedom to conduct a business, article 28 Right of collective bargaining and action, article 5 Prohibition of slavery and forced labor, Article 31 Fair and just working conditions, article 32 Prohibition of child labor and protection of young people at work (Charter of fundamental rights of the European Union).",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system fail to uphold and respect human dignity?",
				"threatif": "Yes",
				"label": "Human Dignity",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Does the AI system treat all users with respect, ensuring no output undermines their dignity?\n* The need for data labeling is growing. Does our labeling process respect the rights and well-being of the workers involved? ",
				"recommendation": "* Ensure system outputs are designed to avoid degrading, offensive, or dehumanizing content. Regularly test and audit the AI system for potential biases or outputs that could harm individuals’ dignity. \n* Establish fair labor conditions, including proper wages, working hours, and protections for workers involved in data labeling. Avoid exploitative labor practices, such as unreasonably low compensation or unsafe working conditions. Conduct regular audits to verify that third-party providers adhere to ethical standards. \n* Engage stakeholders, including user groups and labor rights organizations, to review and improve practices. \n* Train developers, data labelers, and system operators on the importance of preserving human dignity in AI-related tasks. \n* Include guidelines for respectful and non-discriminatory practices in AI system documentation and policies. \n* Implement mechanisms to identify and address cases where AI system outputs or processes violate human dignity. Provide users and stakeholders with channels to report concerns and ensure timely resolution.",
				"sources": "Article 1 Human Dignity (Charter of Fundamental Rights of the European Union)\n[The exploited labor behind AI](https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system affect democracy or have an adverse impact on society at large?",
				"threatif": "Yes",
				"label": "Right to Democracy",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Could your product be used for monitoring and surveillance purposes?\n* Could the system interfere with democratic principles, such as having a pluralistic system of political parties and organizations, or ensuring transparency and accountability in public administration?\n* Could the system influence voting choices, limit citizens' access to voting, or restrict their ability to run as candidates in elections?",
				"recommendation": "* Train the AI system on unbiased data and incorporate mechanisms to detect and address misinformation or disinformation that could affect democratic outcomes. If the system is used in voting or election processes, ensure robust cybersecurity measures and fail-safes to protect against tampering, hacking, or manipulation. \n* Design the system to promote pluralistic views and ensure it does not restrict or prioritize certain political narratives. \n* Adhere to relevant national and international legal standards protecting democracy, political freedoms, and human rights. \n* Continuously monitor the AI system’s impact on democratic institutions and processes, making adjustments as necessary to mitigate risks and uphold democratic principles.\n* Conduct an impact assessment to evaluate how the AI system might influence democratic processes, including political participation, electoral fairness, and public administration transparency. Implement strict policies to prevent the system from favoring or disfavoring specific political parties, candidates, or ideologies. \n* Make the system’s purpose, data sources, and decision-making processes clear and accessible to the public, ensuring that its operations can be scrutinized by independent parties. \n* Collaborate with regulatory bodies and civic organizations to establish oversight committees that monitor the system's impact on democratic processes.",
				"sources": "Right to democracy (Universal Declaration of Human Rights), article 41 Right to good administration, article 39 Right to vote and to stand as a candidate at elections to the European Parliament, article 40 Right to vote and to stand as a candidate at municipal elections (Charter of fundamental rights of the European Union).",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Do we offer users and accessible way to contest AI decisions or seek redress?",
				"threatif": "Yes",
				"label": "User Redress & Remedy",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* For applications that can adversely affect individuals, you might need to consider implementing a redress by design mechanism where affected individuals can request remedy or compensation.\n* Article 22(3) GDPR provides individuals with a right to obtain human intervention if a decision is made solely by an AI system and it also provides the right to contest the decision.\n* When AI systems adversely affect individuals, ethical and legal principles require that users can seek remedy for harm. This includes the right to compensation, correction of wrong decisions, or even halting further use of the system in certain cases. The EU Charter (Article 47), GDPR Article 22(3), and emerging AI regulations affirm these rights. Failing to provide effective redress mechanisms risks infringing fundamental rights and eroding public trust—especially in sensitive domains like healthcare, credit, or law enforcement.",
				"recommendation": "Design redress mechanisms that allow affected individuals to report harm, request compensation, or demand system correction. This includes enabling redress even for those indirectly harmed (e.g., via biased profiling). Ensure accessibility and transparency of the redress process, define timelines and escalation paths, and document how redress outcomes are used to improve system performance.",
				"sources": "Right to be treated fairly by a court (Universal Declaration of Human Rights), article 11 Freedom of expression and information, article 47 Right to an effective remedy and to a fair trial (Charter of fundamental rights of the European Union).\n[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)\n[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Accountability & Human Oversight",
					"Transparency & Accessibility"
				],
				"phases": [
					"Design",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the system have an impact on decisions that affect life, health, or personal safety?",
				"threatif": "Yes",
				"label": "Right to Life",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Consider for instance the risk if your AI system is used in the health sector for choosing the right treatment for a patient. \n* Is the output of the model accurate and fair? \n* Are your datasets representative enough and free from bias? \n* Does the system produce outputs, such as fake news, that could put the life of somebody in danger? \n* Could the system encourage harmful health practices or medical misinformation? \n* Also consider whether the system could lead to loss of human lives or a significant decline in quality of life, especially when used in safety-critical or decision-support contexts.",
				"recommendation": "* Design the system with rigorous safety standards to minimize risks in scenarios affecting the right to life, such as healthcare or emergency response. Ensure datasets are representative and regularly validated for fairness, accuracy, and absence of harmful biases. \n* Include safeguards against outputs that may promote harmful practices, misinformation, or decisions endangering life. Conduct robust testing to identify and mitigate potential errors or unintended consequences. \n* Prohibit the dissemination of outputs that could incite violence, endanger health, or spread medical misinformation. Establish a monitoring mechanism to flag and rectify such outputs in real-time. \n* Engage domain experts, such as healthcare professionals or ethics specialists, in the system design and evaluation process. Use their input to ensure the AI system aligns with ethical standards for protecting life. \n* Establish a post-market monitoring system to identify and address risks that may emerge after deployment, especially in dynamic contexts like healthcare or public safety. \n* Provide training for users and operators to ensure they understand the system's limitations and ethical implications, particularly in life-critical decision-making contexts. Encourage informed and responsible use through comprehensive documentation and guidelines. \n* Create mechanisms for users and affected individuals to challenge decisions that may impact the right to life. Implement a robust redressal process to address grievances and prevent recurrence of harmful outcomes.",
				"sources": "Right to life, No torture and inhuman treatment (Universal Declaration of Human Rights), article 2 Right to life, article 3 Right to the integrity of the person, article 4 Prohibition of torture and inhuman or degrading treatment or punishment (Charter of fundamental rights of the European Union).\n[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)\n[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system limit, suppress or distort users’ freedom of expression?",
				"threatif": "Yes",
				"label": "Freedom of Expression",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Consider whether your AI system’s moderation, recommendation, or censorship mechanisms may inadvertently restrict or distort users' ability to express themselves freely.",
				"recommendation": "* Adhere to ethical guidelines and ensure transparency and accountability. \n* Regularly audit and refine content moderation algorithms to minimize false positives in detecting harmful content. Incorporate diverse training data that reflects a wide range of cultural, linguistic, and contextual nuances. \n* Provide users with clear explanations and opportunities to contest or appeal content moderation decisions. Develop an independent oversight committee to review contentious cases of content removal. \n* Collaborate with diverse stakeholders to ensure freedom of expression is preserved. Test the system with input from underrepresented communities to identify potential biases or oversights. \n* Allow users to customize their interaction with content filters, such as by adjusting sensitivity levels or choosing topics they wish to see moderated differently. Provide clear guidelines and options for users to express themselves within platform policies. \n* Establish mechanisms for users to report errors in content moderation and provide constructive feedback. \n* Continuously monitor the system's performance and adapt to emerging risks or contexts that may affect freedom of expression. \n* Align the system’s operation with international standards protecting freedom of expression, such as article 11 of the Charter of Fundamental Rights of the European Union and article 19 of the Universal Declaration of Human Rights.",
				"sources": "[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)\n[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could our AI system affect access to services such as healthcare, housing, insurance, benefits or education?",
				"threatif": "Yes",
				"label": "Access to Essential Services",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* The output of your model could be used to deny access to certain fundamental rights.\n* How can you be sure that the decisions of your AI system are always fair and correct?\n* How can you prevent causing harm to individuals?\n* AI systems intended to be used to determine access or admission, evaluate learning outcomes, or monitor students’ behaviors are classified as “high risk” by the AI Act (Annex III).\n* Could the AI system create barriers to healthcare access for some groups or individuals?",
				"recommendation": "* Adhere to EU Trustworthy AI guidelines to ensure fairness and accountability\n* Use diverse, representative training data to reduce biases that could disproportionately impact certain groups. \n* Regularly audit the system for unintended discriminatory effects and address identified issues. \n* Provide clear explanations for decisions made by the AI system, including the data and logic used. Allow users to challenge decisions and request human reviews. \n* Establish post-market monitoring processes to detect and address issues that arise after deployment. \n* Update the system regularly to account for changes in legal requirements, societal norms, and data quality. For high-risk applications, such as determining healthcare access or evaluating job candidates, establish stringent safeguards to minimize the risk of harm. Implement thresholds and fail-safes to ensure critical decisions are accurate, fair, and reliable. \n* Work with regulatory bodies, civil society organizations, and industry peers to establish best practices and promote fairness and equity in AI systems. Ensure the AI system is designed to accommodate a wide range of users, including those with varying needs and abilities. Regularly test the system in diverse real-world settings to validate its accessibility and fairness. Use stakeholder consultations to understand the specific needs and vulnerabilities of affected groups.",
				"sources": "Right to education, Right of social service (Universal Declaration of Human Rights), article 14 Right to education, article 34 Social security and social assistance, article 35 Healthcare, article 36 Access to services of general economic interest (Charter of fundamental rights of the European Union)\n[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)\n[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system interfere with users’ autonomy influencing their decision-making process?",
				"threatif": "Yes",
				"label": "Human Autonomy",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Could your system affect which choices and which information is made available to people?\n* Could the AI system affect human autonomy by generating over-reliance by users (too much trust on the technology)?\n* Could this reinforce their beliefs or encourage certain behaviors?\n* Could the AI system create human attachment, stimulate addictive behavior, or manipulate user behavior? \n* Could the AI system mislead consumers or provide false recommendations? ",
				"recommendation": "* Clearly explain how the AI system processes inputs and generates outputs to avoid unintentional manipulation or misrepresentation. Ensure users understand the limitations and intended purposes of the AI system through user-friendly documentation and communication. Offer features that allow users to adjust or override AI recommendations, ensuring they maintain control over decisions. Implement mechanisms for users to pause, disable, or opt-out of certain AI functionalities.\n* Implement safeguards to detect and reduce over-reliance, such as reminders or notifications encouraging users to seek alternative opinions or double-check recommendations. Include disclaimers or warnings about the system’s limitations in contexts where over-reliance might occur. \n* Refrain from using techniques that exploit psychological vulnerabilities, such as gamification, excessive notifications, or reward loops that could encourage addictive behavior. Periodically evaluate whether design elements unintentionally foster dependency on the system. \n* Test the system with users from various cultural, socioeconomic, and demographic backgrounds to understand potential impacts on different groups. Incorporate diverse perspectives to avoid inadvertent biases that could restrict autonomy for certain populations. \n* Continuously monitor for any behaviors or outputs that may interfere with user decision-making processes. Use post-market monitoring to collect feedback and implement updates to reduce unintended autonomy infringements. Ensure human oversight mechanisms are in place for critical decision-making areas. Clearly define the role of the AI system as a tool to assist, not replace, human decision-making.",
				"sources": "Article 6 Right to liberty and security, article 3 Right to the integrity of the person, article 38 Consumer protection (Charter of fundamental rights of the European Union), article 5a (AI Act)\n[Dispositional and Situational Attributions of Human Versus Robot Behaviour](https://www.frontiersin.org/articles/10.3389/frobt.2021.788242/full)\n[Understanding Human Over-Reliance on Technology](https://www.ismp.org/resources/understanding-human-over-reliance-technology)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Transparency & Accessibility",
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system promote certain values or beliefs on users?",
				"threatif": "Yes",
				"label": "Freedom of Thought",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Could cultural and language differences be an issue when it comes to the ethical nuance of your algorithm? Well-meaning values can create unintended consequences.\n* Must the AI system understand the world in all its different contexts?\n* Could ambiguity in rules you teach the AI system be a problem?\n* Can your system interact equitably with users from different cultures and with different abilities?",
				"recommendation": "* Consider designing with value alignment, which means that you want to ensure consideration of existing values and sensitivity to a wide range of cultural norms and values.\n* Make sure that when you test the product you include a large diversity in type of users.\n* Think carefully about what diversity means in the context where the product is going to be used.\n* Remember that this is a team effort and not an individual decision.",
				"sources": "Freedom of thought and religion(Universal Declaration of Human Rights), article 22 Cultural, religious and linguistic diversity, article 10 Freedom of thought, Conscience and religion (Charter of fundamental rights of the European Union)\n[Value alignment](https://www.ibm.com/design/ai/ethics/value-alignment/)\n[Online Ethics Canvas](https://www.ethicscanvas.org/canvas/index.php)\n[AI Values and Alignment](https://www.deepmind.com/publications/artificial-intelligence-values-and-alignment)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Transparency & Accessibility",
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system negatively impact vulnerable groups or fail to protect their rights?",
				"threatif": "Yes",
				"label": "Vulnerable Groups",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* AI systems can unintentionally marginalize or harm vulnerable individuals or groups, such as children, the elderly, migrants, ethnic minorities, or individuals with cognitive or psychosocial disabilities.\n* These groups often face barriers to representation, consent, and redress. AI systems may reflect or amplify societal biases, particularly if training data lacks diversity or design decisions fail to account for structural inequalities.\n* The EU Charter of Fundamental Rights and the AI Act emphasize special protection for vulnerable populations, especially where AI is deployed in high-risk domains like education, health, welfare, or justice.",
				"recommendation": "* Conduct a Human Rights Impact Assessment (HRIA) early in the design process, paying special attention to risks of exclusion, discrimination, or harm to vulnerable populations.\n* Engage with advocacy organizations, domain experts, and affected groups to surface risks that may not be visible from a technical perspective.\n* Ensure that training data includes diverse representations and that the system can adapt to variations in user ability, language, culture, or socioeconomic background.\n* Include clear channels for recourse, appeal, and human oversight, especially for automated decisions that significantly affect individuals.\n* Review deployment contexts for hidden power asymmetries or coercion risks, particularly where vulnerable groups may be subject to profiling or behavioral nudging.",
				"sources": "[Non-discrimination: groups in vulnerable situations](https://www.ohchr.org/en/special-procedures/sr-health/non-discrimination-groups-vulnerable-situations)\n[HUDERIA Methodology](https://www.coe.int/en/web/portal/-/huderia-new-tool-to-assess-the-impact-of-ai-systems-on-human-rights)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Bias, Fairness & Discrimination"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could the AI system fail to uphold the rights and best interests of children?",
				"threatif": "Yes",
				"label": "Children’s Rights",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Children interacting with AI systems require special protections to ensure their rights, safety, and well-being are preserved. AI systems used by or designed for children must prioritize their best interests, such as ensuring age-appropriate content, safeguarding their privacy, and fostering their ability to share, learn, and express themselves freely. A failure to address these factors could result in harm, exploitation, or the suppression of their rights. For example, an AI system might expose children to inappropriate content, fail to protect their personal data, or limit their ability to engage in meaningful learning and expression.",
				"recommendation": "* Develop and test the system for age-appropriateness. \n* Implement mechanisms to filter and block harmful or inappropriate content. \n* Adhere to strict data privacy regulations, such as GDPR, ensuring children’s data is protected. Foster safe environments where children can freely share their thoughts and ideas. Include features that support interactive and meaningful learning experiences. \n* Engage with experts in child development, education, and rights advocacy during the design phase. Consult children (where appropriate) to ensure their perspectives are respected and integrated. \n* Continuously monitor the AI system for unintended harms or risks to children. \n* Clearly communicate to parents, guardians, and educators how the AI system works and the measures in place to protect children. Provide accessible guidelines for safe and effective use.",
				"sources": "Article 24 The rights of the child (Charter of Fundamental Rights of the European Union)\n[Convention on the Rights of the Child, UNICEF](https://www.unicef.org/child-rights-convention)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is the development and use of the AI system proportionate to its intended purpose and impact on rights?",
				"threatif": "No",
				"label": "Proportionality",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Proportionality is a general principle of EU law. It requires you to strike a balance between the means used and the intended aim.\n* In the context of fundamental rights, proportionality is key for any limitation on these rights.",
				"recommendation": "* Proportionality requires that advantages due to limiting the right are not outweighed by the disadvantages to exercise the right. In other words, the limitation on the right must be justified.\n* Safeguards accompanying a measure can support the justification of a measure. A pre-condition is that the measure is adequate to achieve the envisaged objective.\n* In addition, when assessing the processing of personal data, proportionality requires that only that personal data which is adequate and relevant for the purposes of the processing is collected and processed. Source: [EDPS](https://edps.europa.eu/data-protection/our-work/subjects/necessity-proportionality_en)",
				"sources": "Proportionality test: [EDPS Guidelines on assessing the proportionality of measures that limit the fundamental rights to privacy and to the protection of personal data](https://edps.europa.eu/sites/edp/files/publication/19-12-19_edps_proportionality_guidelines_en.pdf)\n\nAssess the possible impact on human rights: [Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012P/TXT&from=EN)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Privacy & Data Protection",
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Does the AI system use behavioral data in ways that may raise ethical, privacy, or human rights concerns?",
				"threatif": "Yes",
				"label": "Behavioral Data",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Behavioral data includes individuals' actions, habits, preferences, or biometric responses, such as keystrokes, browsing history, device usage, or emotional expressions.\n* AI systems that track and learn from behavior can create serious risks, such as:\n  - Privacy violations through covert or unconsented surveillance.\n  - Profiling and discrimination, as behavioral traits may act as proxies for protected characteristics (e.g., gender, ethnicity, age).\n  - Manipulation and behavioral exploitation, especially if labeling or feedback loops reinforce conformity or nudge users toward certain actions.\n  -  Chilling effects on autonomy and expression, particularly in politically sensitive or authoritarian contexts.\n* These risks implicate key rights under the EU Charter of Fundamental Rights, the ECHR, and the EU AI Act, which designates behavior-influencing AI as high-risk.",
				"recommendation": "* Define and document how behavioral data is collected, labeled, and used, including value judgments behind 'positive' or 'negative' classifications.\n* Obtain explicit, informed consent for behavior tracking and provide opt-out mechanisms.\n* Implement privacy-preserving techniques (e.g., differential privacy, federated learning) to reduce data exposure.\n* Regularly audit for bias in behavior-based profiling and assess the representativeness and fairness of training data.\n* Conduct Human/Fundamental Rights Impact Assessments or DPIAs where applicable.\n* Apply safeguards to prevent misuse in sensitive domains (e.g., employment, finance, public services), and assess whether the system qualifies as high-risk under the EU AI Act.",
				"sources": "Article 1, Human dignity, article 7 Right to privacy, article 10 Freedom of thought, conscience, and religion (Charter of Fundamental Rights of the European Union)\n[International dimension of data protection](https://commission.europa.eu/law/law-topic/data-protection/international-dimension-data-protection_en)\n[Court of Justice Schrems II](https://curia.europa.eu/jcms/upload/docs/application/pdf/2020-07/cp200091en.pdf)\n[Human Rights Impact Assessment, Ontario Human Rights Commission](https://www3.ohrc.on.ca/en/human-rights-ai-impact-assessment)",
				"qr": "",
				"categories": [
					"Ethics & Human Rights",
					"Privacy & Data Protection",
					"Bias, Fairness & Discrimination",
					"Data & Data Governance"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Monitor"
				]
			}
		]
	},
	{
		"category": "Accountability & Human Oversight",
		"id": 8,
		"colour": "eea4b5",
		"cards": [
			{
				"question": "Is the AI system's task clearly defined, with well-scoped objectives and boundaries?",
				"threatif": "No",
				"label": "Unclear Task Definition",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Is the problem you want to solve well defined? Are the system's goals specific and measurable?\n* Are the possible benefits clear and aligned with the intended use?\n* Clearly defining the AI system’s task and intended purpose helps set boundaries for design, training, deployment, and oversight.\n* A vague or evolving objective may lead to scope creep, misaligned optimization, or unintended consequences. This is especially critical in high-risk use cases where safety, fairness, or legal compliance are required.\n* The intended purpose must also be documented and traceable throughout the lifecycle, this is essential for risk classification, legal accountability, and effective stakeholder communication (e.g., users, regulators, auditors).",
				"recommendation": "* Clearly define the problem and outcome you are optimizing for.\n* Assess if your AI system will be well-suited for this purpose.\n* Always discuss if there are alternative ways to solve the problem.\n* Define success. Working with individuals who may be directly affected can help you identify an appropriate way to measure success.\n* Make sure there is a stakeholder involved (product owner for instance) with enough knowledge of the business and a clear vision about what the model needs to do.\n* Have you considered using analytics first? In this context analytics could also offer inspiring views that can help you decide on the next steps. They can be a good source of information and are sometimes enough to solve the problem without the need of AI/ML.",
				"sources": "[Data Analytics Functionality Index](https://www.functionalai.org)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Model"
				]
			},
			{
				"question": "Have we identified and involved all key stakeholders relevant to this phase of the AI lifecycle?",
				"threatif": "No",
				"label": "Stakeholder Involvement",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Do you have all the necessary stakeholders on board? Not having the right people that can give the necessary input can put the design of the AI system in danger.\n* Think for instance when attributes or variables need to be selected, or when you need to understand the different data contexts.\n* Data scientists should not be the only ones making assumptions about variables, it should really be a team effort.",
				"recommendation": "* Identify and involve relevant stakeholders early in the AI system lifecycle. This will avoid unnecessary rework and frustrations.\n* Identifying who is responsible for making the decisions and how much control they have over the decision-making process allows for a more evident tracking of responsibility in the AI’s development process.",
				"sources": "",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Have all relevant staff and users received adequate training to understand, oversee, and responsibly interact with the AI system?",
				"threatif": "No",
				"label": "Training and Oversight Readiness",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Individuals involved in the development, deployment, operation, or use of AI systems must understand their functionality, risks, and limitations. Without adequate training, staff may misuse the system, fail to detect errors, or be unable to intervene effectively. This undermines human oversight, accountability, and compliance with regulatory requirements. Article 4 of the EU AI Act emphasizes the need for AI literacy, particularly for those responsible for high-risk systems.",
				"recommendation": "* Provide structured, role-specific training for developers, operators, decision-makers, and users interacting with the AI system.\n* Cover system capabilities, limitations, error detection, appropriate interventions, and escalation procedures.\n* Include modules on fairness, data protection, explainability, and responsible interpretation of AI outputs.\n* Refresh training regularly to reflect system updates and evolving regulations.\n* Track and document training participation as part of accountability measures.\n* Integrate training into onboarding and ongoing professional development frameworks.",
				"sources": "Article 4 EU AI Act: I Literacy",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we have qualified people available to supervise the behavior of AI agents and provide feedback during learning?",
				"threatif": "No",
				"label": "AI Agents’ Feedback",
				"aitypes": [
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Reinforcement Learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n\n* When the agent is learning to perform a complex task, human oversight and feedback are more helpful than just rewards from the environment. Rewards are generally modelled such that they convey to what extent the task was completed, but they do not usually provide sufficient feedback about the safety implications of the agent’s actions. Even if the agent completes the task successfully, it may not be able to infer the side-effects of its actions from the rewards alone. In the ideal setting, a human would provide fine-grained supervision and feedback every time the agent performs an action (Scalable oversight). Though this would provide a much more informative view about the environment to the agent, such a strategy would require far too much time and effort from the human.",
				"recommendation": "* One promising research direction to tackle this problem is semi-supervised learning, where the agent is still evaluated on all the actions (or tasks), but receives rewards only for a small sample of those actions.\n* Another promising research direction is hierarchical reinforcement learning, where a hierarchy is established between different learning agents. There could be a supervisor agent/robot whose task is to assign some work to another agent/robot and provide it with feedback and rewards.",
				"sources": "[Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)\n[Concrete AI Safety Problems](https://openai.com/blog/concrete-ai-safety-problems/)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Monitor"
				]
			},
			{
				"question": "Do we have the resources and processes to effectively oversee AI decision-making?",
				"threatif": "No",
				"label": "Oversight Capacity",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Deployer"
				],
				"explanation": "* Human oversight is essential for identifying errors, biases, or unintended consequences in AI systems, especially in high-risk contexts. However, meaningful oversight requires not only a procedural mechanism but also adequate staffing, expertise, training, and organizational support.\n* Without sufficient resources, human reviewers may merely rubber-stamp decisions or fall into automation bias, reducing accountability and increasing the likelihood of harmful outcomes.",
				"recommendation": "* Allocate clear roles and responsibilities for oversight.\n* Train reviewers to recognize automation bias and understand the system's limitations.\n* Establish workflows that support human-in-the-loop or human-on-the-loop oversight.\n* Involve multidisciplinary stakeholders in the review process to ensure meaningful checks and balances.",
				"sources": "[Automation Bias](https://en.wikipedia.org/wiki/Automation_bias)\n[The Flaws of Policies Requiring Human Oversight of Government Algorithms](https://arxiv.org/ftp/arxiv/papers/2109/2109.05067.pdf)\n[The False Comfort of Human Oversight as an Antidote to AI Harm](https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight"
				],
				"phases": [
					"Design",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Is there a well-defined process to escalate AI-related failures or unexpected outcomes?",
				"threatif": "No",
				"label": "Escalation Path",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Deployer"
				],
				"explanation": "* If an AI system produces harmful or incorrect outputs, is there a predefined process for reporting and addressing these issues? \n* Are employees aware of how to escalate AI failures, and do they have clear channels to report incidents?",
				"recommendation": "* Set up clear escalation protocols to identify, report, and resolve AI-related incidents. \n* Assign responsibilities to ensure accountability for handling AI failures. \n* Keep assessing and improving incident response strategies over time, especially after performing changes or updates in the AI systems.",
				"sources": "[AI: An Accountability Framework for Federal Agencies and Other Entities](https://www.gao.gov/assets/gao-21-519sp.pdf)\n[The Human Factor in AI Safety](https://arxiv.org/abs/2201.04263)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Have we defined who is accountable for the AI system’s decisions and outcomes?",
				"threatif": "No",
				"label": "Responsibility",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Deployer"
				],
				"explanation": "AI outputs can lead to mistakes or even cause harm. In such cases, is it clear who is responsible within your organization? Are accountability structures clearly defined and documented? ",
				"recommendation": "* Assign and communicate responsibilities for AI decision-making, considering both legal and ethical accountability. \n* Use decision logs and role-based access control to document and track accountability throughout the AI system’s lifecycle. \n* Get leadership involved in maintaining oversight, keeping accountability a priority at all levels.",
				"sources": "[AI: An Accountability Framework for Federal Agencies and Other Entities](https://www.gao.gov/assets/gao-21-519sp.pdf)\n[Algorithmic Accountability for the Public Sector](https://ainowinstitute.org/publication/algorithmic-accountability-for-the-public-sector-report)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Privacy & Data Protection"
				],
				"phases": [
					"Design",
					"Input",
					"Model",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we regularly review whether the AI system’s goals, assumptions, and impacts are still appropriate?",
				"threatif": "No",
				"label": "Continuous Assessment",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "AI models and their objectives may drift from their original intent, making human oversight crucial to ensure ongoing alignment with ethical and business objectives. Are there periodic human-led reviews in place to monitor AI system behavior, validate outcomes, and reassess goals? Human oversight should play an active role in detecting unintended consequences, adjusting governance policies, and maintaining accountability throughout the AI system’s lifecycle.",
				"recommendation": "* Schedule regular reassessments of AI objectives and assumptions.\n* Update training data, governance policies, and oversight structures as AI systems evolve.",
				"sources": "[Embedding Ethical Oversight in AI Governance through Independent Review](https://www.responsible.ai/embedding-ethical-oversight-in-ai-governance-through-independent-review/)\n[Effective Human Oversight of AI-Based Systems: A Signal Detection Perspective on the Detection of Inaccurate and Unfair Outputs](https://link.springer.com/article/10.1007/s11023-024-09701-0)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Output",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Can human operators safely interrupt or override the AI system at any time?",
				"threatif": "No",
				"label": "Human Override Mechanisms",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* High-risk AI systems must provide natural persons with the means to stop or override the system when necessary. This includes mechanisms such as a 'stop button' or fallback procedures that bring the system to a safe state.\n* A lack of override capabilities could lead to harm, especially in autonomous systems where malfunction or misalignment may go unnoticed without human intervention.",
				"recommendation": "* Design systems with built-in override or halt capabilities.\n* Ensure that these mechanisms are tested regularly and accessible to responsible personnel.\n* Document override procedures clearly and provide training to relevant users.",
				"sources": "[AI Act – Article 14(4)(e)](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Safety & Environmental Impact"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Could users contest or challenge the decisions made by the AI system?",
				"threatif": "No",
				"label": "Contestability of AI Decisions",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "Some AI systems make or support decisions that significantly affect individuals, such as in hiring, lending, or criminal justice. If users cannot challenge these decisions or request human review, the system may violate oversight obligations and erode trust. Lack of contestability undermines accountability and may breach Article 22(3) of the GDPR or Article 14 of the EU AI Act, both of which require mechanisms for human intervention and review.",
				"recommendation": "* Ensure the AI system includes mechanisms for contestability, allowing users to challenge or seek review of decisions that negatively impact them. Wrong decisions could also have an impact on people that have not been the target of the data collection (data spillovers).\n* Provide clear instructions on how users can initiate such challenges and ensure that this process is transparent, accessible, and user-friendly. \n* Incorporate features that enable human oversight in decision-making processes, ensuring users have the option to escalate issues to human operators. \n* Establish a redressal process that includes timelines for resolution, a clear escalation hierarchy, and mechanisms for feedback integration to improve the system’s decision-making over time. \n* Regularly audit and evaluate the decision-making outcomes of the AI system, focusing on areas where users frequently raise disputes. Use these audits to improve system accuracy and reduce the need for contestation. \n* Provide detailed and comprehensible explanations of the system’s outputs to users, ensuring they understand how decisions are made and what data was used. \n* Engage relevant stakeholders, including legal experts, ethicists, and representatives from affected user groups to design and evaluate the contestability mechanisms and ensure they meet ethical and regulatory standards. \n* Train system operators and customer support staff to handle disputes arising from the AI system effectively, ensuring they are equipped to assist users in navigating the contestation process.",
				"sources": "Right to be treated fairly by a court (Universal Declaration of Human Rights), article 11 Freedom of expression and information, article 47 Right to an effective remedy and to a fair trial (Charter of fundamental rights of the European Union).\n[Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)\n[Operational Guidance on taking account of Fundamental Rights in Commission Impact Assessments](https://ec.europa.eu/info/sites/default/files/opperational-guidance-fundamental-rights-in-impact-assessments_en.pdf)\n[Artificial Intelligence and Fundamental Rights](https://fra.europa.eu/sites/default/files/fra_uploads/fra-2020-artificial-intelligence_en.pdf)",
				"qr": "[Charter of Fundamental Rights of the European Union](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:12012P/TXT&from=EN)",
				"categories": [
					"Accountability & Human Oversight",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Model",
					"Output",
					"Monitor"
				]
			},
			{
				"question": "Have we assessed our legal liability for damages caused by our AI system?",
				"threatif": "No",
				"label": "Liability Risk",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "* Failing to assess liability risks can expose your organization to legal, financial, and reputational damage.\n* Have you identified who could be held liable, your organization, end-users, third parties?\n* Black-box AI systems complicate the attribution of responsibility, especially in high-risk or harmful scenarios, increasing the burden of proof for affected individuals.\n* Legal liability varies across jurisdictions, and evolving regulations such as the EU AI Liability Directive may significantly affect your obligations.\n* Failure to document decision-making processes and ensure auditability can weaken your defense in case of litigation.",
				"recommendation": "* Conduct a liability risk assessment for your AI system, including mapping potential damages and responsible parties.\n* Implement transparency, traceability, and auditability mechanisms throughout the AI lifecycle to support accountability.\n* Ensure that risk scenarios are documented and, where relevant, covered by insurance policies.\n* Stay informed about legal developments in AI liability.\n* Provide training to relevant teams on legal accountability and AI governance best practices.",
				"sources": "[Liability Rules on Artificial Intelligence](https://commission.europa.eu/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Ethics & Human Rights"
				],
				"phases": [
					"Design",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "Do we have adequate resources and MLOps practices in place to manage, monitor, and maintain our AI system?",
				"threatif": "No",
				"label": "Lack of MLOps",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Provider",
					"Deployer"
				],
				"explanation": "MLOps (Machine Learning Operations) refers to the engineering and governance practices required to reliably develop, deploy, and monitor machine learning models in production. Without proper MLOps, organizations may face:\n* Model Drift: Performance degradation due to changes in input data or real-world conditions.\n* Lack of Traceability: Difficulty reproducing results or auditing decisions.\n* Operational Failures: Models failing silently or behaving unpredictably in production.\n* Compliance Risks: Inability to demonstrate accountability or meet regulatory requirements.\n\nMLOps is especially important for high-risk AI applications under the EU AI Act, where continuous monitoring, retraining, and documentation are legal obligations.",
				"recommendation": "* Establish clear MLOps processes including versioning, CI/CD pipelines, and model registry.\n* Continuously monitor model performance, fairness, and drift.\n* Ensure auditability by logging predictions, training runs, and data lineage.\n* Automate testing and rollback mechanisms for safe model updates.\n* Define clear responsibilities between data scientists, ML engineers, and operations staff.\n* Include human-in-the-loop checks or alerts for sensitive or safety-critical applications.",
				"sources": "[MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Safety & Environmental Impact",
					"Data & Data Governance"
				],
				"phases": [
					"Model",
					"Deploy",
					"Monitor"
				]
			},
			{
				"question": "If we plan to deploy a third-party AI tool, have we assessed our shared responsibility for its potential impact on users?",
				"threatif": "No",
				"label": "Shared Responsibility",
				"aitypes": [
					"Traditional",
					"Generative"
				],
				"roles": [
					"Deployer"
				],
				"explanation": "If you use a third-party tool you might still have a responsibility towards the users. Think about employees, job applicants, patients, etc. It is also your responsibility to make sure that the AI system you choose won't cause harm to the individuals.",
				"recommendation": "If personal data is involved, review which ones are your responsibilities (look into art. 24 and 28 GDPR).\n\nYou can also start by checking:\n* That you have the right agreements in place with the third party provider.\n* That the origin and data lineage of their datasets are verified.\n* How their models are fed; do they anonymize the data?\n* How you have assessed their security, ethical data handling, quality processes and measures to prevent bias and discrimination in their AI system.\n* That you have informed users accordingly.",
				"sources": "",
				"qr": "",
				"categories": [
					"Accountability & Human Oversight",
					"Cybersecurity"
				],
				"phases": [
					"Design",
					"Input",
					"Deploy",
					"Monitor"
				]
			}
		]
	}
]
